{
  "run_card_version": "1.0",
  "run_id": "sonnet-4-5_rag_extraction_abs_005_C1_fixed_seed_rep0",
  "task_id": "rag_extraction",
  "task_category": "rag_structured_extraction",
  "prompt_card_ref": "prompt_card_rag_extraction_v1_0.json",
  "prompt_hash": "8954dea2db116d744a5abe2a65fef86fdf3a6f65e3087c482230edf777788243",
  "model_name": "claude-sonnet-4-5-20250929",
  "model_version": "api-managed",
  "weights_hash": "proprietary-not-available",
  "model_source": "anthropic-api",
  "inference_params": {
    "temperature": 0.0,
    "top_p": 1.0,
    "max_tokens": 1024,
    "decoding_strategy": "greedy",
    "seed": 42,
    "seed_note": "logged-only-not-sent-to-api"
  },
  "params_hash": "798df7eed2b7330b62f75040c9b1663c2d05e17eb1a44fef54ecc10b2f8fa2da",
  "interaction_regime": "single-turn-with-context",
  "retrieval_context": {
    "source": "simulated_retrieval",
    "query": "Wei et al. 2022 - Chain-of-Thought Prompting Elicits Reasoning in LLMs (NeurIPS)",
    "retrieved_text": "Related work: Few-shot prompting (Brown et al., 2020) showed that large models can perform tasks with minimal examples. Self-consistency (Wang et al., 2022) improves reasoning by sampling multiple chains. GSM8K (Cobbe et al., 2021) contains 8.5K grade school math word problems for evaluating reasoning.",
    "n_chunks": 1
  },
  "environment_hash": "cd45d42899c58da3d18196ae6b15aa5d337dd0cc98fea1587207f261f93df6a1",
  "code_commit": "6fe31fd70f043bf1aba26073b7dd7c7735ff7e38",
  "environment_details": {
    "os": "Darwin",
    "python_version": "3.14.3",
    "architecture": "arm64"
  },
  "timestamp_start": "2026-02-10T16:35:28.908971+00:00",
  "timestamp_end": "2026-02-10T16:35:31.888486+00:00",
  "execution_duration_ms": 2979.52,
  "output_hash": "d57dda77505b90dbec31032746b939be0e0aca1e9f9e61e418f83ade35ded106",
  "output_metrics": {},
  "logging_overhead_ms": 25.24,
  "storage_kb": 4.52,
  "researcher_id": "lucas_rover",
  "affiliation": "UTFPR - Universidade Tecnologica Federal do Parana",
  "errors": []
}