{
  "run_card_version": "1.0",
  "run_id": "llama3_8b_rag_extraction_abs_004_C1_fixed_seed_rep1",
  "task_id": "rag_extraction",
  "task_category": "rag_structured_extraction",
  "prompt_card_ref": "prompt_card_rag_extraction_v1_0.json",
  "prompt_hash": "8954dea2db116d744a5abe2a65fef86fdf3a6f65e3087c482230edf777788243",
  "model_name": "llama3:8b",
  "model_version": "8.0B",
  "weights_hash": "365c0bd3c000a25d28ddbf732fe1c6add414de7275464c4e4d1c3b5fcb5d8ad1",
  "model_source": "ollama-local",
  "inference_params": {
    "temperature": 0.0,
    "top_p": 1.0,
    "top_k": 0,
    "max_tokens": 1024,
    "decoding_strategy": "greedy",
    "seed": 42
  },
  "params_hash": "5f7d7b01f15d56320643fc4dd41f0fd4168b757a420e7c514e72a9a744f7391d",
  "interaction_regime": "single-turn-with-context",
  "retrieval_context": {
    "source": "simulated_retrieval",
    "query": "Raffel et al. 2020 - Exploring the Limits of Transfer Learning with T5 (JMLR)",
    "retrieved_text": "Related work: BERT (Devlin et al., 2019) established masked language modeling for pre-training. UniLM (Dong et al., 2019) unified different pre-training objectives. The Colossal Clean Crawled Corpus (C4) was introduced specifically for studying transfer learning at scale.",
    "n_chunks": 1
  },
  "environment_hash": "cd45d42899c58da3d18196ae6b15aa5d337dd0cc98fea1587207f261f93df6a1",
  "code_commit": "e57ae1950438395fa23acd727b18e0be41834a65",
  "environment_details": {
    "os": "Darwin",
    "python_version": "3.14.3",
    "architecture": "arm64"
  },
  "timestamp_start": "2026-02-10T02:29:14.157448+00:00",
  "timestamp_end": "2026-02-10T02:29:17.809352+00:00",
  "execution_duration_ms": 3651.9,
  "output_hash": "d22fd64365ab97b56d1536bc88c86bfa59d13867c1ee5c912f6aa3cbe9d9a583",
  "output_metrics": {},
  "logging_overhead_ms": 25.88,
  "storage_kb": 4.31,
  "researcher_id": "lucas_rover",
  "affiliation": "UTFPR - Universidade Tecnologica Federal do Parana",
  "errors": []
}