{
  "run_id": "gpt-4_summarization_abs_029_C3_temp0.3_rep2",
  "task_id": "summarization",
  "task_category": "scientific_summarization",
  "interaction_regime": "single-turn",
  "prompt_hash": "c9644358805b358076a2836466429c97a31bfd6ec5fbb42e9e9b32bcf6190b94",
  "prompt_text": "You are a scientific summarization assistant. Read the following scientific abstract and produce a concise summary in exactly 3 sentences. The summary must: (1) state the main contribution or finding, (2) describe the methodology used, and (3) report the key quantitative result if available. Do not add any information not present in the original abstract. Do not include any preamble or explanation â€” output only the 3-sentence summary.",
  "input_text": "An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA.",
  "input_hash": "600d5481abe80840fd10f890df11a4f04e3e9f4828bc2332c596ce2fb882b3d4",
  "model_name": "gpt-4",
  "model_version": "api-managed",
  "weights_hash": "proprietary-not-available",
  "model_source": "openai-api",
  "inference_params": {
    "temperature": 0.3,
    "top_p": 1.0,
    "max_tokens": 1024,
    "decoding_strategy": "sampling",
    "seed": 456
  },
  "params_hash": "a0e7b39b0130cad4de3535baa2007d540b2236a0c08d3f4f8dee1a7d43af12a3",
  "retrieval_context": null,
  "environment": {
    "os": "Darwin",
    "os_version": "Darwin Kernel Version 24.6.0: Wed Nov  5 21:34:00 PST 2025; root:xnu-11417.140.69.705.2~1/RELEASE_ARM64_T8132",
    "os_release": "24.6.0",
    "architecture": "arm64",
    "processor": "arm",
    "python_version": "3.14.3 (main, Feb  3 2026, 15:32:20) [Clang 17.0.0 (clang-1700.6.3.2)]",
    "hostname": "MacBook-Pro-de-Lucas.local",
    "timestamp": "2026-02-08T14:05:33.039655+00:00"
  },
  "environment_hash": "cd45d42899c58da3d18196ae6b15aa5d337dd0cc98fea1587207f261f93df6a1",
  "code_commit": "957a604c13f80360f0ef4b2eca5fd5e1dd856cd7",
  "researcher_id": "lucas_rover",
  "affiliation": "UTFPR - Universidade Tecnologica Federal do Parana",
  "timestamp_start": "2026-02-08T14:05:33.039592+00:00",
  "timestamp_end": "2026-02-08T14:05:37.585527+00:00",
  "output_text": "The main contribution of this study is the proposal of Low-Rank Adaptation (LoRA), a method that reduces the number of trainable parameters in large-scale pre-training models like GPT-3 175B, making them more feasible for adaptation to specific tasks or domains. LoRA works by freezing the pre-trained model weights and injecting trainable rank decomposition matrices into each layer of the Transformer architecture. The key quantitative result is that, compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times, while performing on-par or better in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, with a higher training throughput and no additional inference latency.",
  "output_metrics": {},
  "execution_duration_ms": 4545.94,
  "logging_overhead_ms": 19.44,
  "system_logs": "{\"inference_duration_ms\": 4522.45, \"finish_reason\": \"stop\", \"usage\": {\"prompt_tokens\": 341, \"completion_tokens\": 166, \"total_tokens\": 507}, \"model_id_returned\": \"gpt-4-0613\", \"system_fingerprint\": \"\", \"response_id\": \"chatcmpl-D6zhlp1Zwm1hilwh15AcUMX3R15XH\"}",
  "errors": [],
  "output_hash": "9e1ffdb5e34621cf93fecf9fcb77bd74775a195a708c97fda363a3e06bb835c4",
  "storage_kb": 4.42
}