{
  "run_id": "llama3_8b_extraction_abs_029_C2_var_seed_rep4",
  "task_id": "extraction",
  "task_category": "structured_extraction",
  "interaction_regime": "single-turn",
  "prompt_hash": "ddc1746ca23ddafc9f63790dbd2d86930c1248713bf7c854b4e119ba9f0063c1",
  "prompt_text": "You are a structured information extraction assistant. Read the following scientific abstract and extract the information into the exact JSON format below. Use only information explicitly stated in the abstract. If a field is not mentioned, use null.\n\nOutput format (JSON only, no explanation):\n{\n  \"objective\": \"string — main goal of the study\",\n  \"method\": \"string — methodology or approach used\",\n  \"key_result\": \"string — most important quantitative or qualitative result\",\n  \"model_or_system\": \"string — name of the model/system proposed (if any)\",\n  \"benchmark\": \"string — evaluation benchmark or dataset used (if any)\"\n}",
  "input_text": "An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA.",
  "input_hash": "600d5481abe80840fd10f890df11a4f04e3e9f4828bc2332c596ce2fb882b3d4",
  "model_name": "llama3:8b",
  "model_version": "8.0B",
  "weights_hash": "",
  "model_source": "ollama-local",
  "inference_params": {
    "temperature": 0.0,
    "top_p": 1.0,
    "top_k": 0,
    "max_tokens": 1024,
    "decoding_strategy": "greedy",
    "seed": 1024
  },
  "params_hash": "3cb9ece140a8186239c29bd67a82b823040655fa2657fe7c05f826faafd3e311",
  "retrieval_context": null,
  "environment": {
    "os": "Darwin",
    "os_version": "Darwin Kernel Version 24.6.0: Wed Nov  5 21:34:00 PST 2025; root:xnu-11417.140.69.705.2~1/RELEASE_ARM64_T8132",
    "os_release": "24.6.0",
    "architecture": "arm64",
    "processor": "arm",
    "python_version": "3.14.3 (main, Feb  3 2026, 15:32:20) [Clang 17.0.0 (clang-1700.6.3.2)]",
    "hostname": "MacBook-Pro-de-Lucas.local",
    "timestamp": "2026-02-08T14:11:49.755129+00:00"
  },
  "environment_hash": "cd45d42899c58da3d18196ae6b15aa5d337dd0cc98fea1587207f261f93df6a1",
  "code_commit": "957a604c13f80360f0ef4b2eca5fd5e1dd856cd7",
  "researcher_id": "lucas_rover",
  "affiliation": "UTFPR - Universidade Tecnologica Federal do Parana",
  "timestamp_start": "2026-02-08T14:11:49.755092+00:00",
  "timestamp_end": "2026-02-08T14:11:56.656151+00:00",
  "output_text": "Here is the extracted information in JSON format:\n\n{\n  \"objective\": \"proposing Low-Rank Adaptation (LoRA) to reduce the number of trainable parameters for downstream tasks\",\n  \"method\": \"freezing pre-trained model weights and injecting trainable rank decomposition matrices into each layer of the Transformer architecture\",\n  \"key_result\": \"reducing the number of trainable parameters by 10,000 times and GPU memory requirement by 3 times compared to fine-tuning with Adam\",\n  \"model_or_system\": \"LoRA\",\n  \"benchmark\": \"RoBERTa, DeBERTa, GPT-2, and GPT-3\"\n}",
  "output_metrics": {},
  "execution_duration_ms": 6901.06,
  "logging_overhead_ms": 11.82,
  "system_logs": "{\"inference_duration_ms\": 6889.22, \"model_reported_duration_ns\": 6886922417, \"eval_count\": 134, \"prompt_eval_count\": 386, \"done\": true, \"done_reason\": \"stop\"}",
  "errors": [],
  "output_hash": "294018e559544bec876b03d6454077a376c10805c6637a1af4b5b85f667bf297",
  "storage_kb": 4.34
}