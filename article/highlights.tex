\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{enumitem}
\usepackage{parskip}

\title{\textbf{Research Highlights}\\[6pt]
\large Hidden Non-Determinism in Large Language Model APIs:\\
A Lightweight Provenance Protocol for Reproducible Generative AI Research}
\author{Lucas Rover \and Yara de Souza Tadano\\
UTFPR -- Universidade Tecnol\'ogica Federal do Paran\'a}
\date{February 2026}

\begin{document}
\maketitle

\section*{Key Findings}

\begin{itemize}[leftmargin=*, itemsep=8pt]
    \item \textbf{API models exhibit hidden non-determinism under greedy decoding.} Across four independent API providers (OpenAI, Anthropic, DeepSeek, Perplexity), API-served models produce non-identical outputs even at temperature zero, with Exact Match Rates (EMR) ranging from 0.010 to 0.800.

    \item \textbf{Local models achieve near-perfect to perfect reproducibility.} Gemma~2 9B attains EMR = 1.000 (perfect bitwise match) across all four tasks. LLaMA~3 8B and Mistral 7B achieve EMR $\geq$ 0.840 under greedy decoding.

    \item \textbf{3-fold reproducibility gap confirmed across four providers.} Average single-turn EMR: local = 0.960 vs.\ API = 0.325. The gap holds in 100\% of abstracts for summarization and 83\% for extraction, surviving Holm-Bonferroni correction across 68 tests.

    \item \textbf{The gap extends to complex interaction regimes.} Multi-turn refinement and RAG extraction maintain high local-model reproducibility (EMR $\geq$ 0.880), while Claude Sonnet 4.5 achieves EMR = 0.040 (multi-turn) and 0.000 (RAG).

    \item \textbf{Comprehensive provenance logging adds negligible overhead.} Less than 1\% of inference time and approximately 4~KB per run across all seven models.
\end{itemize}

\section*{Novel Contributions}

\begin{enumerate}[leftmargin=*, itemsep=6pt]
    \item \textbf{Prompt Cards and Run Cards}: Structured documentation artifacts for generative AI experiments, with cryptographic hashing for tamper detection.
    \item \textbf{W3C PROV integration}: Machine-readable provenance graphs linking every output to its full generation context.
    \item \textbf{Largest controlled reproducibility study}: 3,804 experiments across 7 models, 4 tasks, 30 abstracts, 5 conditions.
\end{enumerate}

\section*{Practical Impact}

\begin{itemize}[leftmargin=*, itemsep=6pt]
    \item Researchers using API-served LLMs should never assume reproducibility without verification.
    \item Structured output formats (JSON extraction) improve reproducibility over open-ended generation.
    \item The protocol, reference implementation, and all data are publicly available at:\\
    \texttt{https://github.com/Roverlucas/genai-reproducibility-protocol}
\end{itemize}

\end{document}
