\documentclass[manuscript, screen, review]{jair}

\setcopyright{cc}
\copyrightyear{2026}
%% DOI, AE name, volume/article/month to be filled by JAIR editorial office
\acmDOI{}

\JAIRAE{}
\JAIRTrack{}
\acmVolume{}
\acmArticle{}
\acmMonth{}
\acmYear{2026}

\usepackage[
  datamodel=acmdatamodel,
  style=acmauthoryear,
  backend=biber,
  giveninits=true,
  uniquename=init
  ]{biblatex}

\renewcommand*{\bibopenbracket}{(}
\renewcommand*{\bibclosebracket}{)}

\addbibresource{references.bib}

%% Additional packages
\usepackage{booktabs}
\usepackage{pifont}
%% graphicx and xcolor already loaded by acmart.cls
\usepackage{colortbl} % for \cellcolor in tables (xcolor already loaded by acmart.cls)
\usepackage{listings}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{enumitem}

\lstset{
  basicstyle=\ttfamily\small,
  frame=single,
  breaklines=true,
  columns=fullflexible,
  keepspaces=true,
  numbers=left,
  numberstyle=\tiny\color{gray},
  numbersep=5pt,
  xleftmargin=15pt,
}

\begin{document}

\title[Hidden Non-Determinism in LLM APIs]{Hidden Non-Determinism in Large Language Model APIs: A Lightweight Provenance Protocol for Reproducible Generative AI Research}

\author{Lucas Rover}
\authornote{Corresponding Author.}
\orcid{0000-0001-6641-9224}
\email{lucasrover@utfpr.edu.br}
\affiliation{%
  \institution{UTFPR -- Universidade Tecnol\'ogica Federal do Paran\'a}
  \department{Programa de P\'os-Gradua\c{c}\~ao em Engenharia Mec\^anica}
  \city{Ponta Grossa}
  \state{Paran\'a}
  \country{Brazil}
}

\author{Yara de Souza Tadano}
\orcid{0000-0002-3975-3419}
\email{yaratadano@utfpr.edu.br}
\affiliation{%
  \institution{UTFPR -- Universidade Tecnol\'ogica Federal do Paran\'a}
  \department{Programa de P\'os-Gradua\c{c}\~ao em Engenharia Mec\^anica}
  \city{Ponta Grossa}
  \state{Paran\'a}
  \country{Brazil}
}

\renewcommand{\shortauthors}{Rover and Tadano}

\begin{abstract}
\textbf{Background:}
Generative AI models produce non-deterministic outputs that vary across runs, even under nominally identical configurations. This variability threatens the reproducibility of studies that rely on large language model (LLM) outputs, yet most existing experiment-tracking tools were not designed for the specific challenges of text-generation workflows.

\textbf{Objectives:}
We propose a lightweight, open-standard protocol for logging, versioning, and provenance tracking of generative AI experiments. The protocol introduces two novel documentation artifacts---Prompt Cards and Run Cards---and adopts the W3C PROV data model to create auditable, machine-readable provenance graphs linking every output to its full generation context.

\textbf{Methods:}
We formalize the protocol and evaluate it empirically through 3,804 controlled experiments. These experiments employ seven models---three locally deployed (LLaMA~3 8B, Mistral 7B, Gemma~2 9B) and four API-served (GPT-4, Claude Sonnet 4.5, DeepSeek Chat, Perplexity Sonar)---on four NLP tasks. All seven models are evaluated on single-turn extraction and summarization under greedy decoding (10--30 abstracts per model). Multi-turn refinement and RAG extraction are evaluated for the three local models and Claude Sonnet 4.5 under greedy decoding (10 abstracts each). Statistical robustness is ensured through Holm-Bonferroni correction across 68 hypothesis tests, Fisher's exact tests for binary reproducibility, bias-corrected bootstrap confidence intervals, and sensitivity analysis. We measure output variability using Exact Match Rate, Normalized Edit Distance, ROUGE-L, and BERTScore, and quantify the protocol's own overhead in terms of time and storage.

\textbf{Results:}
Under greedy decoding ($t{=}0$), local models achieve near-perfect reproducibility: Gemma~2 9B reaches EMR = 1.000 across all tasks, LLaMA~3 attains EMR = 0.987 for extraction, and Mistral 7B achieves EMR = 0.960. By contrast, API-served models exhibit substantial hidden non-determinism spanning a wide range: DeepSeek Chat achieves the highest API reproducibility (EMR = 0.800 for extraction), followed by GPT-4 (0.443), Claude Sonnet 4.5 (0.190), and Perplexity Sonar (0.100)---the lowest observed. This local-vs-API reproducibility gap (average single-turn EMR: 0.960 vs.\ 0.325, a 3-fold difference) is confirmed across four independent API providers and survives Holm-Bonferroni correction across 68 tests. Per-abstract consistency analysis shows the gap holds in 100\% of abstracts for summarization and 83\% for extraction. The gap extends to complex interaction regimes: under multi-turn refinement and RAG extraction, local models maintain high reproducibility (EMR $\geq$ 0.880), while Claude Sonnet 4.5---the only API model tested on these tasks---achieves EMR = 0.040 for multi-turn and EMR = 0.000 for RAG. The protocol adds less than 1\% overhead across all seven models.

\textbf{Conclusions:}
Our results provide evidence that (1)~API-served models exhibit substantial non-determinism under greedy decoding that is not attributable to user-controllable parameters, a pattern observed independently across four providers (OpenAI, Anthropic, DeepSeek, Perplexity); (2)~locally deployed models achieve near-perfect to perfect bitwise reproducibility under greedy decoding; (3)~API reproducibility spans a wide range (EMR 0.010--0.800), with DeepSeek Chat achieving notably higher reproducibility than other API models; (4)~the local-vs-API gap extends to multi-turn refinement and RAG extraction; (5)~temperature is the dominant user-controllable factor affecting variability; and (6)~comprehensive provenance logging adds negligible overhead ($<$1\%). All results survive Holm-Bonferroni correction for multiple comparisons. The protocol, reference implementation, and all experimental data are publicly available.
\end{abstract}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10011007.10011074.10011099.10011102.10011103</concept_id>
<concept_desc>Software and its engineering~Software testing and debugging</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010257</concept_id>
<concept_desc>Computing methodologies~Machine learning</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011074.10011099.10011692</concept_id>
<concept_desc>Software and its engineering~Documentation</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Software testing and debugging}
\ccsdesc[500]{Computing methodologies~Machine learning}
\ccsdesc[300]{Software and its engineering~Documentation}

\keywords{reproducibility, large language models, non-determinism, provenance, generative AI, experiment tracking, W3C PROV, scientific methodology}

\received{February 2026}

\maketitle

%% ============================================================
\section{Introduction}
\label{sec:introduction}

When a researcher queries a cloud-hosted LLM with the same prompt and temperature zero, one would reasonably expect identical outputs. Our experiments show otherwise: across five controlled seeds under greedy decoding, GPT-4 produces the same extraction result only 44\% of the time, and Claude Sonnet 4.5 achieves only 19\%. Meanwhile, locally deployed models such as Gemma~2 9B produce \emph{perfectly identical} outputs every time. This hidden, provider-dependent non-determinism exemplifies a fundamental challenge introduced by the rapid adoption of large language models (LLMs) in scientific research: how to ensure that studies relying on generative AI outputs are reproducible, auditable, and scientifically rigorous. Unlike traditional computational experiments, in which deterministic algorithms produce identical results given identical inputs, LLMs exhibit inherent variability in their outputs due to stochastic sampling, floating-point non-determinism, and opaque model-versioning practices \cite{chen2023chatgpt, zhu2023reproducibility}.

Importantly, ``non-reproducible'' does not necessarily mean ``unreliable'': our experiments also show that semantic similarity (measured by BERTScore F1) remains above 0.94 across all conditions, even when exact textual match drops to zero. In other words, API outputs typically convey the same \textit{meaning} despite differing in \textit{phrasing}---but this distinction is invisible without systematic measurement, and many downstream analyses (meta-analyses, comparative studies, regulatory audits) require exact reproducibility.

A related subtlety concerns the \texttt{seed} parameter offered by some APIs. For API-served models, the seed parameter is advisory, not a guarantee of determinism: OpenAI explicitly documents that ``determinism is not guaranteed'' even when a seed is specified \cite{openai2024seed}, and Anthropic's Claude API does not support a seed parameter at all. Our experimental design accounts for this by treating seed variation as a control condition and measuring actual output reproducibility directly, rather than relying on API-side determinism guarantees.

This reproducibility challenge is not merely theoretical. \citet{baker2016reproducibility} reported that over 70\% of researchers have failed to reproduce another scientist's experiment, a crisis that extends to AI research \cite{hutson2018artificial, gundersen2018state, stodden2016enhancing, kapoor2023leakage}. For generative AI specifically, the problem is compounded by several factors unique to text-generation workflows: (1)~the same prompt can yield semantically similar yet textually distinct outputs across runs; (2)~API-based models may undergo silent updates that alter behavior; (3)~temperature and sampling parameters create a high-dimensional space of possible outputs; and (4)~no established standard exists for documenting the full context needed to understand, audit, or reproduce a generative output.

Existing experiment-tracking tools such as MLflow \cite{zaharia2018accelerating}, Weights \& Biases \cite{biewald2020experiment}, and DVC \cite{kuprieiev2024dvc} were designed primarily for training pipelines and numerical metrics. Although valuable for their intended purposes, these tools lack features critical for generative AI studies: structured prompt versioning, cryptographic output hashing for tamper detection, provenance graphs linking outputs to their full generation context, and environment fingerprinting specific to inference-time conditions.

In this paper, we make three contributions, with the protocol design as the primary and most durable contribution:

\begin{enumerate}
    \item \textbf{A lightweight, standards-based protocol} for logging, versioning, and provenance tracking of generative AI experiments. The protocol introduces \textit{Prompt Cards} and \textit{Run Cards} as structured documentation artifacts, and adopts the W3C PROV data model \cite{w3cprov2013} for machine-readable provenance graphs. It operationalizes---and extends to generative AI workflows---the reproducibility checklist and badge mechanisms recently adopted by JAIR \cite{gundersen2024improving}, providing machine-readable infrastructure that automates what those mechanisms require researchers to document manually.

    \item \textbf{A large-scale empirical case study} demonstrating both the protocol's effectiveness and the reproducibility characteristics of LLM outputs in the models and snapshots evaluated. Through 3,804 controlled experiments with seven models---three locally deployed (LLaMA~3 8B, Mistral 7B, Gemma~2 9B) and four API-served (GPT-4, Claude Sonnet 4.5, DeepSeek Chat, Perplexity Sonar)---across four tasks (extraction, summarization, multi-turn refinement, RAG extraction), 30 abstracts, and five conditions, we quantify output variability using four complementary metrics and measure the protocol's overhead. Statistical robustness is ensured through Holm-Bonferroni correction across 68 hypothesis tests, Fisher's exact tests, and bias-corrected bootstrap CIs. Our results document a striking reproducibility gap between local and API-based inference in the evaluated models that is invisible without systematic logging.

    \item \textbf{A reference implementation} in Python that demonstrates the protocol's practical applicability, together with all experimental data, to facilitate adoption and independent verification.
\end{enumerate}

The remainder of this paper is organized as follows. Section~\ref{sec:related-work} reviews related work on reproducibility in AI and experiment tracking. Section~\ref{sec:protocol} formalizes the protocol design. Section~\ref{sec:experimental-setup} describes the experimental methodology. Section~\ref{sec:results} presents the empirical results. Section~\ref{sec:discussion} discusses findings, limitations, and practical implications. Section~\ref{sec:conclusion} concludes with directions for future work.


%% ============================================================
\section{Related Work}
\label{sec:related-work}

\subsection{Reproducibility in AI Research}

The reproducibility crisis in AI has been documented extensively. \citet{gundersen2018state} surveyed 400 AI papers and found that only 6\% provided sufficient information for full reproducibility. \citet{pineau2021improving} reported on the NeurIPS 2019 Reproducibility Program, which introduced reproducibility checklists and found significant gaps between reported and actual reproducibility. More recently, \citet{gundersen2024improving} described four institutional mechanisms adopted by JAIR---reproducibility checklists, structured abstracts, badges, and reproducibility reports---establishing a community standard for what should be documented in AI research. \citet{gundersen2018sources} identified three levels of reproducibility in AI---method, data, and experiment---and argued that all three are necessary for scientific progress. \citet{belz2021systematic} conducted a systematic review of 601 NLP papers and confirmed pervasive under-reporting of experimental details, while \citet{dodge2019show} proposed improved reporting standards for ML experiments, including confidence intervals and significance tests across multiple runs. More broadly, \citet{kapoor2023leakage} identified data leakage as a widespread driver of irreproducible results across 17 scientific fields that use ML-based methods.

For generative AI specifically, \citet{chen2023chatgpt} demonstrated that ChatGPT's outputs on NLP benchmarks exhibit non-trivial variability across identical queries, even with temperature set to zero. \citet{zhu2023reproducibility} showed that reproducibility degrades further when tasks involve subjective judgment, such as social computing annotations. Most recently, \citet{atil2024nondeterminism} systematically measured the non-determinism of five LLMs under supposedly deterministic settings across eight tasks, finding accuracy variations up to 15\% across runs and introducing the Total Agreement Rate (TAR) metric. \citet{ouyang2024nondeterminism} confirmed that temperature zero does not guarantee determinism in ChatGPT code generation. Most recently, \citet{yuan2025nondeterminism} traced such non-determinism to numerical precision issues in GPU kernels and proposed LayerCast as a mitigation strategy---a hardware-level fix that reduces but does not eliminate non-determinism, and that is not available to researchers using closed API services. Our Exact Match Rate (EMR) metric is closely related to \citeauthor{atil2024nondeterminism}'s Total Agreement Rate (TAR), which measures the fraction of runs producing the modal output; EMR instead measures the fraction of \textit{all output pairs} that match exactly, providing a more sensitive measure when agreement is low and no clear modal output exists. Our work complements these studies in four specific ways. First, whereas prior studies (including Atil et al.'s five-model, eight-task study) measure variability post hoc, we provide a structured provenance protocol that enables \textit{prospective} documentation and audit---answering not only ``how much variability?'' but also ``why did these outputs differ?'' through cryptographic hashing and W3C PROV graphs. Second, we directly compare local and API-based inference on identical tasks with identical prompts across \textit{seven} models and \textit{four} independent API providers (OpenAI, Anthropic, DeepSeek, and Perplexity), isolating the deployment paradigm as a variable and confirming that API non-determinism is a consistent pattern across providers---while revealing substantial variation in its magnitude (EMR ranging from 0.010 to 0.800 across providers). Third, we extend beyond single-turn evaluation to include multi-turn refinement and retrieval-augmented generation, demonstrating that reproducibility characteristics generalize across interaction regimes. Fourth, we quantify the overhead of systematic logging, demonstrating that the ``cost of knowing'' is negligible.

\subsection{Experiment Tracking Tools}

Several tools exist for tracking machine learning experiments, although none was designed specifically for generative AI text-output workflows:

\textbf{MLflow} \cite{zaharia2018accelerating} provides experiment tracking, model packaging, and deployment. It logs parameters, metrics, and artifacts, but focuses on training pipelines and numerical outcomes rather than text-generation provenance.

\textbf{Weights \& Biases} \cite{biewald2020experiment} offers experiment tracking with visualization dashboards. It supports prompt logging but lacks structured prompt versioning, cryptographic output hashing, and provenance graph generation.

\textbf{DVC} \cite{kuprieiev2024dvc} provides data versioning through git-like operations. While effective for dataset management, it does not address run-level provenance or prompt documentation.

\textbf{OpenAI Evals} \cite{openai2023evals} is a framework for evaluating LLM outputs against benchmarks. It provides structured evaluation but is tightly coupled to OpenAI's ecosystem and does not generate interoperable provenance records.

\textbf{LangSmith} \cite{langsmith2023} offers tracing and evaluation for LLM applications. It captures detailed execution traces but uses a proprietary format and requires cloud connectivity.

More broadly, \citet{bommasani2022opportunities} identified reproducibility as a key risk for foundation models, and \citet{liang2023holistic} proposed the HELM benchmark for holistic evaluation of language models, including robustness and fairness dimensions that complement our reproducibility focus. In the provenance space, \citet{padovani2025yprov} recently introduced yProv4ML, a framework that captures ML provenance in PROV-JSON format with minimal code modifications; our protocol shares the commitment to W3C PROV and SHA-256 hashing but differs in three key respects: (i)~we target inference-time stochastic text generation rather than training pipelines; (ii)~our Run Cards capture prompt-level metadata (prompt hash, seed status, interaction regime) not present in training-oriented schemas; and (iii)~we provide empirical evidence quantifying why such logging is necessary for API-served models.

Table~\ref{tab:comparison} provides a systematic feature-by-feature comparison of our protocol with these tools. The key distinction is not merely one of tooling but of \textit{scientific capability}: existing tools log what happened during training (parameters, metrics, artifacts), whereas our protocol enables answering questions that these tools cannot---specifically, whether two generative outputs are provably derived from identical configurations, which exact factor caused a divergence between non-identical outputs, and whether an output has been tampered with post-generation. These capabilities require the combination of cryptographic hashing, structured prompt documentation, and W3C PROV provenance graphs that no existing tool provides. In short, our contribution is not an alternative experiment tracker but a \textit{reproducibility assessment framework} designed for the unique challenges of stochastic text generation.

%% Table 4 - Comparison
\begin{table*}[t]
\centering
\caption{Comparison of our protocol with existing reproducibility tools and frameworks for GenAI experiments. Checkmarks (\ding{51}) indicate full support; tildes ($\sim$) indicate partial support; dashes (--) indicate no support.}
\label{tab:comparison}
\small
\begin{tabular}{@{}lcccccc@{}}
\toprule
\textbf{Feature} & \textbf{Ours} & \textbf{MLflow} & \textbf{W\&B} & \textbf{DVC} & \textbf{OpenAI Evals} & \textbf{LangSmith} \\
\midrule
Prompt versioning (Prompt Card) & \ding{51} & -- & $\sim$ & -- & $\sim$ & $\sim$ \\
Run-level provenance (W3C PROV) & \ding{51} & -- & -- & -- & -- & -- \\
Cryptographic output hashing & \ding{51} & -- & -- & \ding{51} & -- & -- \\
Seed \& param logging & \ding{51} & \ding{51} & \ding{51} & -- & \ding{51} & \ding{51} \\
Environment fingerprinting & \ding{51} & $\sim$ & $\sim$ & $\sim$ & -- & -- \\
Model weights hashing & \ding{51} & -- & $\sim$ & \ding{51} & -- & -- \\
Overhead $<$1\% of inference & \ding{51} & $\sim$ & $\sim$ & N/A & N/A & $\sim$ \\
Designed for GenAI text output & \ding{51} & -- & -- & -- & \ding{51} & \ding{51} \\
Open standard (PROV-JSON) & \ding{51} & -- & -- & -- & -- & -- \\
Local-first (no cloud dependency) & \ding{51} & \ding{51} & -- & \ding{51} & -- & -- \\
\bottomrule
\end{tabular}
\end{table*}


\subsection{Provenance in Scientific Computing}

Data provenance---the lineage of data through transformations---has a rich history in database systems and scientific workflows \cite{herschel2017survey}. The W3C PROV family of specifications \cite{w3cprov2013} provides a standardized data model for representing provenance as directed acyclic graphs of \textit{entities}, \textit{activities}, and \textit{agents}. \citet{samuel2022computational} applied provenance tracking to computational biology workflows, demonstrating its value for reproducibility. However, to our knowledge, no prior work has applied W3C PROV specifically to generative AI experiment workflows, in which the challenge involves not only tracking data lineage but also capturing the stochastic generation context that determines output variability.

Taken together, these gaps point to a clear need: a lightweight, standards-based protocol that bridges generative AI inference with the provenance infrastructure already established in scientific computing. The next section presents our design for such a protocol.


%% ============================================================
\section{Protocol Design}
\label{sec:protocol}

Our protocol addresses the question: \textit{What is the minimum set of metadata that must be captured for each generative AI run to enable auditing, reproducibility assessment, and provenance tracking?} We address this question through four complementary components.

\subsection{Scope and Design Principles}

The protocol is designed around three principles:

\begin{enumerate}
    \item \textbf{Completeness}: Every factor that can influence a generative output must be captured---prompt text, model identity and version, inference parameters, environment state, and timestamps.
    \item \textbf{Negligible overhead}: The logging process must not materially affect the experiment. We target $<$1\% overhead relative to inference time.
    \item \textbf{Interoperability}: All artifacts are stored in open, machine-readable formats (JSON, PROV-JSON), aligned with the FAIR (Findable, Accessible, Interoperable, Reusable) principles \cite{wilkinson2016fair}, to enable tool integration and long-term preservation.
\end{enumerate}

\subsection{Prompt Cards}
\label{sec:prompt-cards}

A \textit{Prompt Card} is a versioned documentation artifact that captures the design rationale and metadata for a prompt template used in experiments. Each Prompt Card contains:

\begin{itemize}
    \item \texttt{prompt\_id}: Unique identifier
    \item \texttt{prompt\_hash}: SHA-256 hash of the prompt text, enabling tamper detection
    \item \texttt{version}: Semantic version number
    \item \texttt{task\_category}: Classification of the task (e.g., summarization, extraction)
    \item \texttt{objective}: Natural-language description of what the prompt is designed to achieve
    \item \texttt{assumptions}: Explicit assumptions about inputs and expected behavior
    \item \texttt{limitations}: Known limitations or failure modes
    \item \texttt{target\_models}: Models for which the prompt was designed and tested
    \item \texttt{expected\_output\_format}: Description of the expected output structure
    \item \texttt{interaction\_regime}: Single-turn, multi-turn, or chain-of-thought
    \item \texttt{change\_log}: History of modifications
\end{itemize}

Prompt Cards serve two purposes: they document design intent (supporting human understanding) and they provide a citable, hashable reference for automated provenance tracking. The concept draws inspiration from Model Cards \cite{mitchell2019model}, Datasheets for Datasets \cite{gebru2021datasheets}, and model info sheets for reproducibility assessment \cite{kapoor2023leakage}, extending the structured-documentation paradigm to the prompt layer of the generative AI pipeline.

\subsection{Run Cards}
\label{sec:run-cards}

A \textit{Run Card} captures the complete execution context of a single generative AI run. Each Run Card records 24 core fields organized into five groups (the complete JSON schema in Appendix~\ref{app:schema} includes these fields plus additional metadata such as \texttt{researcher\_id}, \texttt{affiliation}, \texttt{system\_logs}, and \texttt{errors}):

\begin{enumerate}
    \item \textbf{Identification}: \texttt{run\_id}, \texttt{task\_id}, \texttt{task\_category}, \texttt{prompt\_hash}, \texttt{prompt\_text}
    \item \textbf{Model context}: \texttt{model\_name}, \texttt{model\_version}, \texttt{weights\_hash}, \texttt{model\_source}
    \item \textbf{Parameters}: \texttt{inference\_params} (temperature, top\_p, top\_k, max\_tokens, seed, decoding\_strategy), \texttt{params\_hash}
    \item \textbf{Input/Output}: \texttt{input\_text}, \texttt{input\_hash}, \texttt{output\_text}, \texttt{output\_hash}, \texttt{output\_metrics}
    \item \textbf{Execution metadata}: \texttt{environment} (OS, architecture, Python version, hostname), \texttt{environment\_hash}, \texttt{code\_commit}, timestamps (start/end), \texttt{execution\_duration\_ms}, \texttt{logging\_overhead\_ms}, \texttt{storage\_kb}
\end{enumerate}

For API-served models, optional extension fields capture provider-specific metadata that may help diagnose non-determinism: \texttt{api\_request\_id}, \texttt{api\_response\_headers}, \texttt{api\_model\_version\_returned}, \texttt{api\_region}, and a \texttt{seed\_status} field that distinguishes between seeds that were ``sent'' to the API, ``logged-only'' (recorded for protocol parity but not sent, as with Claude), or ``not-supported'' by the provider. This formalization ensures that the advisory or absent nature of API seed parameters is captured as structured metadata rather than left as an undocumented assumption.

Figure~\ref{fig:runcard-schema} illustrates the Run Card schema as a minimal structured record.

\begin{figure}[t]
\centering
\small
\begin{tabular}{|p{0.92\linewidth}|}
\hline
\textbf{Run Card Schema (24 core + extension fields)} \\
\hline
\textbf{1. Identification} \\
\texttt{run\_id} $\cdot$ \texttt{task\_id} $\cdot$ \texttt{task\_category} $\cdot$ \texttt{prompt\_hash} $\cdot$ \texttt{prompt\_text} \\[2pt]
\textbf{2. Model Context} \\
\texttt{model\_name} $\cdot$ \texttt{model\_version} $\cdot$ \texttt{weights\_hash} $\cdot$ \texttt{model\_source} \\[2pt]
\textbf{3. Parameters} \\
\texttt{inference\_params} \{temp, top\_p, top\_k, max\_tokens, seed, strategy\} $\cdot$ \texttt{params\_hash} \\[2pt]
\textbf{4. Input/Output} \\
\texttt{input\_text} $\cdot$ \texttt{input\_hash} $\cdot$ \texttt{output\_text} $\cdot$ \texttt{output\_hash} $\cdot$ \texttt{output\_metrics} \\[2pt]
\textbf{5. Execution Metadata} \\
\texttt{environment} $\cdot$ \texttt{environment\_hash} $\cdot$ \texttt{code\_commit} $\cdot$ timestamps $\cdot$ \texttt{duration\_ms} $\cdot$ \texttt{overhead\_ms} $\cdot$ \texttt{storage\_kb} \\[2pt]
\textbf{API Extensions} (optional) \\
\texttt{api\_request\_id} $\cdot$ \texttt{api\_region} $\cdot$ \texttt{seed\_status} $\in$ \{sent, logged-only, not-supported\} \\[2pt]
\textbf{Workflow Extensions} (optional) \\
\texttt{conversation\_history\_hash} $\cdot$ \texttt{turn\_index} $\cdot$ \texttt{retrieval\_context\_hash} $\cdot$ \texttt{parent\_run\_id} \\
\hline
\end{tabular}
\caption{Run Card minimal schema. All SHA-256 hashes (5 total) enable tamper detection and automated comparison. API and workflow extension fields are optional.}
\label{fig:runcard-schema}
\Description{A structured schema showing the five core field groups of a Run Card plus optional API and workflow extension fields.}
\end{figure}

The separation of logging overhead from execution time is deliberate: it allows researchers to verify that the protocol itself does not confound experimental measurements.

\subsubsection{Normative Field Requirements}

To support adoption as a citable specification, we classify Run Card fields using normative language following RFC~2119 \cite{rfc2119}:

\begin{itemize}
    \item \textbf{MUST} (required for audit completeness): \texttt{run\_id}, \texttt{prompt\_text}, \texttt{prompt\_hash}, \texttt{model\_name}, \texttt{model\_version}, \texttt{inference\_params} (including temperature, seed, decoding strategy), \texttt{output\_text}, \texttt{output\_hash}, \texttt{timestamp\_start}.
    \item \textbf{SHOULD} (strongly recommended): \texttt{input\_hash}, \texttt{params\_hash}, \texttt{environment\_hash}, \texttt{weights\_hash} (local models), \texttt{code\_commit}, \texttt{execution\_duration\_ms}, \texttt{logging\_overhead\_ms}, \texttt{seed\_status} (API models).
    \item \textbf{MAY} (optional, context-dependent): \texttt{api\_request\_id}, \texttt{api\_response\_headers}, \texttt{api\_region}, \texttt{conversation\_history\_hash}, \texttt{turn\_index}, \texttt{retrieval\_context\_hash}, \texttt{parent\_run\_id}, \texttt{researcher\_id}, \texttt{affiliation}.
\end{itemize}

\noindent A conforming implementation MUST populate all MUST fields and SHOULD populate all SHOULD fields. The MUST set is minimal: removing any MUST field renders at least one audit question from Section~\ref{sec:ablation} unanswerable.

\subsection{W3C PROV Integration}
\label{sec:prov}

Each experimental group (defined by a unique model--task--condition--abstract combination) is automatically translated into a W3C PROV-JSON document \cite{w3cprov2013} that expresses the generation provenance as a directed graph. The mapping defines:

\begin{itemize}
    \item \textbf{Entities}: Prompt, InputText, ModelVersion, InferenceParameters, Output, ExecutionMetadata
    \item \textbf{Activities}: RunGeneration (the inference execution)
    \item \textbf{Agents}: Researcher, SystemExecutor (the execution environment)
\end{itemize}

PROV relations capture the causal structure:
\begin{itemize}
    \item \texttt{used}: RunGeneration used Prompt, InputText, ModelVersion, InferenceParameters
    \item \texttt{wasGeneratedBy}: Output wasGeneratedBy RunGeneration
    \item \texttt{wasAssociatedWith}: RunGeneration wasAssociatedWith Researcher, SystemExecutor
    \item \texttt{wasAttributedTo}: Output wasAttributedTo Researcher
    \item \texttt{wasDerivedFrom}: Output wasDerivedFrom InputText
\end{itemize}

This standardized representation enables automated reasoning about experiment provenance, including detecting when two runs share identical configurations and identifying the specific factors that differ between non-identical outputs. The choice of W3C PROV over plain JSON logs is deliberate: PROV's formal semantics allow automated tools to traverse the provenance graph and answer queries such as ``what changed between these two runs?'' without custom parsing logic. An abbreviated example document is given in Appendix~\ref{app:prov-example}; to illustrate the structure concisely, the core provenance chain is:

\smallskip
\noindent
\texttt{Prompt} $\longrightarrow$\textsuperscript{\scriptsize used} \textbf{RunGeneration} $\longrightarrow$\textsuperscript{\scriptsize generated} \texttt{Output} \\
\texttt{InputText} $\longrightarrow$\textsuperscript{\scriptsize used} \textbf{RunGeneration} $\longrightarrow$\textsuperscript{\scriptsize assoc.} \texttt{Researcher} \\
\texttt{ModelVersion} $\longrightarrow$\textsuperscript{\scriptsize used} \textbf{RunGeneration}; \quad \texttt{Output} $\longrightarrow$\textsuperscript{\scriptsize derived} \texttt{InputText}
\smallskip

\subsection{Reproducibility Checklist}
\label{sec:checklist}

We provide a 15-item checklist organized into four categories---Prompt Documentation, Model and Environment, Execution and Output, and Provenance---that researchers can use to self-assess the reproducibility of their generative AI studies. The complete checklist is provided in Appendix~\ref{app:checklist}.

\subsection{Extensions for Advanced Workflows}
\label{sec:extensions}

The protocol's field schema accommodates complex workflows through optional extension fields. Our empirical evaluation exercises two of these extensions---multi-turn dialogues and RAG---while the remaining extensions are specified in the reference implementation's schema:

\begin{itemize}
    \item \textbf{Multi-turn dialogues:} A \texttt{conversation\_history\_hash} field and \texttt{turn\_index} enable linking each turn to the full conversation state. \textit{Evaluated in Task~3 (multi-turn refinement) using Ollama's \texttt{/api/chat} endpoint.}
    \item \textbf{RAG:} Fields for retrieval context (with hashes) trace which external information influenced the output. \textit{Evaluated in Task~4 (RAG extraction) with prepended context passages.}
    \item \textbf{Tool use and function calling:} Fields for available tools, tool calls (with arguments, results, and hashes) capture the full tool-use chain.
    \item \textbf{Chain-of-thought / agent workflows:} A \texttt{parent\_run\_id} field supports hierarchical provenance graphs for multi-step reasoning chains.
\end{itemize}

\subsection{Formal Definition and Audit Completeness}
\label{sec:formal}

We define the protocol as a tuple $\mathcal{P} = (\mathit{PC}, \mathit{RC}, \mathit{G}, \mathit{CL})$, where $\mathit{PC}$ is a Prompt Card, $\mathit{RC}$ is a Run Card, $G$ is a W3C PROV graph, and $\mathit{CL}$ is the reproducibility checklist. Each Run Card $\mathit{RC}_i$ is itself a tuple of field groups: $\mathit{RC}_i = (\mathit{Id}, \mathit{Mod}, \mathit{Par}, \mathit{IO}, \mathit{Env}, H)$, where $H$ denotes the set of five SHA-256 hashes (prompt, input, parameters, environment, output).

We define an \textit{audit question} as a predicate $Q$ over one or more Run Cards. The protocol satisfies the following \textit{audit completeness} property: for a set of 10 audit questions $\{Q_1, \ldots, Q_{10}\}$ (defined in Section~\ref{sec:ablation}), every $Q_j$ is answerable if and only if all field groups are populated. Formally:

\begin{equation}
\forall Q_j \in \{Q_1, \ldots, Q_{10}\}: \, \text{answerable}(Q_j, \mathit{RC}_i) \;\Leftrightarrow\; \bigwedge_{g \in \text{required}(Q_j)} g \subseteq \mathit{RC}_i
\label{eq:audit-completeness}
\end{equation}

\noindent where $\text{required}(Q_j)$ maps each question to its minimal set of required field groups. The ablation analysis in Section~\ref{sec:ablation} demonstrates that every field group is in the $\text{required}$ set of at least one question, establishing protocol \textit{minimality}: removing any field group violates Equation~\ref{eq:audit-completeness} for at least one $Q_j$.

The \textit{differential diagnosis} property follows from the hash fields: given two Run Cards $\mathit{RC}_a, \mathit{RC}_b$ with $H_{\text{output}}^a \neq H_{\text{output}}^b$, the protocol enables automatic identification of the divergence source by comparing the remaining hashes. If $H_{\text{prompt}}^a = H_{\text{prompt}}^b$, $H_{\text{input}}^a = H_{\text{input}}^b$, $H_{\text{params}}^a = H_{\text{params}}^b$, and $H_{\text{env}}^a = H_{\text{env}}^b$, then the output difference is attributable to non-determinism in the generation process itself---precisely the phenomenon we measure empirically in Section~\ref{sec:results}.

Having defined the protocol's components and formal properties, we now evaluate it empirically along two dimensions: the reproducibility characteristics it reveals across different models and conditions, and the overhead it imposes on the experimental workflow.


%% ============================================================
\section{Experimental Setup}
\label{sec:experimental-setup}

We designed a controlled experiment to simultaneously evaluate (a)~the reproducibility characteristics of LLM outputs under varying conditions and (b)~the overhead imposed by our logging protocol.

\subsection{Models and Infrastructure}

We evaluate seven models representing two fundamentally different deployment paradigms: three locally deployed open-weight models and four cloud API-served models. All local models were served through Ollama v0.15.5 \cite{ollama2024} on an Apple M4 system with 24\,GB unified memory running macOS~14.6 with Python~3.14.3. API-served models span four independent providers: OpenAI (GPT-4), Anthropic (Claude Sonnet 4.5), DeepSeek (DeepSeek Chat), and Perplexity (Sonar, an online model with search augmentation).

\subsubsection{Local Models}

\textbf{LLaMA~3 8B} \cite{grattafiori2024llama3}: An open-weight model in Q4\_0 quantization. Local deployment provides complete control over the execution environment, eliminating confounding factors such as network latency, server-side batching, and silent model updates. The model's SHA-256 weights hash was recorded per run via the Ollama API.

\textbf{Mistral 7B} \cite{jiang2023mistral}: An open-weight model (Q4\_0 quantization) with a sliding-window attention mechanism, providing a second data point for local inference reproducibility at a similar parameter scale.

\textbf{Gemma~2 9B} \cite{team2024gemma}: Google's open-weight model (Q4\_0 quantization), representing a third local model from an independent model family. Gemma~2 proved to be the most deterministic model in our study, though its inference time is substantially higher than the other local models ($\sim$180\,s per run vs.\ 8--14\,s for LLaMA and Mistral), likely due to its larger context window and architectural differences at Q4\_0 quantization on the M4 chip.

\subsubsection{API-Served Models}

\textbf{GPT-4} \cite{achiam2023gpt4}: Accessed via the OpenAI API (\texttt{openai} Python SDK v1.59.9) with controlled seed parameters. The API returned \texttt{gpt-4-0613} as the resolved model version in all runs. The API introduces additional sources of variability: load balancing, server-side batching, potential model-version updates, and floating-point non-determinism across different hardware.

\textbf{Claude Sonnet 4.5} \cite{anthropic2024claude}: Accessed via the Anthropic API using a lightweight \texttt{urllib}-based runner (no SDK dependency). Claude's API does not support a \texttt{seed} parameter; we set \texttt{temperature=0} for greedy decoding and logged a seed value for protocol parity (marked as \texttt{logged-only-not-sent-to-api}). This provides an independent replication of the API non-determinism phenomenon on a second cloud provider.

\subsection{Tasks}

We evaluate four tasks that span the output-structure spectrum and interaction complexity:

\textbf{Task 1: Scientific Summarization.} Given a scientific abstract, produce a concise summary in exactly three sentences covering the main contribution, methodology, and key quantitative result. This is an open-ended generation task in which the model has considerable freedom in word choice and phrasing.

\textbf{Task 2: Structured Extraction.} Given a scientific abstract, extract five fields (objective, method, key\_result, model\_or\_system, benchmark) into a JSON object. This is a constrained generation task in which the output format is fixed and the model must select, rather than generate, content.

\textbf{Task 3: Multi-turn Refinement.} A three-turn dialogue in which the model first extracts structured information, then receives feedback requesting more detail, and finally produces a refined extraction. This tests reproducibility under conversational state accumulation, using Ollama's \texttt{/api/chat} endpoint for local models.

\textbf{Task 4: RAG Extraction.} The same structured extraction task as Task~2, but with an additional retrieved context passage prepended to the input. This tests whether augmenting the prompt with external context affects reproducibility.

\subsection{Input Data}

We use 30 widely-cited scientific abstracts from landmark AI/ML papers, including \citet{vaswani2017attention} (Transformer), \citet{devlin2019bert} (BERT), \citet{brown2020language} (GPT-3), \citet{raffel2020exploring} (T5), \citet{wei2022chain} (Chain-of-Thought), as well as seminal works on GANs, ResNets, VAEs, LSTMs, CLIP, DALL-E~2, Stable Diffusion, LLaMA, InstructGPT, PaLM, and others. These abstracts vary in length (74--227 words), technical complexity, and the number of quantitative results reported, thereby providing substantial diversity in the generation challenge.

\subsection{Experimental Conditions}

We define five conditions (Table~\ref{tab:experimental-design}) that systematically vary the factors hypothesized to affect reproducibility:

%% Table 1 - Experimental Design
\begin{table}[t]
\centering
\caption{Experimental design: conditions, parameters, and expected outcomes.}
\label{tab:experimental-design}
\small
\begin{tabular}{@{}llcccl@{}}
\toprule
\textbf{Cond.} & \textbf{Description} & \textbf{Temp.} & \textbf{Seed} & \textbf{Reps} & \textbf{Expected Outcome} \\
\midrule
C1 & Fixed seed, greedy & 0.0 & 42 (fixed) & 5 & Deterministic output \\
C2 & Variable seeds, greedy & 0.0 & 5 different & 5 & Near-deterministic \\
C3$_{t{=}0.0}$ & Temp.\ baseline & 0.0 & per-rep & 3 & Deterministic \\
C3$_{t{=}0.3}$ & Low temperature & 0.3 & per-rep & 3 & Low variability \\
C3$_{t{=}0.7}$ & High temperature & 0.7 & per-rep & 3 & High variability \\
\bottomrule
\end{tabular}

\par\smallskip\raggedright\footnotesize
\textit{Note:} Tasks~1--2 are evaluated under all five conditions (C1, C2, C3) for the original seven models, plus C1 for DeepSeek Chat and Perplexity Sonar. Tasks~3--4 (multi-turn, RAG) are evaluated under C1 only for the three local models and Claude Sonnet 4.5. Total: 3,804 logged runs across 7~models. For API-served models, C2 uses the same fixed seed as C1; the seed parameter is advisory and does not guarantee determinism.
\end{table}

\textbf{Design principle for API models.} For cloud-hosted APIs whose \texttt{seed} parameter is advisory rather than deterministic (as documented by OpenAI for GPT-4 \cite{openai2024seed}) or entirely absent (as with Claude), the fixed-vs.-variable seed distinction has no guaranteed effect server-side. We therefore treat C2 as the primary test of determinism under greedy decoding for such models.

\textbf{C1 (Fixed seed, greedy decoding):} Temperature = 0, seed = 42 for all 5 repetitions. This represents the maximum-control condition and should yield deterministic outputs.

\textbf{C2 (Variable seeds, greedy decoding):} Temperature = 0, seeds = \{42, 123, 456, 789, 1024\}. This condition tests whether seed variation affects outputs when greedy decoding is used.

\textbf{C3 (Temperature sweep):} Three sub-conditions at $t \in \{0.0, 0.3, 0.7\}$ with 3 repetitions each, using different seeds per repetition. This condition characterizes how temperature affects output variability.

\textbf{Run counts.} For Tasks~1--2 (extraction and summarization), each of the original seven models is evaluated under C1 (5 runs), C2 (5 runs), and C3 (9 runs = 3 temperatures $\times$ 3 reps) per abstract. LLaMA~3 uses 30~abstracts (1,140 runs); the newer models (Mistral 7B, Gemma~2 9B, Claude Sonnet 4.5) use 10~abstracts (380 runs each). For GPT-4, quota exhaustion limited collection to 724 runs (C2: 300/300; C3: 416/450; C1: 8/300 excluded). DeepSeek Chat and Perplexity Sonar are evaluated under C1 with 10 abstracts $\times$ 5 reps $\times$ 2 tasks = 100 runs each (200 runs total). For Tasks~3--4 (multi-turn and RAG), the three local models and Claude Sonnet 4.5 are evaluated under C1 with 10 abstracts $\times$ 5 repetitions = 50 runs each (400 runs total). \textbf{Grand total: 3,804 valid runs.}

Table~\ref{tab:run-summary} summarizes the per-model run distribution.

\begin{table}[h]
\centering
\caption{Run distribution across models and tasks.}
\label{tab:run-summary}
\small
\begin{tabular}{@{}lrrr@{}}
\toprule
\textbf{Model} & \textbf{Tasks 1--2} & \textbf{Tasks 3--4} & \textbf{Total} \\
\midrule
LLaMA~3 8B & 1,140 & 100 & 1,240 \\
Mistral 7B & 380 & 100 & 480 \\
Gemma~2 9B & 380 & 100 & 480 \\
GPT-4 & 724 & --- & 724 \\
Claude Sonnet 4.5 & 380 & 100 & 480 \\
DeepSeek Chat & 100 & --- & 100 \\
Perplexity Sonar & 100 & --- & 100 \\
\addlinespace
Chat-format control\textsuperscript{\dag} & 200 & --- & 200 \\
\midrule
\textbf{Total} & \textbf{3,404} & \textbf{400} & \textbf{3,804}\footnotemark \\
\bottomrule
\end{tabular}
\end{table}
\footnotetext{One Claude run (0.03\%) returned an empty output due to API timeout and is excluded from variability metrics.}
{\footnotesize \textsuperscript{\dag}LLaMA~3 8B via \texttt{/api/chat} endpoint (Appendix~\ref{app:chat-control}).}

\subsection{Metrics}
\label{sec:metrics}

We adopt an operational definition of reproducibility at three levels, each mapped to a specific metric:

\begin{itemize}
    \item \textbf{Exact reproducibility} (string-level): Two outputs are identical character-by-character. Measured by \textit{Exact Match Rate (EMR)}.
    \item \textbf{Near reproducibility} (edit-level): Two outputs differ only in minor surface variations (punctuation, whitespace, synonym substitution). Measured by \textit{Normalized Edit Distance (NED)}.
    \item \textbf{Semantic reproducibility} (meaning-level): Two outputs convey the same information despite different phrasing. Measured by \textit{ROUGE-L F1} and \textit{BERTScore F1}.
\end{itemize}

This three-level framework allows us to distinguish between outputs that are bitwise identical (EMR~=~1), textually close (NED~$<$~0.05), and semantically equivalent (ROUGE-L~$>$~0.90). All variability metrics are computed over all $\binom{n}{2}$ unique output pairs within each experimental group (defined by model, task, condition, and abstract):

\textbf{Exact Match Rate (EMR):} The fraction of output pairs that are character-for-character identical. EMR = 1.0 indicates perfect reproducibility; EMR = 0.0 indicates that no two outputs match exactly. With $n = 5$ repetitions per group ($\binom{5}{2} = 10$ pairs), per-abstract EMR values are discrete: $\{0.0, 0.1, \ldots, 1.0\}$; with $n = 3$ (C3 conditions), EMR takes values in $\{0.0, 0.333, 0.667, 1.0\}$. This granularity should be considered when interpreting standard deviations and confidence intervals for small sample sizes.

\textbf{Normalized Edit Distance (NED):} The Levenshtein edit distance \cite{levenshtein1966binary} between each pair, normalized by the length of the longer string. NED = 0.0 indicates identical outputs; higher values indicate greater textual divergence.

\textbf{ROUGE-L F1:} The F1 score based on the longest common subsequence at the word level \cite{lin2004rouge}. This captures semantic similarity even when surface forms differ. ROUGE-L = 1.0 indicates identical word sequences.

Our primary metrics (EMR, NED, ROUGE-L) focus on exact and near reproducibility, which are the most direct measures for our research question. To complement these surface-level metrics, we also compute \textbf{BERTScore F1} \cite{zhang2020bertscore}---an embedding-based semantic similarity metric---for all conditions. BERTScore captures meaning-level equivalence that surface metrics may miss (e.g., paraphrases), providing a fourth perspective on reproducibility. For the structured extraction task, we additionally report \textbf{JSON validity rate}, \textbf{schema compliance rate}, and \textbf{field-level accuracy}, which measure whether outputs are syntactically valid JSON, contain all expected fields, and agree on individual field values across runs, respectively (see Appendix~\ref{app:json-quality} for detailed results).

For protocol overhead, we measure:
\begin{itemize}
    \item \textbf{Logging time}: Wall-clock time spent on hashing, metadata collection, and file I/O, measured separately from inference time.
    \item \textbf{Storage}: Size of each run record (JSON) and total storage for all protocol artifacts.
    \item \textbf{Overhead ratio}: Logging time as a percentage of total execution time.
\end{itemize}

All EMR values in Tables~\ref{tab:emr_greedy} and \ref{tab:multiturn_rag} are accompanied by 95\% bootstrap confidence intervals (10,000 resamples over per-abstract EMR values, percentile method).

%% ============================================================
\section{Results}
\label{sec:results}

\subsection{Reproducibility Under Greedy Decoding}

Table~\ref{tab:emr_greedy} presents the headline result: Exact Match Rates under greedy decoding for all seven models. Table~\ref{tab:three_level} provides the full three-level reproducibility assessment.

%% Table - EMR under greedy
\input{tables/table_emr_greedy.tex}

\input{tables/table_three_level.tex}

\subsubsection{Local Models: Near-Perfect to Perfect Reproducibility}

\textbf{Finding 1: Gemma~2 9B achieves perfect bitwise reproducibility under greedy decoding.} Across all tasks and conditions with $t{=}0$, Gemma~2 9B produces EMR = 1.000 with NED = 0.000---every single output is character-for-character identical across repetitions. This includes not only single-turn extraction and summarization but also multi-turn refinement and RAG extraction.

\textbf{Finding 2: All three local models achieve high reproducibility.} LLaMA~3 8B attains EMR = 0.987 for extraction and 0.947 for summarization; Mistral 7B achieves 0.960 and 0.840, respectively. The small deviations from perfect reproducibility in LLaMA~3 and Mistral 7B appear to be associated with a warm-up effect on the first inference call after model loading, which affects 2--4 of the 10--30 abstracts per model; we hypothesize this reflects GPU cache initialization, though this was not formally tested. Seed variation (C1 vs.\ C2) has \textit{no effect} under greedy decoding for any local model: the model always selects the highest-probability token, making the seed irrelevant.

\subsubsection{API-Served Models: Substantial Hidden Non-Determinism}

\textbf{Finding 3: Both API-served models exhibit substantial non-determinism under greedy decoding, observed independently across two providers.} Under $t{=}0$ with controlled seeds, GPT-4 achieves EMR = 0.443 for extraction and 0.230 for summarization. Claude Sonnet 4.5 is even less deterministic: EMR = 0.190 for extraction and EMR = 0.020 for summarization---meaning that across 10~abstracts $\times$ 5 repetitions, Claude produced the same summarization output only 2\% of the time.

Table~\ref{tab:api_vs_local} summarizes the deployment-paradigm gap.

\input{tables/table_api_vs_local.tex}

Under the representative greedy condition for each model (C1 for local models, Claude, DeepSeek, and Perplexity; C2 for GPT-4; see Table~\ref{tab:emr_greedy}), the average single-turn EMR is \textbf{0.960 for local models} vs.\ \textbf{0.325 for API models}---a 3-fold reproducibility gap. Within API models, reproducibility spans a striking range: DeepSeek Chat achieves the highest (EMR = 0.800 for extraction, 0.760 for summarization), followed by GPT-4 (0.443/0.230), Claude Sonnet 4.5 (0.190/0.020), and Perplexity Sonar (0.100/0.010). This within-API variation reveals that API non-determinism is not uniform across providers. This gap is not due to user-side parameter differences: all models use $t{=}0$ with the same decoding strategy. The observed variability is consistent with deployment-side factors invisible to the researcher. This pattern, observed independently across \textit{four} API providers (OpenAI, Anthropic, DeepSeek, and Perplexity), is consistent with non-determinism arising from factors common to cloud-hosted LLM inference. Per-abstract consistency analysis confirms the local-vs-API gap holds in 100\% of abstracts for summarization and 83\% for extraction. All comparisons survive Holm-Bonferroni correction across 68 hypothesis tests ($\alpha_{\mathrm{adjusted}} < 0.05$). \textit{Without systematic logging, this non-determinism would be entirely invisible.}

\subsubsection{Temperature Effects Across Models}

\textbf{Finding 4: Temperature is the dominant \textit{user-controllable} factor affecting variability for local models; for API-served models, the relationship is more complex.} Figure~\ref{fig:temperature} shows the relationship between temperature and EMR for all seven models. Table~\ref{tab:temp_sweep} provides the full temperature sweep data.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/fig_temp_effect.pdf}
    \caption{Effect of temperature on Exact Match Rate across seven models. (a)~Extraction task. (b)~Summarization task. Local models (solid lines) start from near-perfect or perfect reproducibility at $t{=}0$, while API models (dashed lines) start from a much lower baseline. All models converge toward EMR = 0 at $t{=}0.7$.}
    \Description{Two line plots showing EMR vs temperature for seven models. Local models start high and decrease; API models start low and also decrease.}
    \label{fig:temperature}
\end{figure*}

\input{tables/table_temp_sweep.tex}

Within the C3 temperature sweep, increasing temperature from 0.0 to 0.7 reduces EMR to zero for all models on summarization. For extraction, local models drop from EMR $>$ 0.93 to near zero, while API models drop from their already-low baselines. Notably, BERTScore F1 remains above 0.94 in all conditions (minimum: 0.943 for LLaMA summarization at $t{=}0.7$) even when EMR drops to zero, indicating that non-determinism is primarily a \textit{phrasing} phenomenon rather than a \textit{meaning} phenomenon: even when outputs differ textually, they convey equivalent information. This distinction is practically important---researchers whose downstream analyses depend on semantic content rather than exact wording may find API outputs acceptable despite low EMR.

However, the temperature--reproducibility relationship is not uniformly monotonic across all models. Claude Sonnet 4.5 exhibits an anomalous pattern under the C3 sweep: extraction EMR \textit{increases} from 0.067 at $t{=}0.0$ to 0.700 at $t{=}0.3$ before declining to 0.133 at $t{=}0.7$; summarization shows a similar inversion (EMR = 0.000 at $t{=}0.0$, rising to 0.233 at $t{=}0.3$). This counterintuitive behavior---where a small positive temperature \textit{improves} reproducibility relative to greedy decoding---may reflect how Anthropic's infrastructure implements the $t{=}0$ decoding path: at exactly zero temperature, server-side stochastic processes (e.g., speculative decoding, hardware-level floating-point non-determinism across GPU types, or request batching effects) may dominate output variability, whereas a small positive temperature may activate a more stable sampling path that happens to converge on similar tokens. With $n{=}10$ abstracts and 30 runs per temperature level (standard deviation $\sigma = 0.38$ for the 0.700 extraction EMR), this observation should be interpreted cautiously. Nevertheless, it underscores that the temperature--reproducibility relationship for API-served models depends on provider-specific implementation details that are opaque to researchers. Finding~4 therefore holds robustly for local models and for the overall $t{=}0$ to $t{=}0.7$ trajectory, but the precise shape of the temperature--response curve for individual API providers merits further investigation with larger sample sizes.

\subsection{Multi-Turn and RAG Reproducibility}

\textbf{Finding 5: The local-vs-API reproducibility gap extends to complex interaction regimes.} Table~\ref{tab:multiturn_rag} and Figure~\ref{fig:multiturn} present results for multi-turn refinement and RAG extraction across the three local models and Claude Sonnet 4.5.

\input{tables/table_multiturn_rag.tex}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/fig_multiturn_comparison.pdf}
    \caption{Reproducibility across interaction regimes (C1, $t{=}0$) for four models. Local models maintain high EMR across all scenarios, while Claude Sonnet 4.5 (API) shows near-zero EMR throughout, confirming the reproducibility gap extends to multi-turn and RAG tasks.}
    \Description{Grouped bar chart showing EMR for three local models and Claude across four scenarios: single-turn extraction, summarization, multi-turn refinement, and RAG extraction.}
    \label{fig:multiturn}
\end{figure}

Gemma~2 9B and Mistral 7B achieve perfect EMR = 1.000 for both multi-turn refinement and RAG extraction, demonstrating that conversational state accumulation and context augmentation do not degrade reproducibility when the underlying model is deterministic. LLaMA~3 8B shows EMR = 0.880 for multi-turn and 0.960 for RAG---slightly lower than its single-turn extraction performance (0.987), consistent with error accumulation across dialogue turns.

Claude Sonnet 4.5, the only API-served model evaluated on these tasks, achieves EMR = 0.040 for multi-turn refinement and EMR = 0.000 for RAG extraction---the lowest values observed in our study. The RAG result is particularly striking: across 50~runs (10~abstracts $\times$ 5~repetitions), not a single pair of outputs was character-for-character identical (NED = 0.256). This confirms that API non-determinism is not limited to single-turn tasks but persists---and may even worsen---under complex interaction regimes where longer outputs and additional context amplify server-side variability.

\subsection{Cross-Model Comparison}

Figure~\ref{fig:heatmap} provides a comprehensive heatmap of EMR across all model-task combinations, and Figure~\ref{fig:radar} shows the three-level reproducibility profile for each model.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/fig_emr_heatmap.pdf}
    \caption{Heatmap of Exact Match Rate under greedy decoding for seven models. The horizontal white line separates local models (top three, green) from API-served models (bottom two, red). Gemma~2 9B achieves perfect 1.000 across all tasks.}
    \Description{A heatmap with 5 rows (models) and 2 columns (tasks), using a red-yellow-green colormap where green indicates high EMR and red indicates low EMR.}
    \label{fig:heatmap}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/fig_three_level_radar.pdf}
    \caption{Three-level reproducibility profiles under greedy decoding. Local models (solid lines) occupy the outer region across all five metrics, while API models (dashed lines) show pronounced deficits in EMR and NED while maintaining high BERTScore, indicating that API non-determinism is primarily lexical rather than semantic.}
    \Description{Radar chart with five axes showing EMR extraction, EMR summarization, 1-NED, ROUGE-L, and BERTScore for seven models.}
    \label{fig:radar}
\end{figure}

The reproducibility gap between local and API-based inference is statistically significant. Because per-abstract EMR is a bounded, discrete metric (taking values in $\{0.0, 0.1, \ldots, 1.0\}$ with $n{=}5$ repetitions per group), we report the non-parametric Wilcoxon signed-rank test as our primary analysis. Across the 30 paired LLaMA~3/GPT-4 abstracts under greedy decoding: for summarization, $W = 0$, $p < 0.001$; for extraction, $W = 3.5$, $p < 0.001$. Parametric paired $t$-tests yield consistent results: summarization $t(29) = 17.250$, $p < 0.0001$, Cohen's $d = 3.149$; extraction $t(29) = 8.996$, $p < 0.0001$, Cohen's $d = 1.642$. Both effect sizes are very large ($d > 1.6$), and all $p$-values survive Bonferroni correction for the four primary comparisons ($\alpha_{\text{adjusted}} = 0.0125$).

Importantly, the effect is not driven by a few outlier abstracts: under greedy decoding, LLaMA~3 achieves EMR $\geq$ 0.8 for 29 of 30 abstracts in extraction and 28 of 30 in summarization, while GPT-4 achieves EMR $\leq$ 0.6 for 20 of 30 abstracts in extraction and 28 of 30 in summarization. The gap is pervasive across the abstract set, not concentrated in a few difficult inputs. Power analysis \cite{cohen1988statistical} confirms that with $n = 30$ paired abstracts and the observed effect sizes ($d > 1.6$), statistical power exceeds 0.999 for all primary comparisons; with $n = 10$ abstracts (as used for the newer models), power remains above 0.95 for effects of this magnitude.

\subsection{Protocol Overhead}

Table~\ref{tab:overhead} presents the protocol's overhead metrics across all seven models.

\input{tables/table_overhead.tex}

The protocol adds less than 1\% overhead for all seven models, with mean logging time ranging from 21--30\,ms depending on the model and task. Storage overhead remains modest at approximately 4\,KB per run record. The overhead is consistent across local and API deployment modes, indicating that the protocol is deployment-agnostic.

Figure~\ref{fig:ned} provides an additional perspective on surface-level variability across models.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/fig_ned_comparison.pdf}
    \caption{Normalized Edit Distance (NED) under greedy decoding. Local models show near-zero NED (Gemma~2: 0.000, Mistral: 0.001), while API models exhibit NED 0.07--0.30, quantifying the surface-level divergence that accompanies the EMR gap.}
    \Description{Grouped bar chart showing NED for seven models across extraction and summarization tasks.}
    \label{fig:ned}
\end{figure}


%% ============================================================
\section{Discussion}
\label{sec:discussion}

The preceding results paint a clear and consistent picture: locally deployed models under greedy decoding achieve near-perfect to perfect bitwise reproducibility across all four tasks, while API-served models---from two independent providers---exhibit substantial hidden variability on single-turn tasks that researchers cannot control. Temperature is the dominant user-controllable factor for local models (though API models show a more complex temperature--reproducibility relationship; see Section~\ref{sec:results}), structured tasks are more reproducible than open-ended ones, and complex interaction regimes (multi-turn, RAG) do not degrade local-model reproducibility. We now consider what these findings mean for research practice, what the protocol enables that was previously invisible, and where the current study's limitations lie.

\subsection{Implications for Reproducibility Practice}

Our results yield several actionable recommendations for researchers conducting generative AI experiments:

\textbf{Use greedy decoding with local models for maximum reproducibility.} Gemma~2 9B achieved \textit{perfect} EMR = 1.000 across all tasks under greedy decoding. LLaMA~3 and Mistral 7B achieved EMR $\geq$ 0.840. Local deployment with $t{=}0$ should be the default configuration for any study in which output consistency is critical.

\textbf{API non-determinism is observed across providers.} Our most consequential finding is that \textit{both} GPT-4 (OpenAI) and Claude Sonnet 4.5 (Anthropic) exhibit substantial non-determinism under greedy decoding on single-turn tasks. Claude's EMR of 0.020 for summarization means that effectively no two runs produce the same output. Researchers using \textit{any} API-served model should never assume reproducibility without verification and should report multiple runs with variability metrics.

\textbf{Prefer structured output formats when possible.} The extraction task's consistently higher reproducibility across all seven models demonstrates that output-format constraints directly improve reproducibility. This effect holds for both local models (EMR 0.960--1.000 for extraction vs.\ 0.840--1.000 for summarization) and API models (EMR 0.190--0.443 for extraction vs.\ 0.020--0.230 for summarization).

\textbf{Include warm-up runs for local models.} The per-abstract analysis revealed that the first inference call after model loading may differ from subsequent calls due to cache initialization. This affects LLaMA~3 and Mistral 7B on 2--4 of their abstracts, slightly reducing aggregate EMR.

\textbf{Log comprehensively; the cost is negligible.} At less than 1\% overhead and approximately 4\,KB per run across all seven models, there is no practical reason not to apply comprehensive logging. The cost of not logging---namely, the inability to detect the kind of pervasive API non-determinism documented herein---far exceeds the protocol's minimal requirements.

\subsection{Local vs.\ API Inference: A Persistent Reproducibility Gap}

The most significant finding of this study is the reproducibility gap between local and API-based inference, observed consistently across the two independent cloud providers evaluated. Under greedy decoding on single-turn tasks, local models average EMR = 0.956 while API models average EMR = 0.221---a more than 4-fold gap. The fact that Claude Sonnet 4.5 (Anthropic) exhibits \textit{even lower} reproducibility than GPT-4 (OpenAI, snapshot \texttt{gpt-4-0613}) is inconsistent with provider-specific implementation as the sole explanation and suggests that non-determinism arises from factors common to distributed cloud inference infrastructure, such as hardware-level floating-point variability, request batching, and model routing. We emphasize that this gap is documented for the specific model versions and API snapshots evaluated; whether it generalizes to other providers, model families, or future API implementations is an open question that our protocol is designed to help answer systematically.

This gap has practical implications for the scientific use of the API-based LLMs evaluated. \textit{Without systematic logging, a researcher using the models and configurations tested in our study would have no way of knowing that their ``deterministic'' experiment produces different outputs across runs.} Our protocol makes this hidden non-determinism visible, measurable, and documentable---and provides the infrastructure for researchers to assess whether the pattern holds for their specific models and configurations.

\subsection{Task-Dependent Reproducibility}

The difference between summarization and extraction reproducibility---observed consistently across all seven models---is consistent with and extends our earlier two-model finding. The reproducibility hierarchy (extraction $>$ summarization) holds for local models (EMR gap of 0.03--0.12) and is amplified for API models (EMR gap of 0.17--0.25). This finding suggests a spectrum ranging from highly constrained tasks (structured extraction) to open-ended tasks (summarization), with the degree of output-space constraint serving as a primary determinant.

\subsection{Multi-Turn and RAG: Reproducibility Under Complexity}

Our multi-turn and RAG results address a key limitation of prior work (including our own earlier two-model study): reproducibility under complex interaction regimes. The finding that Gemma~2 9B and Mistral 7B maintain perfect EMR = 1.000 for both multi-turn refinement and RAG extraction demonstrates that conversational state accumulation and context augmentation do not inherently degrade reproducibility for deterministic local models. LLaMA~3's slight degradation (EMR = 0.880 for multi-turn) suggests model-specific sensitivity to dialogue-turn interactions, possibly related to the warm-up effect observed in single-turn experiments. Crucially, Claude Sonnet 4.5's near-zero EMR for both multi-turn (0.040) and RAG (0.000) confirms that the local-vs-API reproducibility gap extends beyond single-turn tasks. The RAG result---zero exact matches across 50~runs---suggests that longer outputs and additional retrieval context may amplify server-side variability, though a single API model cannot establish this as a general principle.

\subsection{The Role of Provenance}

The W3C PROV graphs generated by our protocol serve multiple purposes beyond simple audit trails:

\begin{enumerate}
    \item \textbf{Automated comparison}: By comparing PROV graphs of two runs, one can automatically identify which factors differed (e.g., same prompt and model but different temperatures), enabling systematic diagnosis of non-reproducibility.
    \item \textbf{Lineage tracking}: When outputs are used as inputs to downstream processes (e.g., summarization outputs fed into a meta-analysis), the provenance chain can be extended to trace any final result back to its full generation context.
    \item \textbf{Compliance}: For regulated domains (healthcare, legal, finance), PROV documents provide the formal evidence trail required by audit standards \cite{nist2023ai} and emerging regulations such as the EU AI Act \cite{euaiact2024}.
\end{enumerate}

To illustrate the diagnostic power of PROV graphs, consider two GPT-4 extraction runs on the same abstract under condition C2 (greedy decoding, $t{=}0$, same seed). Although the PROV entities for Prompt, InputText, ModelVersion, and InferenceParameters are identical (verified via matching SHA-256 hashes), the Output entities differ: \texttt{output\_hash} values diverge, and the \texttt{wasGeneratedBy} timestamps differ by several seconds. The PROV graph thus automatically pinpoints the source of non-reproducibility: the only varying factor is the RunGeneration activity itself, consistent with non-determinism arising from server-side factors.

To demonstrate that PROV-based reasoning goes beyond what plain JSON logs provide, we implemented three programmatic queries over our 3,804-run PROV dataset:

\begin{enumerate}
    \item \textbf{Divergence attribution}: ``For all abstract--condition groups with non-identical outputs, identify which PROV entities diverge.'' Result: across divergent groups for all four API models (GPT-4, Claude, DeepSeek, Perplexity), 100\% share identical Prompt, InputText, ModelVersion, and InferenceParameters entities---the \textit{only} varying component is the RunGeneration activity, providing systematic evidence for server-side non-determinism across the entire dataset rather than anecdotal examples.

    \item \textbf{Cross-provider comparison}: ``Find all abstract--task pairs where multiple API models were given identical Prompt and InputText entities (verified by matching \texttt{genai:hash} attributes) but produced different Output entities.'' Result: across 10~abstracts $\times$ 2~tasks, all four API providers produced non-identical outputs across repetitions on shared inputs, confirming provider-independent non-determinism---though with varying severity (DeepSeek Chat showing the least variability).

    \item \textbf{Provenance chain traversal}: ``Starting from any Output entity, traverse \texttt{wasGeneratedBy} $\rightarrow$ \texttt{used} relations to reconstruct the full generation context, then verify integrity via hash comparison.'' This query validates that every output in our dataset can be traced back to its complete generation context with no broken links---a guarantee that plain JSON logs cannot provide without custom graph-traversal code.
\end{enumerate}

These queries exploit PROV's formal graph structure (entity--activity--agent relations with typed edges) to answer questions that would require bespoke parsing logic on unstructured JSON logs. The queries and their results are included in the project repository.

\subsection{Pipeline Threat Model}
\label{sec:threat-model}

A natural objection is whether the observed output variability in API-served models could originate from our client-side pipeline rather than from server-side non-determinism. We address this systematically.

\textbf{No retries or parallelism.} Our API runners issue exactly one HTTP request per run, with no retry logic, exponential backoff, or concurrent requests. Each run is executed sequentially with a fixed delay between calls. Any request that fails (e.g., the single Claude timeout) is logged with the error and excluded from variability metrics rather than retried.

\textbf{Deterministic client-side processing.} All pre-processing (prompt construction, input hashing) and post-processing (output hashing, metadata collection) are deterministic operations verified by SHA-256 hashes. The Run Card records the exact prompt text sent (\texttt{prompt\_hash}), the exact input (\texttt{input\_hash}), and the exact parameters (\texttt{params\_hash}). For any pair of runs within a group, these three hashes are identical by construction.

\textbf{No text normalization.} Outputs are stored and compared as received from the API, with no whitespace normalization, encoding conversion, or post-processing. The \texttt{output\_hash} is computed on the raw response string.

\textbf{PROV-based differential diagnosis.} Our PROV graphs provide formal evidence: across all experimental groups with non-identical outputs for the four API models (GPT-4, Claude, DeepSeek, Perplexity), 100\% share identical Prompt, InputText, ModelVersion, and InferenceParameters entities (verified via SHA-256 hash comparison). The \textit{only} varying component is the RunGeneration activity itself. This rules out client-side divergence as an explanation and is consistent with server-side factors (hardware-level floating-point variability, request routing, speculative decoding) as the source of non-determinism.

\textbf{API metadata logging.} For API-served models, Run Cards capture \texttt{api\_request\_id}, \texttt{api\_response\_headers}, and \texttt{api\_model\_version\_returned}. In all GPT-4 runs, the returned model version was consistently \texttt{gpt-4-0613}, ruling out silent model updates during the experiment window.

\subsection{Limitations}
\label{sec:limitations}

We organize threats to validity following standard categories:

\subsubsection{Internal Validity}

\textbf{Sample size.} LLaMA~3 uses 30~abstracts per condition, while the newer models (Mistral, Gemma~2, Claude) use 10~abstracts. With $n = 30$, statistical power exceeds 0.999 for all primary comparisons \cite{cohen1988statistical}. With $n = 10$, the study is adequately powered for the large observed effect sizes ($d > 1.6$) but may miss subtler effects. To verify that the unbalanced design does not inflate the local-vs-API gap, we conducted a balanced subsample analysis restricting all models to the same 10~abstracts. Under this balanced comparison, local models average EMR = 0.953 while API models average EMR = 0.190 (5.0$\times$ gap), confirming that the observed reproducibility gap is robust to sample-size equalization and, if anything, slightly larger under balanced conditions.

\textbf{GPT-4 C3 incomplete coverage.} Due to API quota exhaustion, GPT-4 extraction under C3 conditions covers 14--17 of 30~abstracts (summarization C3 is complete at 30). Our central claims rest on the C2 condition (300/300 runs complete), and the C3 temperature sweep serves as a secondary analysis.

\textbf{Warm-up confound.} The first inference after model loading may differ from subsequent calls for LLaMA~3 and Mistral 7B. This affects 2--4 abstracts per model, slightly reducing aggregate EMR. Gemma~2 9B appears immune to this effect.

\textbf{Prompt format confound.} Single-turn experiments use Ollama's \texttt{/api/generate} endpoint for local models, whereas API models use their respective chat APIs. A supplementary control experiment (200 additional runs using Ollama's \texttt{/api/chat} endpoint; see Appendix~\ref{app:chat-control}) shows that this format difference does not explain the reproducibility gap: LLaMA~3 produces \emph{identical} variability metrics (summarization EMR = 0.929, extraction EMR = 1.000) under both completion and chat formats.

\subsubsection{External Validity}

\textbf{Seven models, four providers.} Our evaluation covers three local models and four API-served models from independent providers (OpenAI, Anthropic, DeepSeek, Perplexity). DeepSeek Chat notably achieves substantially higher reproducibility than other API models (EMR = 0.800 vs.\ 0.100--0.443), suggesting that API non-determinism varies meaningfully across providers and architectures. Perplexity Sonar, as an online model with search augmentation, represents a worst case for reproducibility (EMR = 0.010--0.100), where real-time web data injection introduces additional variability. However, other models---including Gemini \cite{geminiteam2024gemini}, larger LLaMA variants, and open-weight models served via cloud APIs---may exhibit different characteristics. Notably, our GPT-4 experiments used the \texttt{gpt-4-0613} snapshot (June 2023); more recent models (GPT-4 Turbo, GPT-4o) may exhibit different reproducibility characteristics.

\textbf{Four tasks.} Our task suite now includes single-turn extraction/summarization, multi-turn refinement, and RAG extraction. However, it does not cover code generation, mathematical reasoning, or creative writing, which may exhibit different reproducibility patterns.

\textbf{English-only, single domain.} Our input data consists of 30 English scientific abstracts from AI/ML papers. Reproducibility characteristics may differ for other languages, domains, or document types.

\textbf{Multi-turn limited to one API model.} Multi-turn and RAG experiments include Claude Sonnet 4.5 as the sole API representative; GPT-4 was not evaluated on Tasks~3--4 due to quota exhaustion. While Claude's near-zero EMR is consistent with the single-turn API pattern, other API providers may exhibit different multi-turn reproducibility characteristics.

\subsubsection{Construct Validity}

\textbf{Surface-level metrics.} Our metrics (EMR, NED, ROUGE-L) capture textual rather than semantic similarity. Two outputs that are semantically equivalent but syntactically different will register as non-matching under EMR and partially divergent under NED. This is by design---our focus is on \textit{exact} reproducibility---but it means our results may overstate the practical impact of non-determinism for downstream applications where semantic equivalence suffices.

\subsubsection{Other Considerations}

\textbf{Privacy.} The protocol's environment metadata includes the machine hostname, which may reveal institutional information. Deployments in privacy-sensitive settings should anonymize this field.

\textbf{Computational cost.} The total cost was modest: approximately 8 GPU-hours on a consumer laptop (Apple M4, 24\,GB) for 2,000 local-model runs (including multi-turn and RAG experiments), plus 1,404 API calls to GPT-4, Claude, DeepSeek, and Perplexity. The carbon footprint is negligible at this scale, and the logging overhead ($<$30\,ms per run) would not materially increase energy consumption even at thousands of runs.

\subsection{Protocol Minimality: An Ablation Analysis}
\label{sec:ablation}

To substantiate our claim that the protocol captures a \textit{minimal} set of metadata, we conducted an ablation analysis in which we systematically removed each field group from the protocol schema and assessed which audit questions became unanswerable. We defined 10 audit questions that a reproducibility-oriented researcher might ask (e.g., ``Can we verify the exact prompt used?'', ``Can we detect output tampering?'', ``Can we trace full provenance?'') and mapped each to the protocol fields required to answer it. For this analysis, we decomposed the Run Card's five sections into eight finer-grained field groups by separating cross-cutting concerns: Identification, Model Context, Parameters, Input Content, Output Content, Hashing (all SHA-256 digests), Environment, and Overhead (timing and storage metadata).

The results show that removing \textit{any} of these eight field groups renders at least one audit question unanswerable, demonstrating that no group is redundant. The Hashing group (SHA-256 hashes for prompts, inputs, outputs, parameters, and environment) has the highest information density: its removal affects 6 of 10 questions despite contributing only 410\,bytes per run. Conversely, the Overhead group (logging time metadata) is the least connected but remains necessary for overhead assessment. The complete ablation results are available in the project repository.

This analysis demonstrates that the protocol is \textit{minimal} in the sense that every field group is necessary for at least one audit capability, while the total overhead remains at approximately 4,052 bytes per run.

\subsection{Practical Costs and Adoption}

One concern with any new protocol is whether the adoption burden is justified. We address this concretely:

\begin{itemize}
    \item \textbf{Implementation effort}: Our reference implementation adds approximately 600 lines of Python (the protocol core) to an existing workflow. Integration requires 3--5 function calls per run.
    \item \textbf{Runtime cost}: $<$30\,ms per run across all seven models, negligible compared to inference times of seconds to minutes for typical LLM calls.
    \item \textbf{Storage cost}: $\sim$4\,KB per run. Our 3,804 runs total approximately 14\,MB---less than a single model checkpoint.
    \item \textbf{Learning curve}: The protocol uses standard JSON and W3C PROV, requiring no specialized knowledge beyond basic Python.
\end{itemize}

Against these modest costs, the protocol provides complete audit trails, automated provenance graphs, tamper-detectable outputs via cryptographic hashing, and structured metadata that enable systematic reproducibility analysis.

\subsection{Minimum Reporting Checklist for Generative AI Studies}

Based on our findings and the protocol design, we recommend that researchers conducting generative AI experiments report, at minimum, the following five items (the full 15-item checklist is provided in Appendix~\ref{app:checklist}):

\begin{enumerate}
    \item \textbf{Model identity and version}: Exact model name, version string, and---for local models---weights hash.
    \item \textbf{Inference parameters}: Temperature, seed, top\_p, top\_k, max\_tokens, and decoding strategy. For APIs where the seed is advisory or unsupported, this should be stated explicitly.
    \item \textbf{Reproducibility metrics over multiple runs}: Report at least EMR (or an equivalent exact-match metric) and one semantic metric (e.g., BERTScore) over $\geq$3 repetitions per condition. A single run is insufficient to characterize output stability.
    \item \textbf{Environment and deployment mode}: Whether inference was local or API-based, and the execution environment (hardware, OS, library versions).
    \item \textbf{Output hashes}: SHA-256 or equivalent cryptographic hashes of outputs, enabling tamper detection and automated comparison across studies.
\end{enumerate}

Run Cards generated by our protocol automatically capture all five items, providing a machine-readable record that satisfies this checklist with no additional effort from the researcher.


%% ============================================================
\section{Conclusion}
\label{sec:conclusion}

We presented a lightweight protocol for logging, versioning, and provenance tracking of generative AI experiments, introducing Prompt Cards and Run Cards as novel documentation artifacts and adopting the W3C PROV data model for machine-readable provenance graphs. Through 3,804 controlled experiments with seven models---three locally deployed (LLaMA~3 8B, Mistral 7B, Gemma~2 9B) and four API-served (GPT-4, Claude Sonnet 4.5, DeepSeek Chat, Perplexity Sonar)---across four NLP tasks and 30 scientific abstracts, we demonstrated six key findings:

\begin{enumerate}
    \item \textbf{API non-determinism is consistent across all four providers evaluated.} All API models---GPT-4 (OpenAI), Claude Sonnet 4.5 (Anthropic), DeepSeek Chat (DeepSeek), and Perplexity Sonar (Perplexity)---exhibit non-determinism under greedy decoding (average API EMR = 0.325), while all three local models achieve average EMR = 0.960. This 3-fold reproducibility gap, observed independently across four cloud providers, is confirmed by Holm-Bonferroni correction across 68 hypothesis tests and per-abstract consistency analysis.

    \item \textbf{API reproducibility varies substantially across providers.} Within the API category, EMR ranges from 0.800 (DeepSeek Chat) to 0.010 (Perplexity Sonar for summarization), revealing that API non-determinism is not a uniform phenomenon. DeepSeek Chat achieves notably higher reproducibility than other API models, while Perplexity's online search-augmented model represents a worst case.

    \item \textbf{Local models can achieve perfect bitwise reproducibility.} Gemma~2 9B attains EMR = 1.000 across all four tasks under greedy decoding---every output is character-for-character identical across repetitions.

    \item \textbf{The local-vs-API gap extends to complex interaction regimes.} Multi-turn refinement and RAG extraction achieve EMR $\geq$ 0.880 for all local models (Gemma~2 9B and Mistral 7B: perfect EMR = 1.000), while Claude Sonnet 4.5---the only API model tested on these tasks---achieves EMR = 0.040 (multi-turn) and EMR = 0.000 (RAG).

    \item \textbf{Temperature is the dominant user-controllable factor for local models.} Increasing from $t{=}0.0$ to $t{=}0.7$ reduces EMR to zero for all seven models on summarization, while seed variation has no effect under greedy decoding for local models. For API-served models, the temperature--reproducibility relationship is more complex and may be non-monotonic (see Section~\ref{sec:results}).

    \item \textbf{Comprehensive provenance logging adds negligible overhead}: less than 1\% of inference time and approximately 4\,KB per run across all seven models, removing any practical argument against systematic documentation.
\end{enumerate}

These findings carry a broader implication: if the pattern observed across the four API providers and model snapshots in our study generalizes, a substantial portion of published research that relies on API-based LLMs may contain non-reproducible results without the authors' knowledge. Regardless of whether API non-determinism proves universal or provider-specific, the protocol itself provides the infrastructure to detect, measure, and document such variability---making hidden non-determinism visible wherever it occurs. The cost of systematic provenance logging---less than one percent of inference time---is trivially small compared to the cost of publishing non-reproducible science.

Looking ahead, we plan to (i)~extend the model suite to include Gemini \cite{geminiteam2024gemini} and open-weight models served via cloud APIs (e.g., Hugging Face Inference Endpoints) to further disentangle model architecture from deployment infrastructure; (ii)~extend the task coverage to code generation, mathematical reasoning, and agentic workflows; and (iii)~develop automated reproducibility scoring based on provenance graph analysis. Ultimately, we envision a future in which every generative AI output carries a provenance certificate, and reproducibility metrics are reported alongside accuracy as a standard component of empirical evaluation.

The reference implementation, all 3,804 run records, provenance documents, and analysis scripts are publicly available to support adoption and independent verification.


%% ============================================================
\begin{acks}
This work was supported by UTFPR -- Universidade Tecnol\'ogica Federal do Paran\'a. The experiments were conducted using locally deployed open-weight models to ensure full reproducibility of the computational environment.
\end{acks}

%% ============================================================
%% Declarations
%% ============================================================

\section*{Data Availability Statement}
The reference implementation, all 3,804 run records (JSON), PROV-JSON provenance documents, Run Cards, Prompt Cards, input data, analysis scripts, and generated figures are publicly available at:
\begin{center}
\url{https://github.com/Roverlucas/genai-reproducibility-protocol}
\end{center}
The repository includes instructions for reproducing all experiments and regenerating all tables and figures from the raw data.

\section*{Author Contributions}
Following the CRediT (Contributor Roles Taxonomy) framework:
\textbf{Lucas Rover}: Conceptualization, Methodology, Software, Validation, Formal Analysis, Investigation, Data Curation, Writing -- Original Draft, Writing -- Review \& Editing, Visualization, Project Administration.
\textbf{Yara de Souza Tadano}: Supervision, Conceptualization, Methodology, Writing -- Review \& Editing, Project Administration.

\section*{Conflict of Interest}
The authors declare no conflicts of interest. This research was conducted independently at UTFPR with no external funding from commercial AI providers. The use of OpenAI's GPT-4 API was for research evaluation purposes only and does not constitute an endorsement.

\section*{Use of AI-Assisted Tools}
The authors used AI-assisted tools (Claude, Anthropic) during the preparation of this manuscript for language editing, code development support, and data analysis scripting. All AI-generated content was critically reviewed, validated, and revised by the authors, who take full responsibility for the accuracy and integrity of the final manuscript. The scientific design, experimental execution, interpretation of results, and intellectual contributions are entirely the authors' own work.

\printbibliography


%% ============================================================
\clearpage
\appendix

\section{Reproducibility Checklist}
\label{app:checklist}

The following checklist is designed for self-assessment of reproducibility in generative AI studies. Each item maps to a specific field or artifact in our protocol.

\subsection*{Prompt Documentation}
\begin{enumerate}
    \item Is the exact prompt text recorded and versioned? \hfill [Prompt Card: \texttt{prompt\_text}, \texttt{prompt\_hash}]
    \item Are design assumptions and limitations documented? \hfill [Prompt Card: \texttt{assumptions}, \texttt{limitations}]
    \item Is the expected output format specified? \hfill [Prompt Card: \texttt{expected\_output\_format}]
    \item Is the interaction regime documented (single/multi-turn)? \hfill [Prompt Card: \texttt{interaction\_regime}]
\end{enumerate}

\subsection*{Model and Environment}
\begin{enumerate}[resume]
    \item Is the model name and version recorded? \hfill [Run Card: \texttt{model\_name}, \texttt{model\_version}]
    \item Are model weights hashed for identity verification? \hfill [Run Card: \texttt{weights\_hash}]
    \item Is the execution environment fingerprinted? \hfill [Run Card: \texttt{environment}, \texttt{environment\_hash}]
    \item Is the source code version recorded? \hfill [Run Card: \texttt{code\_commit}]
\end{enumerate}

\subsection*{Execution and Output}
\begin{enumerate}[resume]
    \item Are all inference parameters logged? \hfill [Run Card: \texttt{inference\_params}]
    \item Is the random seed recorded? \hfill [Run Card: \texttt{inference\_params.seed}]
    \item Is the output cryptographically hashed? \hfill [Run Card: \texttt{output\_hash}]
    \item Are execution timestamps recorded? \hfill [Run Card: \texttt{timestamp\_start}, \texttt{timestamp\_end}]
    \item Is logging overhead measured separately? \hfill [Run Card: \texttt{logging\_overhead\_ms}]
\end{enumerate}

\subsection*{Provenance}
\begin{enumerate}[resume]
    \item Is a provenance graph generated per group? \hfill [PROV-JSON document]
    \item Are provenance documents in an interoperable format? \hfill [W3C PROV standard]
\end{enumerate}


\section{Run Card Schema}
\label{app:schema}

The complete Run Card schema, with data types and descriptions:

\begin{lstlisting}[caption={Run Card JSON schema (simplified).},label={lst:schema}]
{
  "run_id": "string (unique identifier)",
  "task_id": "string (task identifier)",
  "task_category": "string (e.g., summarization)",
  "prompt_hash": "string (SHA-256 of prompt)",
  "prompt_text": "string (full prompt text)",
  "input_text": "string (input to the model)",
  "input_hash": "string (SHA-256 of input)",
  "model_name": "string (e.g., llama3:8b)",
  "model_version": "string (e.g., 8.0B)",
  "weights_hash": "string (SHA-256 of weights)",
  "model_source": "string (e.g., ollama-local)",
  "inference_params": {
    "temperature": "float",
    "top_p": "float",
    "top_k": "integer",
    "max_tokens": "integer",
    "seed": "integer|null",
    "decoding_strategy": "string"
  },
  "params_hash": "string (SHA-256 of params)",
  "environment": {
    "os": "string",
    "os_version": "string",
    "architecture": "string",
    "python_version": "string",
    "hostname": "string",
    "timestamp": "ISO 8601 datetime"
  },
  "environment_hash": "string (SHA-256)",
  "code_commit": "string (git commit hash)",
  "researcher_id": "string",
  "affiliation": "string",
  "timestamp_start": "ISO 8601 datetime",
  "timestamp_end": "ISO 8601 datetime",
  "output_text": "string (model output)",
  "output_hash": "string (SHA-256 of output)",
  "output_metrics": "object (task-specific)",
  "execution_duration_ms": "float",
  "logging_overhead_ms": "float",
  "storage_kb": "float",
  "system_logs": "string (raw system info)",
  "errors": "array of strings",

  // --- API-specific optional fields ---
  "api_request_id": "string|null (provider request ID)",
  "api_response_headers": "object|null (selected headers)",
  "api_model_version_returned": "string|null",
  "api_region": "string|null (if available)",
  "seed_status": "string (sent|logged-only|not-supported)",

  // --- Multi-turn extension fields ---
  "conversation_history_hash": "string|null (SHA-256)",
  "turn_index": "integer|null",
  "parent_run_id": "string|null",

  // --- RAG extension fields ---
  "retrieval_context": "string|null",
  "retrieval_context_hash": "string|null (SHA-256)"
}
\end{lstlisting}


\section{Example PROV-JSON Document}
\label{app:prov-example}

An abbreviated example of a PROV-JSON document generated for a single summarization run:

\begin{lstlisting}[caption={Abbreviated PROV-JSON for a summarization run.},label={lst:prov}]
{
  "prefix": {
    "genai": "https://genai-prov.org/ns#",
    "prov": "http://www.w3.org/ns/prov#"
  },
  "entity": {
    "genai:prompt_c9644358": {
      "prov:type": "genai:Prompt",
      "genai:hash": "c9644358805b...",
      "genai:task_category": "summarization"
    },
    "genai:model_llama3_8b": {
      "prov:type": "genai:ModelVersion",
      "genai:name": "llama3:8b",
      "genai:source": "ollama-local"
    },
    "genai:output_590d0835": {
      "prov:type": "genai:Output",
      "genai:hash": "590d08359e7d..."
    }
  },
  "activity": {
    "genai:run_llama3_8b_sum_001_C1_rep0": {
      "prov:type": "genai:RunGeneration",
      "prov:startTime": "2026-02-07T21:54:34Z",
      "prov:endTime": "2026-02-07T21:54:40Z"
    }
  },
  "wasGeneratedBy": {
    "_:wGB1": {
      "prov:entity": "genai:output_590d0835",
      "prov:activity": "genai:run_llama3_8b_..."
    }
  },
  "used": {
    "_:u1": {
      "prov:activity": "genai:run_llama3_...",
      "prov:entity": "genai:prompt_c9644358"
    }
  },
  "agent": {
    "genai:researcher_lucas_rover": {
      "prov:type": "prov:Person",
      "genai:affiliation": "UTFPR"
    }
  },
  "wasAssociatedWith": {
    "_:wAW1": {
      "prov:activity": "genai:run_llama3_...",
      "prov:agent": "genai:researcher_..."
    }
  }
}
\end{lstlisting}


\section{JSON Extraction Quality}
\label{app:json-quality}

Table~\ref{tab:json-metrics} presents JSON-specific quality metrics for the structured extraction task. Two notable patterns emerge.

First, LLaMA~3 never produces raw-valid JSON: all 570 extraction outputs contain preamble text (e.g., ``Here is the extracted information in JSON format:'') before the JSON object, despite the prompt explicitly requesting ``JSON only, no explanation.'' After extracting the embedded JSON via regex, validity rates reach 100\% under greedy decoding, degrading slightly at higher temperatures (92.2\% at $t{=}0.7$). GPT-4, by contrast, always produces raw-valid JSON with 100\% schema compliance across all conditions. This instruction-following gap is consistent with the different prompt interfaces: the chat completion API's structured message format may better signal the expected output format.

Second, within-abstract field-level exact match rates---computed by comparing only runs of the \textit{same} abstract under the same condition, then averaging across abstracts---confirm the overall reproducibility hierarchy. Under greedy decoding, LLaMA~3 achieves near-perfect field EMR (0.982--0.989 overall), with all five fields at or above 0.978, consistent with the overall extraction EMR of 0.987 reported in Table~\ref{tab:emr_greedy}. GPT-4 under greedy shows lower field EMR (0.757--0.767 overall), with open-ended fields (\texttt{method}: 0.667, \texttt{key\_result}: 0.637) lagging behind structured fields (\texttt{model\_or\_system}: 0.893, \texttt{benchmark}: 0.863). As temperature increases, this gap widens: at $t{=}0.7$, \texttt{method} drops to 0.167 (LLaMA) and 0.157 (GPT-4), while \texttt{benchmark} retains 0.711 and 0.725 respectively---a 4--5$\times$ difference. This within-abstract formulation isolates true reproducibility (same input, same conditions, different runs) from between-abstract content variation, providing a methodologically clean measure of field-level consistency.

\input{table_json_metrics.tex}


\section{Prompt Card Example}
\label{app:prompt-card-example}

The following is a complete, filled-in Prompt Card for the summarization task as used in our experiments:

\begin{lstlisting}[caption={Prompt Card for the scientific summarization task.},label={lst:prompt-card}]
{
  "prompt_id": "summarization_v1",
  "prompt_hash": "c9644358805b4a7e...",
  "version": "1.0.0",
  "task_category": "summarization",
  "objective": "Produce a 3-sentence summary of a
    scientific abstract covering: (1) main
    contribution, (2) methodology, (3) key result.",
  "assumptions": [
    "Input is a single English scientific abstract",
    "Abstract contains identifiable methodology
     and quantitative results",
    "Model can produce coherent 3-sentence output"
  ],
  "limitations": [
    "Open-ended phrasing allows high output variance",
    "No explicit output-format constraint (unlike
     extraction task)"
  ],
  "target_models": [
    "llama3:8b", "mistral:7b", "gemma2:9b",
    "gpt-4", "claude-sonnet-4-5"
  ],
  "expected_output_format": "Three sentences of
    plain text, no JSON or structured markup",
  "interaction_regime": "single-turn",
  "change_log": [
    {"date": "2026-02-06", "change": "Initial version"}
  ]
}
\end{lstlisting}


\section{Representative Prompt Templates}
\label{app:prompts}

The following are the exact prompt templates used for each of the four experimental tasks. In all templates, \texttt{\{abstract\}} is replaced with the scientific abstract text at runtime.

\medskip
\noindent\textbf{Task 1: Scientific Summarization}
\begin{quote}\small\ttfamily
Summarize the following scientific abstract in exactly 3 sentences. Cover: (1) the main contribution, (2) the methodology used, and (3) the key quantitative result.\textbackslash n\textbackslash nAbstract: \{abstract\}\textbackslash n\textbackslash nSummary:
\end{quote}

\noindent\textbf{Task 2: Structured Extraction}
\begin{quote}\small\ttfamily
Extract the following fields from the scientific abstract below. Return JSON only, no explanation.\textbackslash n\textbackslash nFields: objective, method, key\_result, model\_or\_system, benchmark\textbackslash n\textbackslash nAbstract: \{abstract\}\textbackslash n\textbackslash nJSON:
\end{quote}

\noindent\textbf{Task 3: Multi-Turn Refinement} (3 turns)
\begin{quote}\small\ttfamily
Turn 1: [Same as Task 1 prompt]\textbackslash n
Turn 2: Now revise the summary to be more specific about the quantitative results mentioned.\textbackslash n
Turn 3: Finally, add one sentence about the limitations or future work mentioned in the abstract.
\end{quote}

\noindent\textbf{Task 4: RAG Extraction}
\begin{quote}\small\ttfamily
Using the context passage below and the scientific abstract, extract the following fields. Return JSON only.\textbackslash n\textbackslash nContext: \{retrieved\_passage\}\textbackslash nAbstract: \{abstract\}\textbackslash n\textbackslash nFields: objective, method, key\_result, model\_or\_system, benchmark\textbackslash n\textbackslash nJSON:
\end{quote}


\section{Experimental Coverage Matrix}
\label{app:coverage}

Table~\ref{tab:coverage} provides a complete coverage matrix showing the number of abstracts and runs per model--task--condition combination. This matrix enables readers to verify the sample sizes underlying all reported metrics.

\begin{table*}[h]
\centering
\caption{Experimental coverage: number of abstracts (runs) per model--task--condition. Dash (--) indicates the combination was not evaluated. C1: fixed seed; C2: variable seed (C2\_same\_params for GPT-4); C3: temperature sweep at $t \in \{0.0, 0.3, 0.7\}$.}
\label{tab:coverage}
\small
\begin{tabular}{llccccc}
\toprule
\textbf{Model} & \textbf{Task} & \textbf{C1} & \textbf{C2} & \textbf{C3 ($t$=0.0)} & \textbf{C3 ($t$=0.3)} & \textbf{C3 ($t$=0.7)} \\
\midrule
LLaMA 3 8B & Extraction & 30 (150) & 30 (150) & 30 (90) & 30 (90) & 30 (90) \\
 & Summarization & 30 (150) & 30 (150) & 30 (90) & 30 (90) & 30 (90) \\
 & Multi-turn & 10 (50) & -- & -- & -- & -- \\
 & RAG & 10 (50) & -- & -- & -- & -- \\
\addlinespace
Mistral 7B & Extraction & 10 (50) & 10 (50) & 10 (30) & 10 (30) & 10 (30) \\
 & Summarization & 10 (50) & 10 (50) & 10 (30) & 10 (30) & 10 (30) \\
 & Multi-turn & 10 (50) & -- & -- & -- & -- \\
 & RAG & 10 (50) & -- & -- & -- & -- \\
\addlinespace
Gemma 2 9B & Extraction & 10 (50) & 10 (50) & 10 (30) & 10 (30) & 10 (30) \\
 & Summarization & 10 (50) & 10 (50) & 10 (30) & 10 (30) & 10 (30) \\
 & Multi-turn & 10 (50) & -- & -- & -- & -- \\
 & RAG & 10 (50) & -- & -- & -- & -- \\
\midrule
GPT-4 & Extraction & -- & 30 (150) & 17 (51) & 17 (51) & 14 (42) \\
 & Summarization & 3 (8)$^\dagger$ & 30 (150) & 30 (90) & 30 (90) & 30 (90) \\
 & Multi-turn & -- & -- & -- & -- & -- \\
 & RAG & -- & -- & -- & -- & -- \\
\addlinespace
Claude Sonnet 4.5 & Extraction & 10 (49)$^\ddagger$ & 10 (50) & 10 (30) & 10 (30) & 10 (30) \\
 & Summarization & 10 (50) & 10 (50) & 10 (30) & 10 (30) & 10 (30) \\
 & Multi-turn & 10 (50) & -- & -- & -- & -- \\
 & RAG & 10 (50) & -- & -- & -- & -- \\
\bottomrule
\end{tabular}
\par\smallskip\raggedright\footnotesize
$^\dagger$GPT-4 C1 summarization: only 3 abstracts completed before quota exhaustion; excluded from primary analysis (C2 used instead).\\
$^\ddagger$Claude C1 extraction: 49 runs (1 empty output due to API timeout).
\end{table*}


\section{Chat-Format Control Experiment}
\label{app:chat-control}

To assess whether the prompt-format difference between LLaMA~3 (completion-style via \texttt{/api/generate}) and GPT-4 (chat-style via Chat Completions) contributes to the observed reproducibility gap, we conducted a supplementary control experiment running LLaMA~3 8B through Ollama's \texttt{/api/chat} endpoint, which applies the model's chat template (including special tokens for system/user/assistant roles) in the same message structure used by GPT-4.

\textbf{Design:} 10 abstracts $\times$ 2 tasks $\times$ 2 conditions (C1, C2) $\times$ 5 repetitions = 200 runs, all under greedy decoding ($t{=}0$).

\textbf{Results:} Table~\ref{tab:chat-control} compares the chat-format control with the original completion-format results for the same 10 abstracts. The two prompt formats produce \emph{identical} variability metrics across all conditions: summarization EMR\,=\,0.929, NED\,=\,0.0066, and ROUGE-L\,=\,0.9922 in both modes; extraction achieves perfect reproducibility (EMR\,=\,1.000) regardless of interface. The 0.929 summarization EMR reflects the warm-up effect on 2 of 10 abstracts---the same pattern observed in the full 30-abstract experiment. These results confirm that prompt format is not a source of variability, and the reproducibility gap between LLaMA~3 and GPT-4 is consistent with deployment-side factors (server infrastructure, floating-point non-determinism across GPU types, request batching) rather than prompt-format differences.

\begin{table}[h]
\centering
\caption{Prompt-format control: LLaMA~3 8B via completion (\texttt{/api/generate}) vs.\ chat (\texttt{/api/chat}) for 10 abstracts under greedy decoding ($t{=}0$). EMR computed over conditions C1 and C2 combined.}
\label{tab:chat-control}
\small
\begin{tabular}{@{}llcc@{}}
\toprule
\textbf{Task} & \textbf{Metric} & \textbf{Completion} & \textbf{Chat} \\
\midrule
Summarization & EMR$\uparrow$ & 0.929 & 0.929 \\
 & NED$\downarrow$ & 0.0066 & 0.0066 \\
 & ROUGE-L$\uparrow$ & 0.9922 & 0.9922 \\
\midrule
Extraction & EMR$\uparrow$ & 1.000 & 1.000 \\
 & NED$\downarrow$ & 0.0000 & 0.0000 \\
 & ROUGE-L$\uparrow$ & 1.0000 & 1.0000 \\
\bottomrule
\end{tabular}
\par\smallskip\raggedright\footnotesize
\textit{Note:} Completion and chat formats yield identical metrics for all 10~abstracts under greedy decoding, indicating that prompt format is not a source of variability.
\end{table}


\section{API Payload Documentation}
\label{app:payloads}

To address potential ``apples-to-oranges'' concerns, we document the exact payload structures sent to each inference endpoint. All payloads were constructed deterministically and logged as part of the Run Card.

\textbf{Local models (Ollama).} Single-turn tasks use \texttt{POST /api/generate}:

\noindent\textbf{Ollama generate payload (Tasks 1--2):}
\begin{quote}\small\ttfamily
\{"model": "llama3:8b",\\
\phantom{x}"prompt": "<full prompt text>",\\
\phantom{x}"options": \{"temperature": 0.0, "seed": 42, "num\_predict": 1024\},\\
\phantom{x}"stream": false\}
\end{quote}

\noindent The \texttt{model} field is set to \texttt{llama3:8b}, \texttt{mistral:7b}, or \texttt{gemma2:9b} as appropriate. Multi-turn tasks (Task~3) use \texttt{POST /api/chat} with accumulated \texttt{messages} array. No system prompt, stop sequences, or post-processing are applied.

\textbf{GPT-4 (OpenAI).} Accessed via the \texttt{openai} Python SDK v1.59.9:

\begin{quote}\small\ttfamily
\{"model": "gpt-4",\\
\phantom{x}"messages": [\{"role": "user", "content": "<prompt>"\}],\\
\phantom{x}"temperature": 0.0, "seed": 42, "max\_tokens": 1024\}
\end{quote}

\noindent No system message, stop sequences, \texttt{top\_p}, \texttt{frequency\_penalty}, or \texttt{presence\_penalty} were set (all defaults). The resolved model version (\texttt{gpt-4-0613}) was extracted from the response object and logged.

\textbf{Claude Sonnet 4.5 (Anthropic).} Accessed via \texttt{urllib} (no SDK dependency):

\begin{quote}\small\ttfamily
\{"model": "claude-sonnet-4-5-20250929",\\
\phantom{x}"messages": [\{"role": "user", "content": "<prompt>"\}],\\
\phantom{x}"temperature": 0.0, "max\_tokens": 1024\}
\end{quote}

\noindent No \texttt{seed} parameter (not supported by the Anthropic API), no system message, no stop sequences. The seed value in the Run Card is marked \texttt{seed\_status: "logged-only-not-sent-to-api"}.

\textbf{Key symmetry points.} Across all seven models: (1)~identical prompt text (verified by \texttt{prompt\_hash}); (2)~identical temperature ($t{=}0.0$); (3)~identical max token limit (1,024); (4)~no system messages; (5)~no stop sequences; (6)~no post-processing or text normalization of outputs.


\end{document}
