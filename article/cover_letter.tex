\documentclass[11pt]{letter}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{parskip}

\signature{Lucas Rover and Yara de Souza Tadano\\
Programa de P\'os-Gradua\c{c}\~ao em Engenharia Mec\^anica\\
UTFPR -- Universidade Tecnol\'ogica Federal do Paran\'a\\
Ponta Grossa, Paran\'a, Brazil\\
\texttt{lucasrover@utfpr.edu.br}, \texttt{yaratadano@utfpr.edu.br}}

\address{Lucas Rover and Yara de Souza Tadano\\
UTFPR -- Universidade Tecnol\'ogica Federal do Paran\'a\\
Ponta Grossa, Paran\'a, Brazil}

\date{February 2026}

\begin{document}

\begin{letter}{Professors J.\ Christopher Beck, Edith Elkind, and Mykel Kochenderfer\\
Editors-in-Chief\\
Journal of Artificial Intelligence Research (JAIR)}

\opening{Dear Professors Beck, Elkind, and Kochenderfer,}

We are pleased to submit the manuscript entitled \textbf{``Same Prompt, Different Answer: Exposing the Reproducibility Illusion in Large Language Model APIs''} for consideration for publication in the \textit{Journal of Artificial Intelligence Research}.

This work addresses a timely and critical challenge in AI research: the reproducibility of studies that rely on large language model outputs. While the AI reproducibility crisis has been widely documented, existing experiment-tracking tools were not designed for the specific challenges of generative text outputs. The paper makes three contributions:

\begin{enumerate}
    \item \textbf{A lightweight protocol} introducing novel documentation artifacts---Prompt Cards and Run Cards---built on the W3C PROV data model for machine-readable provenance graphs, with formally verified audit completeness and less than 1\% overhead.

    \item \textbf{A large-scale empirical evaluation} through 4,104 controlled experiments with nine model deployments---three locally deployed (LLaMA~3 8B, Mistral 7B, Gemma~2 9B), five closed-source API-served (GPT-4, Claude Sonnet 4.5, Gemini 2.5 Pro, DeepSeek Chat, Perplexity Sonar), and one cloud-served open-weight model (LLaMA~3 8B via Together AI as a quasi-isolation probe)---across four NLP tasks, 30 scientific abstracts, and seven execution environments. We document a striking reproducibility gap: local models achieve near-perfect reproducibility (average single-turn EMR = 0.960; Gemma~2 9B attains perfect EMR = 1.000 across all tasks), while API-served models exhibit substantial hidden non-determinism (average single-turn EMR = 0.325), spanning a wide range from DeepSeek Chat (EMR = 0.800) to Gemini 2.5 Pro (multi-turn EMR = 0.010). This approximately 3-fold gap is observed independently across \textit{five} cloud providers and survives Holm-Bonferroni correction across 68 hypothesis tests (51 significant).

    \item \textbf{A reference implementation} in Python with all 4,104 run records, provenance documents, and analysis scripts publicly available for independent verification.
\end{enumerate}

\textbf{Why JAIR.} This manuscript directly complements the reproducibility mechanisms recently adopted by JAIR, as formalized by Gundersen et al.\ (2024). Our work (a)~demonstrates empirically why these mechanisms are necessary---documenting pervasive hidden non-determinism across five independent API providers---(b)~provides infrastructure to operationalize reproducibility tracking for generative AI experiments, and (c)~includes a dedicated JAIR Reproducibility Compliance appendix mapping our protocol to all four mechanisms described by Gundersen et al. The scope---spanning reproducibility, provenance, and empirical methodology---aligns with JAIR's broad interest in AI foundations.

Statistical robustness is ensured through Holm-Bonferroni correction, Fisher's exact tests, bias-corrected bootstrap confidence intervals (10,000 resamples), and balanced subsample sensitivity analysis. Supplementary control experiments (200 additional runs using Ollama's \texttt{/api/chat} endpoint) show that the prompt-format difference between deployment paradigms does not explain the reproducibility gap, strengthening internal validity.

This manuscript has not been published elsewhere and is not under consideration by any other journal. All authors have approved the manuscript and agree with its submission to JAIR.

\textbf{Suggested Reviewers:}
\begin{enumerate}
    \item \textbf{Odd Erik Gundersen} (Norwegian University of Science and Technology) --- Leading expert on AI reproducibility; recently published the four-mechanism framework adopted by JAIR.
    \item \textbf{Anya Belz} (Dublin City University) --- Conducted systematic reviews of NLP reproducibility; expertise in evaluation methodology.
    \item \textbf{Joelle Pineau} (McGill University / Meta FAIR) --- Led the NeurIPS Reproducibility Program; pioneer of reproducibility checklists.
    \item \textbf{Luc Moreau} (King's College London) --- Co-editor of the W3C PROV specification; expertise in provenance data models.
    \item \textbf{Jesse Dodge} (Allen Institute for AI) --- Expert in ML reporting standards and experimental methodology.
\end{enumerate}

\closing{Sincerely,}

\end{letter}
\end{document}
