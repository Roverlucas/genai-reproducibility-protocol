\documentclass[11pt]{letter}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{parskip}

\signature{Lucas Rover and Yara de Souza Tadano\\
Programa de P\'os-Gradua\c{c}\~ao em Engenharia Mec\^anica\\
UTFPR -- Universidade Tecnol\'ogica Federal do Paran\'a\\
Ponta Grossa, Paran\'a, Brazil\\
\texttt{lucasrover@utfpr.edu.br}, \texttt{yaratadano@utfpr.edu.br}}

\address{Lucas Rover and Yara de Souza Tadano\\
UTFPR -- Universidade Tecnol\'ogica Federal do Paran\'a\\
Ponta Grossa, Paran\'a, Brazil}

\date{February 2026}

\begin{document}

\begin{letter}{Professors J.\ Christopher Beck, Edith Elkind, and Mykel Kochenderfer\\
Editors-in-Chief\\
Journal of Artificial Intelligence Research (JAIR)}

\opening{Dear Professors Beck, Elkind, and Kochenderfer,}

We are pleased to submit the manuscript entitled \textbf{``Hidden Non-Determinism in Large Language Model APIs: A Lightweight Provenance Protocol for Reproducible Generative AI Research''} for consideration for publication in the \textit{Journal of Artificial Intelligence Research}.

This work addresses a timely and critical challenge in AI research: the reproducibility of studies that rely on large language model outputs. While the AI reproducibility crisis has been widely documented, existing experiment-tracking tools were not designed for the specific challenges of generative text outputs. The paper makes three contributions:

\begin{enumerate}
    \item \textbf{A lightweight protocol} introducing novel documentation artifacts---Prompt Cards and Run Cards---built on the W3C PROV data model for machine-readable provenance graphs.

    \item \textbf{An empirical evaluation} through 3,604 controlled experiments with five models---three locally deployed (LLaMA~3 8B, Mistral 7B, Gemma~2 9B) and two API-served (GPT-4, Claude Sonnet 4.5)---across four NLP tasks (extraction, summarization, multi-turn refinement, RAG extraction), 30 scientific abstracts, and five experimental conditions. We document a striking reproducibility gap between local and API-based inference: local models achieve near-perfect reproducibility (average single-turn EMR = 0.956; Gemma~2 9B attains perfect EMR = 1.000 across all tasks), while API-served models exhibit substantial hidden non-determinism (average single-turn EMR = 0.221), with Claude Sonnet 4.5 achieving only EMR = 0.020 for summarization. This more than 4-fold gap is observed independently across two cloud providers.

    \item \textbf{A reference implementation} in Python with all 3,604 run records, provenance documents, and analysis scripts publicly available for independent verification.
\end{enumerate}

Supplementary control experiments (200 additional runs using Ollama's \texttt{/api/chat} endpoint) show that the prompt-format difference between deployment paradigms does not explain the reproducibility gap, strengthening the internal validity of our findings.

We believe this work is well-suited for JAIR for several reasons: (a) it addresses a foundational methodological issue affecting all AI research that uses generative models, directly complementing the reproducibility mechanisms recently adopted by JAIR (Gundersen et al., 2024); (b) it provides rigorous empirical evidence across five models, four tasks, and two independent API providers; (c) the protocol and tools are immediately practical for the AI research community; and (d) the paper's scope---spanning reproducibility, provenance, and empirical methodology---aligns with JAIR's broad interest in AI foundations.

The protocol adds less than 1\% overhead to inference time (approximately 4~KB per run) while providing complete audit trails, tamper detection via cryptographic hashing, and interoperable W3C PROV provenance graphs. We hope this work will contribute to raising the bar for reproducibility in generative AI research.

This manuscript has not been published elsewhere and is not under consideration by any other journal. All authors have approved the manuscript and agree with its submission to JAIR.

\closing{Sincerely,}

\end{letter}
\end{document}
