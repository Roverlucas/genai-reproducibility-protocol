{
  "n_total_runs": 1864,
  "variability_per_abstract": {
    "gpt4_extraction_C2_abs_001": {
      "n_outputs": 5,
      "exact_match_rate": 0.3,
      "edit_distance": {
        "mean": 6.4,
        "std": 5.71314274283428,
        "min": 0,
        "max": 16,
        "normalized_mean": 0.00864503280823142
      },
      "rouge_l": {
        "mean": 0.9915953933699869,
        "std": 0.007489557210700411,
        "min": 0.9789473684210526,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9986032009124756,
        "bertscore_f1_std": 0.0013493715023175728,
        "bertscore_f1_min": 0.9966101050376892,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9986039161682129,
        "bertscore_recall_mean": 0.9986026883125305
      },
      "avg_output_length_chars": 737.2,
      "avg_output_length_words": 95.0,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_001",
      "json_validity": {
        "json_valid_count": 5,
        "json_valid_total": 5,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 5,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 1.0,
          "method": 1.0,
          "key_result": 0.6,
          "model_or_system": 1.0,
          "benchmark": 0.6
        },
        "overall_field_emr": 0.84
      }
    },
    "gpt4_extraction_C2_abs_002": {
      "n_outputs": 5,
      "exact_match_rate": 0.3,
      "edit_distance": {
        "mean": 89.9,
        "std": 108.27506638187761,
        "min": 0,
        "max": 223,
        "normalized_mean": 0.0853941037608347
      },
      "rouge_l": {
        "mean": 0.9298626886768642,
        "std": 0.07344647644018926,
        "min": 0.8387096774193549,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9917943477630615,
        "bertscore_f1_std": 0.008575738135294674,
        "bertscore_f1_min": 0.9807780385017395,
        "bertscore_f1_max": 0.9999999403953552,
        "bertscore_precision_mean": 0.9917417705059052,
        "bertscore_recall_mean": 0.9918537855148315
      },
      "avg_output_length_chars": 1038.8,
      "avg_output_length_words": 141.0,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_002",
      "json_validity": {
        "json_valid_count": 5,
        "json_valid_total": 5,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 5,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 0.3,
          "method": 0.3,
          "key_result": 0.4,
          "model_or_system": 1.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 0.6
      }
    },
    "gpt4_extraction_C2_abs_003": {
      "n_outputs": 5,
      "exact_match_rate": 0.6,
      "edit_distance": {
        "mean": 6.0,
        "std": 7.3484692283495345,
        "min": 0,
        "max": 15,
        "normalized_mean": 0.006389776357827476
      },
      "rouge_l": {
        "mean": 0.994979079497908,
        "std": 0.0061493466346021096,
        "min": 0.9874476987447699,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9980480551719666,
        "bertscore_f1_std": 0.0023905614168903016,
        "bertscore_f1_min": 0.9951202273368835,
        "bertscore_f1_max": 0.9999999403953552,
        "bertscore_precision_mean": 0.9982896208763122,
        "bertscore_recall_mean": 0.9978076338768005
      },
      "avg_output_length_chars": 927.0,
      "avg_output_length_words": 118.6,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_003",
      "json_validity": {
        "json_valid_count": 5,
        "json_valid_total": 5,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 5,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 1.0,
          "method": 0.6,
          "key_result": 1.0,
          "model_or_system": 1.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 0.92
      }
    },
    "gpt4_extraction_C2_abs_004": {
      "n_outputs": 5,
      "exact_match_rate": 0.4,
      "edit_distance": {
        "mean": 49.8,
        "std": 40.661529730200755,
        "min": 0,
        "max": 83,
        "normalized_mean": 0.07114285714285715
      },
      "rouge_l": {
        "mean": 0.9573770491803278,
        "std": 0.034801493613312896,
        "min": 0.9289617486338798,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9929023385047913,
        "bertscore_f1_std": 0.005795216343420308,
        "bertscore_f1_min": 0.9881705641746521,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.992906928062439,
        "bertscore_recall_mean": 0.9928977489471436
      },
      "avg_output_length_chars": 696.4,
      "avg_output_length_words": 91.8,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_004",
      "json_validity": {
        "json_valid_count": 5,
        "json_valid_total": 5,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 5,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 1.0,
          "method": 0.4,
          "key_result": 1.0,
          "model_or_system": 1.0,
          "benchmark": 0.4
        },
        "overall_field_emr": 0.76
      }
    },
    "gpt4_extraction_C2_abs_005": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 628.0,
      "avg_output_length_words": 85.0,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_005",
      "json_validity": {
        "json_valid_count": 5,
        "json_valid_total": 5,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 5,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 1.0,
          "method": 1.0,
          "key_result": 1.0,
          "model_or_system": 1.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 1.0
      }
    },
    "gpt4_extraction_C2_abs_006": {
      "n_outputs": 5,
      "exact_match_rate": 0.6,
      "edit_distance": {
        "mean": 34.8,
        "std": 42.6211215244273,
        "min": 0,
        "max": 87,
        "normalized_mean": 0.03936651583710407
      },
      "rouge_l": {
        "mean": 0.9692307692307691,
        "std": 0.037684457581279696,
        "min": 0.923076923076923,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9958784341812134,
        "bertscore_f1_std": 0.005047866598661789,
        "bertscore_f1_min": 0.9896960854530334,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9967929244041442,
        "bertscore_recall_mean": 0.994980537891388
      },
      "avg_output_length_chars": 837.6,
      "avg_output_length_words": 127.6,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_006",
      "json_validity": {
        "json_valid_count": 5,
        "json_valid_total": 5,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 5,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 1.0,
          "method": 0.6,
          "key_result": 0.6,
          "model_or_system": 1.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 0.84
      }
    },
    "gpt4_extraction_C2_abs_007": {
      "n_outputs": 5,
      "exact_match_rate": 0.1,
      "edit_distance": {
        "mean": 76.0,
        "std": 59.794648589986714,
        "min": 0,
        "max": 165,
        "normalized_mean": 0.09999181784111084
      },
      "rouge_l": {
        "mean": 0.9255654946934481,
        "std": 0.06063968880341847,
        "min": 0.8365384615384616,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.993028211593628,
        "bertscore_f1_std": 0.0038772245677716677,
        "bertscore_f1_min": 0.9866056442260742,
        "bertscore_f1_max": 0.9999999403953552,
        "bertscore_precision_mean": 0.993139135837555,
        "bertscore_recall_mean": 0.9929261267185211
      },
      "avg_output_length_chars": 750.2,
      "avg_output_length_words": 102.6,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_007",
      "json_validity": {
        "json_valid_count": 5,
        "json_valid_total": 5,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 5,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 0.6,
          "method": 1.0,
          "key_result": 0.6,
          "model_or_system": 1.0,
          "benchmark": 0.3
        },
        "overall_field_emr": 0.7
      }
    },
    "gpt4_extraction_C2_abs_008": {
      "n_outputs": 5,
      "exact_match_rate": 0.3,
      "edit_distance": {
        "mean": 134.8,
        "std": 163.26469306007345,
        "min": 0,
        "max": 337,
        "normalized_mean": 0.19750947598807198
      },
      "rouge_l": {
        "mean": 0.8190988433586952,
        "std": 0.21124822295921078,
        "min": 0.5443037974683544,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.976099169254303,
        "bertscore_f1_std": 0.027142080785575926,
        "bertscore_f1_min": 0.9405734539031982,
        "bertscore_f1_max": 0.9999999403953552,
        "bertscore_precision_mean": 0.976177167892456,
        "bertscore_recall_mean": 0.9761464655399322
      },
      "avg_output_length_chars": 663.4,
      "avg_output_length_words": 84.4,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_008",
      "json_validity": {
        "json_valid_count": 5,
        "json_valid_total": 5,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 5,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 0.6,
          "method": 0.6,
          "key_result": 0.6,
          "model_or_system": 0.6,
          "benchmark": 1.0
        },
        "overall_field_emr": 0.68
      }
    },
    "gpt4_extraction_C2_abs_009": {
      "n_outputs": 5,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 351.5,
        "std": 177.98553311997017,
        "min": 5,
        "max": 592,
        "normalized_mean": 0.3204803011331605
      },
      "rouge_l": {
        "mean": 0.7406022965366585,
        "std": 0.11970616643862446,
        "min": 0.5609756097560976,
        "max": 0.9863013698630136
      },
      "bert_score": {
        "bertscore_f1_mean": 0.962346363067627,
        "bertscore_f1_std": 0.013645306582289874,
        "bertscore_f1_min": 0.9458943009376526,
        "bertscore_f1_max": 0.9932201504707336,
        "bertscore_precision_mean": 0.9709443926811219,
        "bertscore_recall_mean": 0.9540844142436982
      },
      "avg_output_length_chars": 978.4,
      "avg_output_length_words": 129.8,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_009",
      "json_validity": {
        "json_valid_count": 5,
        "json_valid_total": 5,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 5,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 0.1,
          "method": 0.0,
          "key_result": 0.3,
          "model_or_system": 1.0,
          "benchmark": 0.6
        },
        "overall_field_emr": 0.4
      }
    },
    "gpt4_extraction_C2_abs_010": {
      "n_outputs": 5,
      "exact_match_rate": 0.3,
      "edit_distance": {
        "mean": 42.2,
        "std": 51.074063868073,
        "min": 0,
        "max": 105,
        "normalized_mean": 0.03481848184818481
      },
      "rouge_l": {
        "mean": 0.9664839545545145,
        "std": 0.03762848556082774,
        "min": 0.9190751445086706,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9949865758419036,
        "bertscore_f1_std": 0.005920344354106921,
        "bertscore_f1_min": 0.9876540899276733,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9949860453605652,
        "bertscore_recall_mean": 0.9949871063232422
      },
      "avg_output_length_chars": 1206.0,
      "avg_output_length_words": 173.6,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_010",
      "json_validity": {
        "json_valid_count": 5,
        "json_valid_total": 5,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 5,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 0.6,
          "method": 0.6,
          "key_result": 0.4,
          "model_or_system": 1.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 0.72
      }
    },
    "gpt4_extraction_C2_abs_011": {
      "n_outputs": 5,
      "exact_match_rate": 0.3,
      "edit_distance": {
        "mean": 210.3,
        "std": 141.0113825192846,
        "min": 0,
        "max": 323,
        "normalized_mean": 0.21805870328657845
      },
      "rouge_l": {
        "mean": 0.8503103923379793,
        "std": 0.10186808263537717,
        "min": 0.759493670886076,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9745160818099976,
        "bertscore_f1_std": 0.017227564786884374,
        "bertscore_f1_min": 0.9590184092521667,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9826251864433289,
        "bertscore_recall_mean": 0.9666714668273926
      },
      "avg_output_length_chars": 899.2,
      "avg_output_length_words": 128.0,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_011",
      "json_validity": {
        "json_valid_count": 5,
        "json_valid_total": 5,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 5,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 1.0,
          "method": 0.4,
          "key_result": 0.3,
          "model_or_system": 0.6,
          "benchmark": 1.0
        },
        "overall_field_emr": 0.66
      }
    },
    "gpt4_extraction_C2_abs_012": {
      "n_outputs": 5,
      "exact_match_rate": 0.2,
      "edit_distance": {
        "mean": 25.0,
        "std": 17.877359984069237,
        "min": 0,
        "max": 44,
        "normalized_mean": 0.03528481012658228
      },
      "rouge_l": {
        "mean": 0.9679768948999717,
        "std": 0.0207944945747633,
        "min": 0.9404761904761905,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9922726333141327,
        "bertscore_f1_std": 0.004929683228486152,
        "bertscore_f1_min": 0.9864009022712708,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9944679915904999,
        "bertscore_recall_mean": 0.9900916516780853
      },
      "avg_output_length_chars": 702.0,
      "avg_output_length_words": 84.4,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_012",
      "json_validity": {
        "json_valid_count": 5,
        "json_valid_total": 5,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 5,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 0.4,
          "method": 1.0,
          "key_result": 0.4,
          "model_or_system": 0.6,
          "benchmark": 1.0
        },
        "overall_field_emr": 0.68
      }
    },
    "gpt4_extraction_C2_abs_013": {
      "n_outputs": 5,
      "exact_match_rate": 0.1,
      "edit_distance": {
        "mean": 41.2,
        "std": 25.42754412050051,
        "min": 0,
        "max": 80,
        "normalized_mean": 0.07395536732708932
      },
      "rouge_l": {
        "mean": 0.9419817099007434,
        "std": 0.0367291560759951,
        "min": 0.8823529411764706,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9839613199234009,
        "bertscore_f1_std": 0.01090717999799383,
        "bertscore_f1_min": 0.9710218906402588,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9840445518493652,
        "bertscore_recall_mean": 0.9838943660259247
      },
      "avg_output_length_chars": 547.2,
      "avg_output_length_words": 69.4,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_013",
      "json_validity": {
        "json_valid_count": 5,
        "json_valid_total": 5,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 5,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 0.6,
          "method": 0.4,
          "key_result": 1.0,
          "model_or_system": 1.0,
          "benchmark": 0.4
        },
        "overall_field_emr": 0.68
      }
    },
    "gpt4_extraction_C2_abs_014": {
      "n_outputs": 5,
      "exact_match_rate": 0.3,
      "edit_distance": {
        "mean": 10.4,
        "std": 8.138795979750322,
        "min": 0,
        "max": 21,
        "normalized_mean": 0.01008729388942774
      },
      "rouge_l": {
        "mean": 0.9922856091277144,
        "std": 0.007488563263028997,
        "min": 0.9824561403508772,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9987750768661499,
        "bertscore_f1_std": 0.0009285662546352506,
        "bertscore_f1_min": 0.9977840781211853,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9988543570041657,
        "bertscore_recall_mean": 0.9986959576606751
      },
      "avg_output_length_chars": 1025.0,
      "avg_output_length_words": 142.4,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_014",
      "json_validity": {
        "json_valid_count": 5,
        "json_valid_total": 5,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 5,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 1.0,
          "method": 0.6,
          "key_result": 0.4,
          "model_or_system": 1.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 0.8
      }
    },
    "gpt4_extraction_C2_abs_015": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0000001192092896,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0000001192092896,
        "bertscore_f1_max": 1.0000001192092896,
        "bertscore_precision_mean": 1.0000001192092896,
        "bertscore_recall_mean": 1.0000001192092896
      },
      "avg_output_length_chars": 562.0,
      "avg_output_length_words": 76.0,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_015",
      "json_validity": {
        "json_valid_count": 5,
        "json_valid_total": 5,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 5,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 1.0,
          "method": 1.0,
          "key_result": 1.0,
          "model_or_system": 1.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 1.0
      }
    },
    "gpt4_extraction_C2_abs_016": {
      "n_outputs": 5,
      "exact_match_rate": 0.6,
      "edit_distance": {
        "mean": 7.2,
        "std": 8.818163074019441,
        "min": 0,
        "max": 18,
        "normalized_mean": 0.010810810810810811
      },
      "rouge_l": {
        "mean": 0.9951807228915662,
        "std": 0.005902384922369122,
        "min": 0.9879518072289156,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9987646341323853,
        "bertscore_f1_std": 0.001513008010653381,
        "bertscore_f1_min": 0.9969115853309631,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9984084129333496,
        "bertscore_recall_mean": 0.9991214752197266
      },
      "avg_output_length_chars": 651.6,
      "avg_output_length_words": 82.4,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_016",
      "json_validity": {
        "json_valid_count": 5,
        "json_valid_total": 5,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 5,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 1.0,
          "method": 1.0,
          "key_result": 0.6,
          "model_or_system": 1.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 0.92
      }
    },
    "gpt4_extraction_C2_abs_017": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 547.0,
      "avg_output_length_words": 75.0,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_017",
      "json_validity": {
        "json_valid_count": 5,
        "json_valid_total": 5,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 5,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 1.0,
          "method": 1.0,
          "key_result": 1.0,
          "model_or_system": 1.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 1.0
      }
    },
    "gpt4_extraction_C2_abs_018": {
      "n_outputs": 5,
      "exact_match_rate": 0.6,
      "edit_distance": {
        "mean": 63.6,
        "std": 77.89377382050506,
        "min": 0,
        "max": 159,
        "normalized_mean": 0.06017029328287606
      },
      "rouge_l": {
        "mean": 0.9403636363636364,
        "std": 0.07303933051208025,
        "min": 0.8509090909090908,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9910454511642456,
        "bertscore_f1_std": 0.01096703776221572,
        "bertscore_f1_min": 0.977613627910614,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9923351764678955,
        "bertscore_recall_mean": 0.9897888660430908
      },
      "avg_output_length_chars": 1038.0,
      "avg_output_length_words": 142.0,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_018",
      "json_validity": {
        "json_valid_count": 5,
        "json_valid_total": 5,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 5,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 1.0,
          "method": 0.6,
          "key_result": 0.6,
          "model_or_system": 1.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 0.84
      }
    },
    "gpt4_extraction_C2_abs_019": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 602.0,
      "avg_output_length_words": 79.0,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_019",
      "json_validity": {
        "json_valid_count": 5,
        "json_valid_total": 5,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 5,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 1.0,
          "method": 1.0,
          "key_result": 1.0,
          "model_or_system": 1.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 1.0
      }
    },
    "gpt4_extraction_C2_abs_020": {
      "n_outputs": 5,
      "exact_match_rate": 0.6,
      "edit_distance": {
        "mean": 3.2,
        "std": 3.9191835884530852,
        "min": 0,
        "max": 8,
        "normalized_mean": 0.006517311608961303
      },
      "rouge_l": {
        "mean": 0.9882352941176471,
        "std": 0.014408763192842228,
        "min": 0.9705882352941176,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.998832893371582,
        "bertscore_f1_std": 0.0014294078575220362,
        "bertscore_f1_min": 0.9970822334289551,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9986874878406524,
        "bertscore_recall_mean": 0.9989787518978119
      },
      "avg_output_length_chars": 489.4,
      "avg_output_length_words": 68.6,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_020",
      "json_validity": {
        "json_valid_count": 5,
        "json_valid_total": 5,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 5,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 1.0,
          "method": 0.6,
          "key_result": 1.0,
          "model_or_system": 1.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 0.92
      }
    },
    "gpt4_extraction_C2_abs_021": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9999999403953552,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9999999403953552,
        "bertscore_f1_max": 0.9999999403953552,
        "bertscore_precision_mean": 0.9999999403953552,
        "bertscore_recall_mean": 0.9999999403953552
      },
      "avg_output_length_chars": 622.0,
      "avg_output_length_words": 78.0,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_021",
      "json_validity": {
        "json_valid_count": 5,
        "json_valid_total": 5,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 5,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 1.0,
          "method": 1.0,
          "key_result": 1.0,
          "model_or_system": 1.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 1.0
      }
    },
    "gpt4_extraction_C2_abs_022": {
      "n_outputs": 5,
      "exact_match_rate": 0.3,
      "edit_distance": {
        "mean": 134.1,
        "std": 95.1750492513663,
        "min": 0,
        "max": 211,
        "normalized_mean": 0.16011450599657337
      },
      "rouge_l": {
        "mean": 0.8208821093519185,
        "std": 0.1272972120713066,
        "min": 0.6910994764397906,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9744549810886383,
        "bertscore_f1_std": 0.018657046629168315,
        "bertscore_f1_min": 0.9571365118026733,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9745650172233582,
        "bertscore_recall_mean": 0.9743627846240998
      },
      "avg_output_length_chars": 747.0,
      "avg_output_length_words": 93.0,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_022",
      "json_validity": {
        "json_valid_count": 5,
        "json_valid_total": 5,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 5,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 0.3,
          "method": 0.3,
          "key_result": 0.6,
          "model_or_system": 0.6,
          "benchmark": 0.6
        },
        "overall_field_emr": 0.48
      }
    },
    "gpt4_extraction_C2_abs_023": {
      "n_outputs": 5,
      "exact_match_rate": 0.1,
      "edit_distance": {
        "mean": 37.0,
        "std": 26.540535036053814,
        "min": 0,
        "max": 61,
        "normalized_mean": 0.053752334500900834
      },
      "rouge_l": {
        "mean": 0.9355481025066517,
        "std": 0.03256059000793843,
        "min": 0.8958333333333334,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9915678203105927,
        "bertscore_f1_std": 0.004662829527338268,
        "bertscore_f1_min": 0.9863947629928589,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9924845695495605,
        "bertscore_recall_mean": 0.9906585872173309
      },
      "avg_output_length_chars": 670.4,
      "avg_output_length_words": 97.0,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_023",
      "json_validity": {
        "json_valid_count": 5,
        "json_valid_total": 5,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 5,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 0.3,
          "method": 0.2,
          "key_result": 0.4,
          "model_or_system": 1.0,
          "benchmark": 0.4
        },
        "overall_field_emr": 0.46
      }
    },
    "gpt4_extraction_C2_abs_024": {
      "n_outputs": 5,
      "exact_match_rate": 0.6,
      "edit_distance": {
        "mean": 58.8,
        "std": 72.01499843782544,
        "min": 0,
        "max": 147,
        "normalized_mean": 0.08609077598828697
      },
      "rouge_l": {
        "mean": 0.9350000000000002,
        "std": 0.07960841664045323,
        "min": 0.8375000000000001,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9911858320236206,
        "bertscore_f1_std": 0.010795107024654645,
        "bertscore_f1_min": 0.9779645800590515,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9939095139503479,
        "bertscore_recall_mean": 0.9886059165000916
      },
      "avg_output_length_chars": 565.4,
      "avg_output_length_words": 72.8,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_024",
      "json_validity": {
        "json_valid_count": 5,
        "json_valid_total": 5,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 5,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 1.0,
          "method": 1.0,
          "key_result": 0.6,
          "model_or_system": 1.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 0.92
      }
    },
    "gpt4_extraction_C2_abs_025": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9999999403953552,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9999999403953552,
        "bertscore_f1_max": 0.9999999403953552,
        "bertscore_precision_mean": 0.9999999403953552,
        "bertscore_recall_mean": 0.9999999403953552
      },
      "avg_output_length_chars": 595.0,
      "avg_output_length_words": 86.0,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_025",
      "json_validity": {
        "json_valid_count": 5,
        "json_valid_total": 5,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 5,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 1.0,
          "method": 1.0,
          "key_result": 1.0,
          "model_or_system": 1.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 1.0
      }
    },
    "gpt4_extraction_C2_abs_026": {
      "n_outputs": 5,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 179.7,
        "std": 78.1128030478999,
        "min": 66,
        "max": 310,
        "normalized_mean": 0.16761462251656842
      },
      "rouge_l": {
        "mean": 0.8593827824306285,
        "std": 0.06567374243231683,
        "min": 0.7491166077738517,
        "max": 0.9664429530201343
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9770974457263947,
        "bertscore_f1_std": 0.009038606360547007,
        "bertscore_f1_min": 0.9644783139228821,
        "bertscore_f1_max": 0.9913722276687622,
        "bertscore_precision_mean": 0.9753899157047272,
        "bertscore_recall_mean": 0.9788434326648712
      },
      "avg_output_length_chars": 1018.8,
      "avg_output_length_words": 137.8,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_026",
      "json_validity": {
        "json_valid_count": 5,
        "json_valid_total": 5,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 5,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 0.4,
          "method": 0.2,
          "key_result": 0.1,
          "model_or_system": 0.6,
          "benchmark": 0.6
        },
        "overall_field_emr": 0.38
      }
    },
    "gpt4_extraction_C2_abs_027": {
      "n_outputs": 5,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 102.8,
        "std": 82.01438898144643,
        "min": 5,
        "max": 218,
        "normalized_mean": 0.11933756660177323
      },
      "rouge_l": {
        "mean": 0.9134525314675237,
        "std": 0.06396471369865507,
        "min": 0.8246445497630333,
        "max": 0.9894736842105263
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9831347763538361,
        "bertscore_f1_std": 0.0104454126656807,
        "bertscore_f1_min": 0.9692434668540955,
        "bertscore_f1_max": 0.9988033771514893,
        "bertscore_precision_mean": 0.9873329877853394,
        "bertscore_recall_mean": 0.9790323734283447
      },
      "avg_output_length_chars": 767.4,
      "avg_output_length_words": 97.6,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_027",
      "json_validity": {
        "json_valid_count": 5,
        "json_valid_total": 5,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 5,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 1.0,
          "method": 0.0,
          "key_result": 0.6,
          "model_or_system": 0.4,
          "benchmark": 1.0
        },
        "overall_field_emr": 0.6
      }
    },
    "gpt4_extraction_C2_abs_028": {
      "n_outputs": 5,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 239.7,
        "std": 97.10514919405665,
        "min": 92,
        "max": 384,
        "normalized_mean": 0.2175257866634513
      },
      "rouge_l": {
        "mean": 0.8113886263983744,
        "std": 0.08738354649769337,
        "min": 0.6877192982456141,
        "max": 0.9190938511326862
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9683189988136292,
        "bertscore_f1_std": 0.016725667132576203,
        "bertscore_f1_min": 0.9446138739585876,
        "bertscore_f1_max": 0.9895703196525574,
        "bertscore_precision_mean": 0.9730845332145691,
        "bertscore_recall_mean": 0.9637141525745392
      },
      "avg_output_length_chars": 1051.2,
      "avg_output_length_words": 151.8,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_028",
      "json_validity": {
        "json_valid_count": 5,
        "json_valid_total": 5,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 5,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 1.0,
          "method": 1.0,
          "key_result": 0.0,
          "model_or_system": 1.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 0.8
      }
    },
    "gpt4_extraction_C2_abs_029": {
      "n_outputs": 5,
      "exact_match_rate": 0.4,
      "edit_distance": {
        "mean": 23.4,
        "std": 19.106019993708788,
        "min": 0,
        "max": 39,
        "normalized_mean": 0.02782401902497027
      },
      "rouge_l": {
        "mean": 0.972972972972973,
        "std": 0.022067475160208804,
        "min": 0.954954954954955,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9950791001319885,
        "bertscore_f1_std": 0.0040178735839573685,
        "bertscore_f1_min": 0.9917985200881958,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9941205382347107,
        "bertscore_recall_mean": 0.9960446715354919
      },
      "avg_output_length_chars": 818.8,
      "avg_output_length_words": 110.4,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_029",
      "json_validity": {
        "json_valid_count": 5,
        "json_valid_total": 5,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 5,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 0.4,
          "method": 1.0,
          "key_result": 0.4,
          "model_or_system": 1.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 0.76
      }
    },
    "gpt4_extraction_C2_abs_030": {
      "n_outputs": 5,
      "exact_match_rate": 0.3,
      "edit_distance": {
        "mean": 40.9,
        "std": 37.23828674898994,
        "min": 0,
        "max": 94,
        "normalized_mean": 0.06097072182110996
      },
      "rouge_l": {
        "mean": 0.9325111311713522,
        "std": 0.05640717219754834,
        "min": 0.8522727272727274,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.989775562286377,
        "bertscore_f1_std": 0.0075233340053887255,
        "bertscore_f1_min": 0.9781045913696289,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9914877951145172,
        "bertscore_recall_mean": 0.9880881786346436
      },
      "avg_output_length_chars": 661.8,
      "avg_output_length_words": 92.2,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_030",
      "json_validity": {
        "json_valid_count": 5,
        "json_valid_total": 5,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 5,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 0.6,
          "method": 0.6,
          "key_result": 0.6,
          "model_or_system": 0.4,
          "benchmark": 1.0
        },
        "overall_field_emr": 0.64
      }
    },
    "gpt4_extraction_C3_t0.0_abs_001": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 10.666666666666666,
        "std": 4.988876515698588,
        "min": 4,
        "max": 16,
        "normalized_mean": 0.014397445857603644
      },
      "rouge_l": {
        "mean": 0.9859831856157749,
        "std": 0.0065446808057091615,
        "min": 0.9789473684210526,
        "max": 0.9947089947089947
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9976946910222372,
        "bertscore_f1_std": 0.0011958837636833017,
        "bertscore_f1_min": 0.9966101050376892,
        "bertscore_f1_max": 0.9993607997894287,
        "bertscore_precision_mean": 0.997994065284729,
        "bertscore_recall_mean": 0.9973956346511841
      },
      "avg_output_length_chars": 736.0,
      "avg_output_length_words": 95.0,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_001",
      "json_validity": {
        "json_valid_count": 3,
        "json_valid_total": 3,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 3,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 1.0,
          "method": 1.0,
          "key_result": 0.3333333333333333,
          "model_or_system": 1.0,
          "benchmark": 0.3333333333333333
        },
        "overall_field_emr": 0.7333333333333333
      }
    },
    "gpt4_extraction_C3_t0.0_abs_002": {
      "n_outputs": 3,
      "exact_match_rate": 0.3333333333333333,
      "edit_distance": {
        "mean": 2.0,
        "std": 1.4142135623730951,
        "min": 0,
        "max": 3,
        "normalized_mean": 0.001895734597156398
      },
      "rouge_l": {
        "mean": 0.9859154929577465,
        "std": 0.009959250439247135,
        "min": 0.9788732394366197,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9983043471972147,
        "bertscore_f1_std": 0.001198965448538906,
        "bertscore_f1_min": 0.9974565505981445,
        "bertscore_f1_max": 0.9999999403953552,
        "bertscore_precision_mean": 0.9981318513552347,
        "bertscore_recall_mean": 0.9984770019849142
      },
      "avg_output_length_chars": 1053.0,
      "avg_output_length_words": 142.0,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_002",
      "json_validity": {
        "json_valid_count": 3,
        "json_valid_total": 3,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 3,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 0.3333333333333333,
          "method": 0.3333333333333333,
          "key_result": 0.3333333333333333,
          "model_or_system": 1.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 0.6
      }
    },
    "gpt4_extraction_C3_t0.0_abs_003": {
      "n_outputs": 3,
      "exact_match_rate": 0.3333333333333333,
      "edit_distance": {
        "mean": 10.0,
        "std": 7.0710678118654755,
        "min": 0,
        "max": 15,
        "normalized_mean": 0.010649627263045794
      },
      "rouge_l": {
        "mean": 0.99163179916318,
        "std": 0.005917211558046412,
        "min": 0.9874476987447699,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9967467983563741,
        "bertscore_f1_std": 0.0023003187959265816,
        "bertscore_f1_min": 0.9951202273368835,
        "bertscore_f1_max": 0.9999999403953552,
        "bertscore_precision_mean": 0.9975510636965433,
        "bertscore_recall_mean": 0.9959444403648376
      },
      "avg_output_length_chars": 934.0,
      "avg_output_length_words": 120.0,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_003",
      "json_validity": {
        "json_valid_count": 3,
        "json_valid_total": 3,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 3,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 1.0,
          "method": 0.3333333333333333,
          "key_result": 1.0,
          "model_or_system": 1.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 0.8666666666666667
      }
    },
    "gpt4_extraction_C3_t0.0_abs_004": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 70.66666666666667,
        "std": 8.956685895029603,
        "min": 62,
        "max": 83,
        "normalized_mean": 0.10133691682172145
      },
      "rouge_l": {
        "mean": 0.921252071935691,
        "std": 0.00572548508734788,
        "min": 0.9152542372881356,
        "max": 0.9289617486338798
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9867684046427408,
        "bertscore_f1_std": 0.0020084211762669397,
        "bertscore_f1_min": 0.9839281439781189,
        "bertscore_f1_max": 0.9882065057754517,
        "bertscore_precision_mean": 0.9847751458485922,
        "bertscore_recall_mean": 0.988773783047994
      },
      "avg_output_length_chars": 675.3333333333334,
      "avg_output_length_words": 89.0,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_004",
      "json_validity": {
        "json_valid_count": 3,
        "json_valid_total": 3,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 3,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 1.0,
          "method": 0.0,
          "key_result": 1.0,
          "model_or_system": 1.0,
          "benchmark": 0.3333333333333333
        },
        "overall_field_emr": 0.6666666666666666
      }
    },
    "gpt4_extraction_C3_t0.0_abs_005": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 628.0,
      "avg_output_length_words": 85.0,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_005",
      "json_validity": {
        "json_valid_count": 3,
        "json_valid_total": 3,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 3,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 1.0,
          "method": 1.0,
          "key_result": 1.0,
          "model_or_system": 1.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 1.0
      }
    },
    "gpt4_extraction_C3_t0.0_abs_006": {
      "n_outputs": 3,
      "exact_match_rate": 0.3333333333333333,
      "edit_distance": {
        "mean": 142.0,
        "std": 100.40916292848975,
        "min": 0,
        "max": 213,
        "normalized_mean": 0.1463917525773196
      },
      "rouge_l": {
        "mean": 0.8942189421894219,
        "std": 0.07479850329894602,
        "min": 0.8413284132841328,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9832875728607178,
        "bertscore_f1_std": 0.011817470560272556,
        "bertscore_f1_min": 0.9749313592910767,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9763779640197754,
        "bertscore_recall_mean": 0.9903473059336344
      },
      "avg_output_length_chars": 874.0,
      "avg_output_length_words": 132.33333333333334,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_006",
      "json_validity": {
        "json_valid_count": 3,
        "json_valid_total": 3,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 3,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 1.0,
          "method": 0.3333333333333333,
          "key_result": 0.3333333333333333,
          "model_or_system": 1.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 0.7333333333333333
      }
    },
    "gpt4_extraction_C3_t0.0_abs_007": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 34.0,
        "std": 13.490737563232042,
        "min": 18,
        "max": 51,
        "normalized_mean": 0.04491484015077482
      },
      "rouge_l": {
        "mean": 0.9641295132079152,
        "std": 0.014191481505915713,
        "min": 0.9458128078817734,
        "max": 0.9803921568627451
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9936962922414144,
        "bertscore_f1_std": 0.002319998607684038,
        "bertscore_f1_min": 0.9904471039772034,
        "bertscore_f1_max": 0.9957154393196106,
        "bertscore_precision_mean": 0.9942602515220642,
        "bertscore_recall_mean": 0.9931349953015646
      },
      "avg_output_length_chars": 747.0,
      "avg_output_length_words": 102.33333333333333,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_007",
      "json_validity": {
        "json_valid_count": 3,
        "json_valid_total": 3,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 3,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 1.0,
          "method": 1.0,
          "key_result": 0.3333333333333333,
          "model_or_system": 1.0,
          "benchmark": 0.3333333333333333
        },
        "overall_field_emr": 0.7333333333333333
      }
    },
    "gpt4_extraction_C3_t0.0_abs_008": {
      "n_outputs": 3,
      "exact_match_rate": 0.3333333333333333,
      "edit_distance": {
        "mean": 2.0,
        "std": 1.4142135623730951,
        "min": 0,
        "max": 3,
        "normalized_mean": 0.0029282576866764276
      },
      "rouge_l": {
        "mean": 0.9885714285714284,
        "std": 0.00808122035641773,
        "min": 0.9828571428571428,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9976280132929484,
        "bertscore_f1_std": 0.0016772478854405314,
        "bertscore_f1_min": 0.9964420199394226,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9975879987080892,
        "bertscore_recall_mean": 0.9976680278778076
      },
      "avg_output_length_chars": 681.6666666666666,
      "avg_output_length_words": 87.33333333333333,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_008",
      "json_validity": {
        "json_valid_count": 3,
        "json_valid_total": 3,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 3,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 1.0,
          "method": 1.0,
          "key_result": 1.0,
          "model_or_system": 0.3333333333333333,
          "benchmark": 1.0
        },
        "overall_field_emr": 0.8666666666666667
      }
    },
    "gpt4_extraction_C3_t0.0_abs_009": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 394.0,
        "std": 24.752104287649296,
        "min": 359,
        "max": 412,
        "normalized_mean": 0.36814568230287015
      },
      "rouge_l": {
        "mean": 0.7395426769170116,
        "std": 0.047946804573115114,
        "min": 0.7000000000000001,
        "max": 0.8070175438596492
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9554406801859537,
        "bertscore_f1_std": 0.008228553762375343,
        "bertscore_f1_min": 0.9468952417373657,
        "bertscore_f1_max": 0.9665541052818298,
        "bertscore_precision_mean": 0.963896632194519,
        "bertscore_recall_mean": 0.9472326238950094
      },
      "avg_output_length_chars": 983.6666666666666,
      "avg_output_length_words": 132.0,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_009",
      "json_validity": {
        "json_valid_count": 3,
        "json_valid_total": 3,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 3,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 0.3333333333333333,
          "method": 0.0,
          "key_result": 0.0,
          "model_or_system": 0.3333333333333333,
          "benchmark": 0.3333333333333333
        },
        "overall_field_emr": 0.2
      }
    },
    "gpt4_extraction_C3_t0.0_abs_010": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 196.66666666666666,
        "std": 29.10135544762286,
        "min": 157,
        "max": 226,
        "normalized_mean": 0.16072473100968634
      },
      "rouge_l": {
        "mean": 0.8193306236762106,
        "std": 0.029954452459788287,
        "min": 0.7932011331444759,
        "max": 0.861271676300578
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9756910800933838,
        "bertscore_f1_std": 0.0027169463382703926,
        "bertscore_f1_min": 0.9735128283500671,
        "bertscore_f1_max": 0.9795213937759399,
        "bertscore_precision_mean": 0.9798293113708496,
        "bertscore_recall_mean": 0.9715924461682638
      },
      "avg_output_length_chars": 1198.6666666666667,
      "avg_output_length_words": 173.33333333333334,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_010",
      "json_validity": {
        "json_valid_count": 3,
        "json_valid_total": 3,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 3,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 0.0,
          "method": 0.0,
          "key_result": 0.0,
          "model_or_system": 1.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 0.4
      }
    },
    "gpt4_extraction_C3_t0.0_abs_011": {
      "n_outputs": 2,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9999999403953552,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9999999403953552,
        "bertscore_f1_max": 0.9999999403953552,
        "bertscore_precision_mean": 0.9999999403953552,
        "bertscore_recall_mean": 0.9999999403953552
      },
      "avg_output_length_chars": 970.0,
      "avg_output_length_words": 139.0,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_011",
      "json_validity": {
        "json_valid_count": 2,
        "json_valid_total": 2,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 2,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 1.0,
          "method": 1.0,
          "key_result": 1.0,
          "model_or_system": 1.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 1.0
      }
    },
    "gpt4_extraction_C3_t0.0_abs_019": {
      "n_outputs": 2,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 101.0,
        "std": 0.0,
        "min": 101,
        "max": 101,
        "normalized_mean": 0.15805946791862285
      },
      "rouge_l": {
        "mean": 0.8074534161490684,
        "std": 0.0,
        "min": 0.8074534161490684,
        "max": 0.8074534161490684
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9746075868606567,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9746075868606567,
        "bertscore_f1_max": 0.9746075868606567,
        "bertscore_precision_mean": 0.9704822301864624,
        "bertscore_recall_mean": 0.9787681102752686
      },
      "avg_output_length_chars": 620.5,
      "avg_output_length_words": 80.5,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_019",
      "json_validity": {
        "json_valid_count": 2,
        "json_valid_total": 2,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 2,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 1.0,
          "method": 0.0,
          "key_result": 1.0,
          "model_or_system": 1.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 0.8
      }
    },
    "gpt4_extraction_C3_t0.0_abs_021": {
      "n_outputs": 2,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9999999403953552,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9999999403953552,
        "bertscore_f1_max": 0.9999999403953552,
        "bertscore_precision_mean": 0.9999999403953552,
        "bertscore_recall_mean": 0.9999999403953552
      },
      "avg_output_length_chars": 622.0,
      "avg_output_length_words": 78.0,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_021",
      "json_validity": {
        "json_valid_count": 2,
        "json_valid_total": 2,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 2,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 1.0,
          "method": 1.0,
          "key_result": 1.0,
          "model_or_system": 1.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 1.0
      }
    },
    "gpt4_extraction_C3_t0.0_abs_025": {
      "n_outputs": 2,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9999999403953552,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9999999403953552,
        "bertscore_f1_max": 0.9999999403953552,
        "bertscore_precision_mean": 0.9999999403953552,
        "bertscore_recall_mean": 0.9999999403953552
      },
      "avg_output_length_chars": 595.0,
      "avg_output_length_words": 86.0,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_025",
      "json_validity": {
        "json_valid_count": 2,
        "json_valid_total": 2,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 2,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 1.0,
          "method": 1.0,
          "key_result": 1.0,
          "model_or_system": 1.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 1.0
      }
    },
    "gpt4_extraction_C3_t0.3_abs_001": {
      "n_outputs": 3,
      "exact_match_rate": 0.3333333333333333,
      "edit_distance": {
        "mean": 1.3333333333333333,
        "std": 0.9428090415820634,
        "min": 0,
        "max": 2,
        "normalized_mean": 0.001828989483310471
      },
      "rouge_l": {
        "mean": 0.9858156028368793,
        "std": 0.010029883421085795,
        "min": 0.9787234042553191,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9987607796986898,
        "bertscore_f1_std": 0.000876345372137516,
        "bertscore_f1_min": 0.9981411099433899,
        "bertscore_f1_max": 1.0000001192092896,
        "bertscore_precision_mean": 0.9988478422164917,
        "bertscore_recall_mean": 0.9986737171808878
      },
      "avg_output_length_chars": 727.6666666666666,
      "avg_output_length_words": 94.0,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_001",
      "json_validity": {
        "json_valid_count": 3,
        "json_valid_total": 3,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 3,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 0.3333333333333333,
          "method": 1.0,
          "key_result": 0.3333333333333333,
          "model_or_system": 1.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 0.7333333333333333
      }
    },
    "gpt4_extraction_C3_t0.3_abs_002": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 248.0,
        "std": 57.867665121954474,
        "min": 169,
        "max": 306,
        "normalized_mean": 0.21808548801015482
      },
      "rouge_l": {
        "mean": 0.8188696152656384,
        "std": 0.027555796418850677,
        "min": 0.7905405405405406,
        "max": 0.8562091503267975
      },
      "bert_score": {
        "bertscore_f1_mean": 0.976080079873403,
        "bertscore_f1_std": 0.002740400415860261,
        "bertscore_f1_min": 0.9729710817337036,
        "bertscore_f1_max": 0.979638397693634,
        "bertscore_precision_mean": 0.9794505635897318,
        "bertscore_recall_mean": 0.9727361003557841
      },
      "avg_output_length_chars": 1079.0,
      "avg_output_length_words": 147.66666666666666,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_002",
      "json_validity": {
        "json_valid_count": 3,
        "json_valid_total": 3,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 3,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 0.0,
          "method": 0.0,
          "key_result": 0.3333333333333333,
          "model_or_system": 1.0,
          "benchmark": 0.3333333333333333
        },
        "overall_field_emr": 0.3333333333333333
      }
    },
    "gpt4_extraction_C3_t0.3_abs_003": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9999999403953552,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9999999403953552,
        "bertscore_f1_max": 0.9999999403953552,
        "bertscore_precision_mean": 0.9999999403953552,
        "bertscore_recall_mean": 0.9999999403953552
      },
      "avg_output_length_chars": 924.0,
      "avg_output_length_words": 118.0,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_003",
      "json_validity": {
        "json_valid_count": 3,
        "json_valid_total": 3,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 3,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 1.0,
          "method": 1.0,
          "key_result": 1.0,
          "model_or_system": 1.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 1.0
      }
    },
    "gpt4_extraction_C3_t0.3_abs_004": {
      "n_outputs": 3,
      "exact_match_rate": 0.3333333333333333,
      "edit_distance": {
        "mean": 56.666666666666664,
        "std": 40.069384267237695,
        "min": 0,
        "max": 85,
        "normalized_mean": 0.08923884514435697
      },
      "rouge_l": {
        "mean": 0.9287211740041929,
        "std": 0.050401741216651176,
        "min": 0.8930817610062893,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9904755353927612,
        "bertscore_f1_std": 0.006734813510949801,
        "bertscore_f1_min": 0.9857133030891418,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9860933621724447,
        "bertscore_recall_mean": 0.9949169953664144
      },
      "avg_output_length_chars": 618.3333333333334,
      "avg_output_length_words": 81.0,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_004",
      "json_validity": {
        "json_valid_count": 3,
        "json_valid_total": 3,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 3,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 1.0,
          "method": 0.3333333333333333,
          "key_result": 1.0,
          "model_or_system": 1.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 0.8666666666666667
      }
    },
    "gpt4_extraction_C3_t0.3_abs_005": {
      "n_outputs": 3,
      "exact_match_rate": 0.3333333333333333,
      "edit_distance": {
        "mean": 20.0,
        "std": 14.142135623730951,
        "min": 0,
        "max": 30,
        "normalized_mean": 0.030441400304414
      },
      "rouge_l": {
        "mean": 0.9733333333333333,
        "std": 0.018856180831641284,
        "min": 0.96,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9941317041714987,
        "bertscore_f1_std": 0.004149469627493546,
        "bertscore_f1_min": 0.9911975860595703,
        "bertscore_f1_max": 0.9999999403953552,
        "bertscore_precision_mean": 0.9951572418212891,
        "bertscore_recall_mean": 0.9931093454360962
      },
      "avg_output_length_chars": 637.6666666666666,
      "avg_output_length_words": 86.66666666666667,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_005",
      "json_validity": {
        "json_valid_count": 3,
        "json_valid_total": 3,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 3,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 1.0,
          "method": 0.3333333333333333,
          "key_result": 1.0,
          "model_or_system": 1.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 0.8666666666666667
      }
    },
    "gpt4_extraction_C3_t0.3_abs_006": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 132.0,
        "std": 59.90548110704618,
        "min": 53,
        "max": 198,
        "normalized_mean": 0.14175269390458653
      },
      "rouge_l": {
        "mean": 0.8610763314102038,
        "std": 0.05868132668495487,
        "min": 0.7956204379562043,
        "max": 0.937984496124031
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9810376962025961,
        "bertscore_f1_std": 0.007499715812633669,
        "bertscore_f1_min": 0.9728183150291443,
        "bertscore_f1_max": 0.9909525513648987,
        "bertscore_precision_mean": 0.9830875992774963,
        "bertscore_recall_mean": 0.9790882468223572
      },
      "avg_output_length_chars": 885.6666666666666,
      "avg_output_length_words": 133.0,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_006",
      "json_validity": {
        "json_valid_count": 3,
        "json_valid_total": 3,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 3,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 0.3333333333333333,
          "method": 0.3333333333333333,
          "key_result": 0.0,
          "model_or_system": 0.3333333333333333,
          "benchmark": 1.0
        },
        "overall_field_emr": 0.4
      }
    },
    "gpt4_extraction_C3_t0.3_abs_007": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 47.333333333333336,
        "std": 13.224556283251582,
        "min": 37,
        "max": 66,
        "normalized_mean": 0.06314829777476907
      },
      "rouge_l": {
        "mean": 0.9403061816854921,
        "std": 0.011196278282281475,
        "min": 0.9292929292929293,
        "max": 0.9556650246305419
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9935810764630636,
        "bertscore_f1_std": 0.0012367327611844119,
        "bertscore_f1_min": 0.9926242828369141,
        "bertscore_f1_max": 0.9953274130821228,
        "bertscore_precision_mean": 0.9935176769892374,
        "bertscore_recall_mean": 0.9936483899752299
      },
      "avg_output_length_chars": 734.0,
      "avg_output_length_words": 100.66666666666667,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_007",
      "json_validity": {
        "json_valid_count": 3,
        "json_valid_total": 3,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 3,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 0.3333333333333333,
          "method": 0.3333333333333333,
          "key_result": 0.0,
          "model_or_system": 1.0,
          "benchmark": 0.3333333333333333
        },
        "overall_field_emr": 0.4
      }
    },
    "gpt4_extraction_C3_t0.3_abs_008": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 287.0,
        "std": 197.9915823126495,
        "min": 7,
        "max": 428,
        "normalized_mean": 0.3983248380895141
      },
      "rouge_l": {
        "mean": 0.7259336962797814,
        "std": 0.18207932066978877,
        "min": 0.5955056179775282,
        "max": 0.9834254143646408
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9559764464696249,
        "bertscore_f1_std": 0.028659849318677644,
        "bertscore_f1_min": 0.9350332617759705,
        "bertscore_f1_max": 0.9964999556541443,
        "bertscore_precision_mean": 0.9555741548538208,
        "bertscore_recall_mean": 0.9563790758450826
      },
      "avg_output_length_chars": 707.0,
      "avg_output_length_words": 89.33333333333333,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_008",
      "json_validity": {
        "json_valid_count": 3,
        "json_valid_total": 3,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 3,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 0.3333333333333333,
          "method": 0.0,
          "key_result": 0.3333333333333333,
          "model_or_system": 1.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 0.5333333333333333
      }
    },
    "gpt4_extraction_C3_t0.3_abs_009": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 254.0,
        "std": 38.02630668366309,
        "min": 206,
        "max": 299,
        "normalized_mean": 0.23117292251863278
      },
      "rouge_l": {
        "mean": 0.7903288400849376,
        "std": 0.02278002494857387,
        "min": 0.7709090909090909,
        "max": 0.822299651567944
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9648989041646322,
        "bertscore_f1_std": 0.004025440897622395,
        "bertscore_f1_min": 0.9592840671539307,
        "bertscore_f1_max": 0.9685196280479431,
        "bertscore_precision_mean": 0.9707015951474508,
        "bertscore_recall_mean": 0.9591703414916992
      },
      "avg_output_length_chars": 1010.0,
      "avg_output_length_words": 135.66666666666666,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_009",
      "json_validity": {
        "json_valid_count": 3,
        "json_valid_total": 3,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 3,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 0.0,
          "method": 0.0,
          "key_result": 0.0,
          "model_or_system": 0.3333333333333333,
          "benchmark": 0.3333333333333333
        },
        "overall_field_emr": 0.13333333333333333
      }
    },
    "gpt4_extraction_C3_t0.3_abs_010": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 234.66666666666666,
        "std": 42.679685513784605,
        "min": 203,
        "max": 295,
        "normalized_mean": 0.2037249729595132
      },
      "rouge_l": {
        "mean": 0.7706501219085459,
        "std": 0.03741921345097054,
        "min": 0.7192429022082019,
        "max": 0.8072289156626506
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9654143651326498,
        "bertscore_f1_std": 0.0074416493265569,
        "bertscore_f1_min": 0.9558246731758118,
        "bertscore_f1_max": 0.9739636182785034,
        "bertscore_precision_mean": 0.9604960282643636,
        "bertscore_recall_mean": 0.970412035783132
      },
      "avg_output_length_chars": 1088.0,
      "avg_output_length_words": 158.66666666666666,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_010",
      "json_validity": {
        "json_valid_count": 3,
        "json_valid_total": 3,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 3,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 0.0,
          "method": 0.0,
          "key_result": 1.0,
          "model_or_system": 1.0,
          "benchmark": 0.3333333333333333
        },
        "overall_field_emr": 0.4666666666666667
      }
    },
    "gpt4_extraction_C3_t0.3_abs_011": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 289.0,
        "std": 16.30950643030009,
        "min": 267,
        "max": 306,
        "normalized_mean": 0.3081463200867219
      },
      "rouge_l": {
        "mean": 0.7010483400711959,
        "std": 0.02279330810118328,
        "min": 0.6818181818181818,
        "max": 0.7330677290836652
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9561156829198202,
        "bertscore_f1_std": 0.0028805559045461714,
        "bertscore_f1_min": 0.9529463052749634,
        "bertscore_f1_max": 0.9599168300628662,
        "bertscore_precision_mean": 0.9566648006439209,
        "bertscore_recall_mean": 0.9555930296579996
      },
      "avg_output_length_chars": 914.0,
      "avg_output_length_words": 127.0,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_011",
      "json_validity": {
        "json_valid_count": 3,
        "json_valid_total": 3,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 3,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 0.0,
          "method": 0.0,
          "key_result": 0.0,
          "model_or_system": 0.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 0.2
      }
    },
    "gpt4_extraction_C3_t0.3_abs_017": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 12.0,
        "std": 0.816496580927726,
        "min": 11,
        "max": 13,
        "normalized_mean": 0.021778584392014518
      },
      "rouge_l": {
        "mean": 0.9602060338484181,
        "std": 0.009282402484450367,
        "min": 0.9536423841059603,
        "max": 0.9733333333333334
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9966463843981425,
        "bertscore_f1_std": 0.0004494520541544316,
        "bertscore_f1_min": 0.9962176084518433,
        "bertscore_f1_max": 0.9972671270370483,
        "bertscore_precision_mean": 0.9967001676559448,
        "bertscore_recall_mean": 0.996592660744985
      },
      "avg_output_length_chars": 549.6666666666666,
      "avg_output_length_words": 75.33333333333333,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_017",
      "json_validity": {
        "json_valid_count": 3,
        "json_valid_total": 3,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 3,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 0.3333333333333333,
          "method": 0.3333333333333333,
          "key_result": 0.3333333333333333,
          "model_or_system": 1.0,
          "benchmark": 0.3333333333333333
        },
        "overall_field_emr": 0.4666666666666667
      }
    },
    "gpt4_extraction_C3_t0.3_abs_026": {
      "n_outputs": 2,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 298.0,
        "std": 0.0,
        "min": 298,
        "max": 298,
        "normalized_mean": 0.29475766567754697
      },
      "rouge_l": {
        "mean": 0.723076923076923,
        "std": 0.0,
        "min": 0.723076923076923,
        "max": 0.723076923076923
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9523472189903259,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9523472189903259,
        "bertscore_f1_max": 0.9523472189903259,
        "bertscore_precision_mean": 0.9597543478012085,
        "bertscore_recall_mean": 0.9450535178184509
      },
      "avg_output_length_chars": 957.0,
      "avg_output_length_words": 130.0,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_026",
      "json_validity": {
        "json_valid_count": 2,
        "json_valid_total": 2,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 2,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 0.0,
          "method": 0.0,
          "key_result": 0.0,
          "model_or_system": 0.0,
          "benchmark": 0.0
        },
        "overall_field_emr": 0.0
      }
    },
    "gpt4_extraction_C3_t0.3_abs_027": {
      "n_outputs": 2,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 49.0,
        "std": 0.0,
        "min": 49,
        "max": 49,
        "normalized_mean": 0.06472919418758256
      },
      "rouge_l": {
        "mean": 0.9578947368421052,
        "std": 0.0,
        "min": 0.9578947368421052,
        "max": 0.9578947368421052
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9932746887207031,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9932746887207031,
        "bertscore_f1_max": 0.9932746887207031,
        "bertscore_precision_mean": 0.9924720525741577,
        "bertscore_recall_mean": 0.9940786957740784
      },
      "avg_output_length_chars": 747.0,
      "avg_output_length_words": 95.0,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_027",
      "json_validity": {
        "json_valid_count": 2,
        "json_valid_total": 2,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 2,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 1.0,
          "method": 0.0,
          "key_result": 1.0,
          "model_or_system": 1.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 0.8
      }
    },
    "gpt4_extraction_C3_t0.7_abs_001": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 282.6666666666667,
        "std": 187.87643693543785,
        "min": 17,
        "max": 419,
        "normalized_mean": 0.26291602662570407
      },
      "rouge_l": {
        "mean": 0.7878012210053053,
        "std": 0.12773992147679153,
        "min": 0.6945606694560669,
        "max": 0.968421052631579
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9663747151692709,
        "bertscore_f1_std": 0.020135118941033033,
        "bertscore_f1_min": 0.9515687823295593,
        "bertscore_f1_max": 0.9948424100875854,
        "bertscore_precision_mean": 0.9585636655489603,
        "bertscore_recall_mean": 0.974372923374176
      },
      "avg_output_length_chars": 852.6666666666666,
      "avg_output_length_words": 111.0,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_001",
      "json_validity": {
        "json_valid_count": 3,
        "json_valid_total": 3,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 3,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 0.0,
          "method": 0.3333333333333333,
          "key_result": 0.0,
          "model_or_system": 0.3333333333333333,
          "benchmark": 0.3333333333333333
        },
        "overall_field_emr": 0.2
      }
    },
    "gpt4_extraction_C3_t0.7_abs_002": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 265.6666666666667,
        "std": 69.2451362110646,
        "min": 177,
        "max": 346,
        "normalized_mean": 0.24816043648160435
      },
      "rouge_l": {
        "mean": 0.7787319145622394,
        "std": 0.03050961035505506,
        "min": 0.736111111111111,
        "max": 0.8058608058608059
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9640333851178488,
        "bertscore_f1_std": 0.00784645998886019,
        "bertscore_f1_min": 0.9563291668891907,
        "bertscore_f1_max": 0.9748017191886902,
        "bertscore_precision_mean": 0.96271280447642,
        "bertscore_recall_mean": 0.9653581579526266
      },
      "avg_output_length_chars": 1021.0,
      "avg_output_length_words": 139.66666666666666,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_002",
      "json_validity": {
        "json_valid_count": 3,
        "json_valid_total": 3,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 3,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 0.0,
          "method": 0.0,
          "key_result": 0.3333333333333333,
          "model_or_system": 0.3333333333333333,
          "benchmark": 0.3333333333333333
        },
        "overall_field_emr": 0.2
      }
    },
    "gpt4_extraction_C3_t0.7_abs_003": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 112.0,
        "std": 72.24033960791344,
        "min": 10,
        "max": 168,
        "normalized_mean": 0.12167476289613695
      },
      "rouge_l": {
        "mean": 0.9068324621152121,
        "std": 0.06000838429402495,
        "min": 0.8598130841121496,
        "max": 0.9915254237288136
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9849889675776163,
        "bertscore_f1_std": 0.010488175135828648,
        "bertscore_f1_min": 0.97746342420578,
        "bertscore_f1_max": 0.9998209476470947,
        "bertscore_precision_mean": 0.9912120898564657,
        "bertscore_recall_mean": 0.9788825114568075
      },
      "avg_output_length_chars": 869.0,
      "avg_output_length_words": 110.66666666666667,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_003",
      "json_validity": {
        "json_valid_count": 3,
        "json_valid_total": 3,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 3,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 0.0,
          "method": 0.3333333333333333,
          "key_result": 0.3333333333333333,
          "model_or_system": 1.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 0.5333333333333333
      }
    },
    "gpt4_extraction_C3_t0.7_abs_004": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 95.33333333333333,
        "std": 11.61416759345623,
        "min": 79,
        "max": 105,
        "normalized_mean": 0.1527754491854584
      },
      "rouge_l": {
        "mean": 0.8700114075114076,
        "std": 0.018658038706808967,
        "min": 0.8441558441558441,
        "max": 0.8875
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9787997603416443,
        "bertscore_f1_std": 0.0030722071225004533,
        "bertscore_f1_min": 0.9746755361557007,
        "bertscore_f1_max": 0.9820454120635986,
        "bertscore_precision_mean": 0.9770380258560181,
        "bertscore_recall_mean": 0.9806425174077352
      },
      "avg_output_length_chars": 599.6666666666666,
      "avg_output_length_words": 77.0,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_004",
      "json_validity": {
        "json_valid_count": 3,
        "json_valid_total": 3,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 3,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 0.3333333333333333,
          "method": 0.0,
          "key_result": 0.3333333333333333,
          "model_or_system": 1.0,
          "benchmark": 0.3333333333333333
        },
        "overall_field_emr": 0.4
      }
    },
    "gpt4_extraction_C3_t0.7_abs_005": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 25.0,
        "std": 9.93310961716756,
        "min": 11,
        "max": 33,
        "normalized_mean": 0.03827019561418343
      },
      "rouge_l": {
        "mean": 0.9351004060778693,
        "std": 0.02263115666049569,
        "min": 0.9132947976878613,
        "max": 0.9662921348314608
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9910517533620199,
        "bertscore_f1_std": 0.002779445234973325,
        "bertscore_f1_min": 0.9890834093093872,
        "bertscore_f1_max": 0.9949824810028076,
        "bertscore_precision_mean": 0.9906232555707296,
        "bertscore_recall_mean": 0.9914820988972982
      },
      "avg_output_length_chars": 645.3333333333334,
      "avg_output_length_words": 87.66666666666667,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_005",
      "json_validity": {
        "json_valid_count": 3,
        "json_valid_total": 3,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 3,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 0.0,
          "method": 0.0,
          "key_result": 0.3333333333333333,
          "model_or_system": 1.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 0.4666666666666667
      }
    },
    "gpt4_extraction_C3_t0.7_abs_006": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 319.0,
        "std": 38.21866908549625,
        "min": 290,
        "max": 373,
        "normalized_mean": 0.3842885375494071
      },
      "rouge_l": {
        "mean": 0.63016599740956,
        "std": 0.01518216687285468,
        "min": 0.6086956521739131,
        "max": 0.641025641025641
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9478335579236349,
        "bertscore_f1_std": 0.0004808801288710856,
        "bertscore_f1_min": 0.9474826455116272,
        "bertscore_f1_max": 0.9485135078430176,
        "bertscore_precision_mean": 0.9620081981023153,
        "bertscore_recall_mean": 0.9341446161270142
      },
      "avg_output_length_chars": 721.0,
      "avg_output_length_words": 104.0,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_006",
      "json_validity": {
        "json_valid_count": 3,
        "json_valid_total": 3,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 3,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 0.3333333333333333,
          "method": 0.0,
          "key_result": 0.0,
          "model_or_system": 0.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 0.26666666666666666
      }
    },
    "gpt4_extraction_C3_t0.7_abs_007": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 115.33333333333333,
        "std": 15.797327481430381,
        "min": 93,
        "max": 127,
        "normalized_mean": 0.14424658777309826
      },
      "rouge_l": {
        "mean": 0.8660441752589708,
        "std": 0.023274655947592846,
        "min": 0.8333333333333334,
        "max": 0.8855721393034826
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9807379245758057,
        "bertscore_f1_std": 0.0011611382118949544,
        "bertscore_f1_min": 0.9791747331619263,
        "bertscore_f1_max": 0.9819550514221191,
        "bertscore_precision_mean": 0.9810546437899271,
        "bertscore_recall_mean": 0.9804515242576599
      },
      "avg_output_length_chars": 761.0,
      "avg_output_length_words": 104.0,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_007",
      "json_validity": {
        "json_valid_count": 3,
        "json_valid_total": 3,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 3,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 0.3333333333333333,
          "method": 0.0,
          "key_result": 0.0,
          "model_or_system": 1.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 0.4666666666666667
      }
    },
    "gpt4_extraction_C3_t0.7_abs_008": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 196.0,
        "std": 69.6323679524592,
        "min": 100,
        "max": 263,
        "normalized_mean": 0.26161741800293586
      },
      "rouge_l": {
        "mean": 0.7158330678560052,
        "std": 0.08407857687504157,
        "min": 0.616279069767442,
        "max": 0.821917808219178
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9635287125905355,
        "bertscore_f1_std": 0.015619915311650159,
        "bertscore_f1_min": 0.9495698809623718,
        "bertscore_f1_max": 0.9853349924087524,
        "bertscore_precision_mean": 0.9729244112968445,
        "bertscore_recall_mean": 0.9543937643369039
      },
      "avg_output_length_chars": 663.3333333333334,
      "avg_output_length_words": 81.66666666666667,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_008",
      "json_validity": {
        "json_valid_count": 3,
        "json_valid_total": 3,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 3,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 0.3333333333333333,
          "method": 0.0,
          "key_result": 0.0,
          "model_or_system": 0.3333333333333333,
          "benchmark": 1.0
        },
        "overall_field_emr": 0.3333333333333333
      }
    },
    "gpt4_extraction_C3_t0.7_abs_009": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 399.3333333333333,
        "std": 31.562988170042175,
        "min": 355,
        "max": 426,
        "normalized_mean": 0.3418036893040329
      },
      "rouge_l": {
        "mean": 0.6520557289383483,
        "std": 0.033882786457535244,
        "min": 0.6064981949458484,
        "max": 0.6876971608832808
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9556754231452942,
        "bertscore_f1_std": 0.011556831442690447,
        "bertscore_f1_min": 0.9473718404769897,
        "bertscore_f1_max": 0.9720185399055481,
        "bertscore_precision_mean": 0.9633675018946329,
        "bertscore_recall_mean": 0.9482239087422689
      },
      "avg_output_length_chars": 1084.3333333333333,
      "avg_output_length_words": 146.33333333333334,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_009",
      "json_validity": {
        "json_valid_count": 3,
        "json_valid_total": 3,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 3,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 0.0,
          "method": 0.0,
          "key_result": 0.0,
          "model_or_system": 0.3333333333333333,
          "benchmark": 0.3333333333333333
        },
        "overall_field_emr": 0.13333333333333333
      }
    },
    "gpt4_extraction_C3_t0.7_abs_010": {
      "n_outputs": 2,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 234.0,
        "std": 0.0,
        "min": 234,
        "max": 234,
        "normalized_mean": 0.208
      },
      "rouge_l": {
        "mean": 0.8167202572347267,
        "std": 0.0,
        "min": 0.8167202572347267,
        "max": 0.8167202572347267
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9830679893493652,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9830679893493652,
        "bertscore_f1_max": 0.9830679893493652,
        "bertscore_precision_mean": 0.9826904535293579,
        "bertscore_recall_mean": 0.9834458827972412
      },
      "avg_output_length_chars": 1092.5,
      "avg_output_length_words": 155.5,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_010",
      "json_validity": {
        "json_valid_count": 2,
        "json_valid_total": 2,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 2,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 0.0,
          "method": 0.0,
          "key_result": 1.0,
          "model_or_system": 1.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 0.6
      }
    },
    "gpt4_extraction_C3_t0.7_abs_011": {
      "n_outputs": 2,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 330.0,
        "std": 0.0,
        "min": 330,
        "max": 330,
        "normalized_mean": 0.358695652173913
      },
      "rouge_l": {
        "mean": 0.6798418972332015,
        "std": 0.0,
        "min": 0.6798418972332015,
        "max": 0.6798418972332015
      },
      "bert_score": {
        "bertscore_f1_mean": 0.955298125743866,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.955298125743866,
        "bertscore_f1_max": 0.955298125743866,
        "bertscore_precision_mean": 0.9487400054931641,
        "bertscore_recall_mean": 0.9619476199150085
      },
      "avg_output_length_chars": 878.0,
      "avg_output_length_words": 126.5,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_011",
      "json_validity": {
        "json_valid_count": 2,
        "json_valid_total": 2,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 2,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 0.0,
          "method": 0.0,
          "key_result": 0.0,
          "model_or_system": 0.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 0.2
      }
    },
    "gpt4_extraction_C3_t0.7_abs_012": {
      "n_outputs": 2,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 163.0,
        "std": 0.0,
        "min": 163,
        "max": 163,
        "normalized_mean": 0.18735632183908046
      },
      "rouge_l": {
        "mean": 0.8290155440414508,
        "std": 0.0,
        "min": 0.8290155440414508,
        "max": 0.8290155440414508
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9712501168251038,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9712501168251038,
        "bertscore_f1_max": 0.9712501168251038,
        "bertscore_precision_mean": 0.9769957661628723,
        "bertscore_recall_mean": 0.9655715823173523
      },
      "avg_output_length_chars": 791.5,
      "avg_output_length_words": 96.5,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_012",
      "json_validity": {
        "json_valid_count": 2,
        "json_valid_total": 2,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 2,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 0.0,
          "method": 0.0,
          "key_result": 0.0,
          "model_or_system": 1.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 0.4
      }
    },
    "gpt4_extraction_C3_t0.7_abs_018": {
      "n_outputs": 2,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 330.0,
        "std": 0.0,
        "min": 330,
        "max": 330,
        "normalized_mean": 0.35947712418300654
      },
      "rouge_l": {
        "mean": 0.6804979253112033,
        "std": 0.0,
        "min": 0.6804979253112033,
        "max": 0.6804979253112033
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9573975801467896,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9573975801467896,
        "bertscore_f1_max": 0.9573975801467896,
        "bertscore_precision_mean": 0.9578647613525391,
        "bertscore_recall_mean": 0.9569308757781982
      },
      "avg_output_length_chars": 895.0,
      "avg_output_length_words": 120.5,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_018",
      "json_validity": {
        "json_valid_count": 2,
        "json_valid_total": 2,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 2,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 0.0,
          "method": 0.0,
          "key_result": 0.0,
          "model_or_system": 1.0,
          "benchmark": 0.0
        },
        "overall_field_emr": 0.2
      }
    },
    "gpt4_extraction_C3_t0.7_abs_019": {
      "n_outputs": 2,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 29.0,
        "std": 0.0,
        "min": 29,
        "max": 29,
        "normalized_mean": 0.04865771812080537
      },
      "rouge_l": {
        "mean": 0.9523809523809524,
        "std": 0.0,
        "min": 0.9523809523809524,
        "max": 0.9523809523809524
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9916019439697266,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9916019439697266,
        "bertscore_f1_max": 0.9916019439697266,
        "bertscore_precision_mean": 0.9954612255096436,
        "bertscore_recall_mean": 0.9877724647521973
      },
      "avg_output_length_chars": 581.5,
      "avg_output_length_words": 73.5,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_019",
      "json_validity": {
        "json_valid_count": 2,
        "json_valid_total": 2,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 2,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 0.0,
          "method": 1.0,
          "key_result": 1.0,
          "model_or_system": 1.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 0.8
      }
    },
    "gpt4_extraction_C3_t0.7_abs_024": {
      "n_outputs": 2,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 151.0,
        "std": 0.0,
        "min": 151,
        "max": 151,
        "normalized_mean": 0.25166666666666665
      },
      "rouge_l": {
        "mean": 0.8059701492537314,
        "std": 0.0,
        "min": 0.8059701492537314,
        "max": 0.8059701492537314
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9741552472114563,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9741552472114563,
        "bertscore_f1_max": 0.9741552472114563,
        "bertscore_precision_mean": 0.984040379524231,
        "bertscore_recall_mean": 0.9644668102264404
      },
      "avg_output_length_chars": 525.0,
      "avg_output_length_words": 67.0,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_024",
      "json_validity": {
        "json_valid_count": 2,
        "json_valid_total": 2,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 2,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 0.0,
          "method": 0.0,
          "key_result": 1.0,
          "model_or_system": 1.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 0.6
      }
    },
    "gpt4_extraction_C3_t0.7_abs_025": {
      "n_outputs": 2,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 91.0,
        "std": 0.0,
        "min": 91,
        "max": 91,
        "normalized_mean": 0.14677419354838708
      },
      "rouge_l": {
        "mean": 0.7857142857142857,
        "std": 0.0,
        "min": 0.7857142857142857,
        "max": 0.7857142857142857
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9760143756866455,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9760143756866455,
        "bertscore_f1_max": 0.9760143756866455,
        "bertscore_precision_mean": 0.9735279083251953,
        "bertscore_recall_mean": 0.9785135388374329
      },
      "avg_output_length_chars": 605.5,
      "avg_output_length_words": 84.0,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_025",
      "json_validity": {
        "json_valid_count": 2,
        "json_valid_total": 2,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 2,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 1.0,
          "method": 1.0,
          "key_result": 0.0,
          "model_or_system": 1.0,
          "benchmark": 0.0
        },
        "overall_field_emr": 0.6
      }
    },
    "gpt4_extraction_C3_t0.7_abs_028": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 348.0,
        "std": 142.33996861973333,
        "min": 147,
        "max": 458,
        "normalized_mean": 0.3042978895694031
      },
      "rouge_l": {
        "mean": 0.7196251519580604,
        "std": 0.08189909362508646,
        "min": 0.6466666666666666,
        "max": 0.8340080971659919
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9623619914054871,
        "bertscore_f1_std": 0.01132518842005816,
        "bertscore_f1_min": 0.9540990591049194,
        "bertscore_f1_max": 0.9783754944801331,
        "bertscore_precision_mean": 0.9636451403299967,
        "bertscore_recall_mean": 0.9612442255020142
      },
      "avg_output_length_chars": 977.3333333333334,
      "avg_output_length_words": 139.33333333333334,
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_028",
      "json_validity": {
        "json_valid_count": 3,
        "json_valid_total": 3,
        "json_validity_rate": 1.0
      },
      "schema_compliance": {
        "schema_compliant_count": 3,
        "schema_compliant_of_valid": 1.0,
        "schema_compliance_rate": 1.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": 0.0,
          "method": 0.0,
          "key_result": 0.0,
          "model_or_system": 0.0,
          "benchmark": 1.0
        },
        "overall_field_emr": 0.2
      }
    },
    "gpt4_summarization_C1_abs_001": {
      "n_outputs": 3,
      "exact_match_rate": 0.3333333333333333,
      "edit_distance": {
        "mean": 30.666666666666668,
        "std": 21.684607956387456,
        "min": 0,
        "max": 46,
        "normalized_mean": 0.04821802935010482
      },
      "rouge_l": {
        "mean": 0.9672131147540983,
        "std": 0.023183828891362238,
        "min": 0.9508196721311475,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9944849809010824,
        "bertscore_f1_std": 0.003899707403217988,
        "bertscore_f1_min": 0.9917274713516235,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9944877227147421,
        "bertscore_recall_mean": 0.9944877227147421
      },
      "avg_output_length_chars": 630.3333333333334,
      "avg_output_length_words": 91.66666666666667,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C1",
      "abstract": "abs_001"
    },
    "gpt4_summarization_C1_abs_003": {
      "n_outputs": 2,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 43.0,
        "std": 0.0,
        "min": 43,
        "max": 43,
        "normalized_mean": 0.06836248012718601
      },
      "rouge_l": {
        "mean": 0.9068322981366459,
        "std": 0.0,
        "min": 0.9068322981366459,
        "max": 0.9068322981366459
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9903980493545532,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9903980493545532,
        "bertscore_f1_max": 0.9903980493545532,
        "bertscore_precision_mean": 0.9915088415145874,
        "bertscore_recall_mean": 0.9892898201942444
      },
      "avg_output_length_chars": 628.5,
      "avg_output_length_words": 80.5,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C1",
      "abstract": "abs_003"
    },
    "gpt4_summarization_C1_abs_004": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 87.0,
        "std": 31.506613062445584,
        "min": 47,
        "max": 124,
        "normalized_mean": 0.14718397564519478
      },
      "rouge_l": {
        "mean": 0.8148685695855505,
        "std": 0.0685625611667881,
        "min": 0.7295597484276728,
        "max": 0.8974358974358975
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9842525720596313,
        "bertscore_f1_std": 0.0056279266724953555,
        "bertscore_f1_min": 0.9779000878334045,
        "bertscore_f1_max": 0.9915814995765686,
        "bertscore_precision_mean": 0.9851713379224142,
        "bertscore_recall_mean": 0.9833356936772665
      },
      "avg_output_length_chars": 583.3333333333334,
      "avg_output_length_words": 79.0,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C1",
      "abstract": "abs_004"
    },
    "gpt4_summarization_C2_abs_001": {
      "n_outputs": 5,
      "exact_match_rate": 0.4,
      "edit_distance": {
        "mean": 10.8,
        "std": 8.818163074019441,
        "min": 0,
        "max": 18,
        "normalized_mean": 0.017419354838709676
      },
      "rouge_l": {
        "mean": 0.9868131868131869,
        "std": 0.010766987880365635,
        "min": 0.978021978021978,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9991673350334167,
        "bertscore_f1_std": 0.000679941099540279,
        "bertscore_f1_min": 0.9986121654510498,
        "bertscore_f1_max": 1.0000001192092896,
        "bertscore_precision_mean": 0.999167287349701,
        "bertscore_recall_mean": 0.9991673827171326
      },
      "avg_output_length_chars": 619.6,
      "avg_output_length_words": 91.0,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_001"
    },
    "gpt4_summarization_C2_abs_002": {
      "n_outputs": 5,
      "exact_match_rate": 0.1,
      "edit_distance": {
        "mean": 112.6,
        "std": 81.43610992673949,
        "min": 0,
        "max": 228,
        "normalized_mean": 0.1492734885447436
      },
      "rouge_l": {
        "mean": 0.8901324919688921,
        "std": 0.0803838758552236,
        "min": 0.781725888324873,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9859697341918945,
        "bertscore_f1_std": 0.009726223484001697,
        "bertscore_f1_min": 0.9718327522277832,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.98850177526474,
        "bertscore_recall_mean": 0.9835189402103424
      },
      "avg_output_length_chars": 695.6,
      "avg_output_length_words": 94.4,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_002"
    },
    "gpt4_summarization_C2_abs_003": {
      "n_outputs": 5,
      "exact_match_rate": 0.3,
      "edit_distance": {
        "mean": 25.8,
        "std": 19.046259475288053,
        "min": 0,
        "max": 51,
        "normalized_mean": 0.040830429034971756
      },
      "rouge_l": {
        "mean": 0.94082868091189,
        "std": 0.040911064501317526,
        "min": 0.8902439024390244,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9939688622951508,
        "bertscore_f1_std": 0.004396032421977915,
        "bertscore_f1_min": 0.9875215291976929,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9938091099262237,
        "bertscore_recall_mean": 0.994130676984787
      },
      "avg_output_length_chars": 629.0,
      "avg_output_length_words": 80.8,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_003"
    },
    "gpt4_summarization_C2_abs_004": {
      "n_outputs": 5,
      "exact_match_rate": 0.1,
      "edit_distance": {
        "mean": 55.7,
        "std": 40.87554280985147,
        "min": 0,
        "max": 124,
        "normalized_mean": 0.09428727459793208
      },
      "rouge_l": {
        "mean": 0.8863871414308562,
        "std": 0.08717547274602921,
        "min": 0.7295597484276728,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9898293793201447,
        "bertscore_f1_std": 0.007416725563077238,
        "bertscore_f1_min": 0.9779000878334045,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9901823401451111,
        "bertscore_recall_mean": 0.9894776582717896
      },
      "avg_output_length_chars": 585.0,
      "avg_output_length_words": 78.8,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_004"
    },
    "gpt4_summarization_C2_abs_005": {
      "n_outputs": 5,
      "exact_match_rate": 0.1,
      "edit_distance": {
        "mean": 28.4,
        "std": 15.749285698088025,
        "min": 0,
        "max": 53,
        "normalized_mean": 0.05702633405170232
      },
      "rouge_l": {
        "mean": 0.9432884041168716,
        "std": 0.0294294787176399,
        "min": 0.9022556390977443,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9932526171207428,
        "bertscore_f1_std": 0.0038325918204497227,
        "bertscore_f1_min": 0.9890812635421753,
        "bertscore_f1_max": 0.9999999403953552,
        "bertscore_precision_mean": 0.9935385107994079,
        "bertscore_recall_mean": 0.9929711103439331
      },
      "avg_output_length_chars": 487.4,
      "avg_output_length_words": 67.0,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_005"
    },
    "gpt4_summarization_C2_abs_006": {
      "n_outputs": 5,
      "exact_match_rate": 0.1,
      "edit_distance": {
        "mean": 62.2,
        "std": 44.959537364167794,
        "min": 0,
        "max": 121,
        "normalized_mean": 0.10457718032612398
      },
      "rouge_l": {
        "mean": 0.8764498432601882,
        "std": 0.08777591667189448,
        "min": 0.7586206896551724,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9804846048355103,
        "bertscore_f1_std": 0.013999821862924044,
        "bertscore_f1_min": 0.9651782512664795,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9807804584503174,
        "bertscore_recall_mean": 0.980189859867096
      },
      "avg_output_length_chars": 580.6,
      "avg_output_length_words": 87.4,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_006"
    },
    "gpt4_summarization_C2_abs_007": {
      "n_outputs": 5,
      "exact_match_rate": 0.1,
      "edit_distance": {
        "mean": 156.4,
        "std": 117.87722426321379,
        "min": 0,
        "max": 295,
        "normalized_mean": 0.2508965790556127
      },
      "rouge_l": {
        "mean": 0.7682944816537608,
        "std": 0.17446174471121087,
        "min": 0.5222222222222223,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9714259326457977,
        "bertscore_f1_std": 0.021094032716558752,
        "bertscore_f1_min": 0.9451671838760376,
        "bertscore_f1_max": 0.9999999403953552,
        "bertscore_precision_mean": 0.9688938856124878,
        "bertscore_recall_mean": 0.9739910304546356
      },
      "avg_output_length_chars": 607.4,
      "avg_output_length_words": 92.8,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_007"
    },
    "gpt4_summarization_C2_abs_008": {
      "n_outputs": 5,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 180.2,
        "std": 103.14533435885504,
        "min": 12,
        "max": 276,
        "normalized_mean": 0.27903343332545905
      },
      "rouge_l": {
        "mean": 0.7206185036472699,
        "std": 0.17411492215127497,
        "min": 0.5116279069767442,
        "max": 0.9822485207100591
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9645780086517334,
        "bertscore_f1_std": 0.02185618571112571,
        "bertscore_f1_min": 0.9359158277511597,
        "bertscore_f1_max": 0.9972244501113892,
        "bertscore_precision_mean": 0.9648396134376526,
        "bertscore_recall_mean": 0.9643371641635895
      },
      "avg_output_length_chars": 631.2,
      "avg_output_length_words": 83.8,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_008"
    },
    "gpt4_summarization_C2_abs_009": {
      "n_outputs": 5,
      "exact_match_rate": 0.1,
      "edit_distance": {
        "mean": 35.0,
        "std": 30.880414504990053,
        "min": 0,
        "max": 87,
        "normalized_mean": 0.04995397689714738
      },
      "rouge_l": {
        "mean": 0.9544147387873074,
        "std": 0.031403778635349405,
        "min": 0.892156862745098,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9928788542747498,
        "bertscore_f1_std": 0.006251364843202869,
        "bertscore_f1_min": 0.9836415648460388,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9905867338180542,
        "bertscore_recall_mean": 0.9952426970005035
      },
      "avg_output_length_chars": 659.0,
      "avg_output_length_words": 99.6,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_009"
    },
    "gpt4_summarization_C2_abs_010": {
      "n_outputs": 5,
      "exact_match_rate": 0.3,
      "edit_distance": {
        "mean": 157.8,
        "std": 107.40558644688832,
        "min": 0,
        "max": 258,
        "normalized_mean": 0.23943296120260862
      },
      "rouge_l": {
        "mean": 0.7444806681325045,
        "std": 0.17370141989391188,
        "min": 0.5888888888888889,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9592012524604797,
        "bertscore_f1_std": 0.028366227880746036,
        "bertscore_f1_min": 0.9296872615814209,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9613599419593811,
        "bertscore_recall_mean": 0.9570774137973785
      },
      "avg_output_length_chars": 637.8,
      "avg_output_length_words": 94.4,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_010"
    },
    "gpt4_summarization_C2_abs_011": {
      "n_outputs": 5,
      "exact_match_rate": 0.1,
      "edit_distance": {
        "mean": 103.0,
        "std": 110.57033960334932,
        "min": 0,
        "max": 246,
        "normalized_mean": 0.16211035349507832
      },
      "rouge_l": {
        "mean": 0.8492305813385441,
        "std": 0.1387922479469386,
        "min": 0.6739130434782609,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9866758048534393,
        "bertscore_f1_std": 0.012230930594318533,
        "bertscore_f1_min": 0.9710326790809631,
        "bertscore_f1_max": 0.9999998807907104,
        "bertscore_precision_mean": 0.9857334852218628,
        "bertscore_recall_mean": 0.9876351118087768
      },
      "avg_output_length_chars": 598.2,
      "avg_output_length_words": 90.2,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_011"
    },
    "gpt4_summarization_C2_abs_012": {
      "n_outputs": 5,
      "exact_match_rate": 0.2,
      "edit_distance": {
        "mean": 159.6,
        "std": 128.6943666210763,
        "min": 0,
        "max": 266,
        "normalized_mean": 0.2140322618423447
      },
      "rouge_l": {
        "mean": 0.7628860641705488,
        "std": 0.19154088607418665,
        "min": 0.6054054054054054,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9722208380699158,
        "bertscore_f1_std": 0.022235201431615136,
        "bertscore_f1_min": 0.9540120363235474,
        "bertscore_f1_max": 0.9999999403953552,
        "bertscore_precision_mean": 0.9708908557891845,
        "bertscore_recall_mean": 0.9736202120780945
      },
      "avg_output_length_chars": 708.6,
      "avg_output_length_words": 93.6,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_012"
    },
    "gpt4_summarization_C2_abs_013": {
      "n_outputs": 5,
      "exact_match_rate": 0.6,
      "edit_distance": {
        "mean": 29.6,
        "std": 36.252448193191036,
        "min": 0,
        "max": 74,
        "normalized_mean": 0.06836027713625867
      },
      "rouge_l": {
        "mean": 0.9290322580645161,
        "std": 0.0869173779697257,
        "min": 0.8225806451612903,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9933970689773559,
        "bertscore_f1_std": 0.008086905906135712,
        "bertscore_f1_min": 0.9834926724433899,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9937291502952575,
        "bertscore_recall_mean": 0.9930672287940979
      },
      "avg_output_length_chars": 410.6,
      "avg_output_length_words": 60.8,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_013"
    },
    "gpt4_summarization_C2_abs_014": {
      "n_outputs": 5,
      "exact_match_rate": 0.6,
      "edit_distance": {
        "mean": 107.6,
        "std": 131.78254816173498,
        "min": 0,
        "max": 269,
        "normalized_mean": 0.17382875605815834
      },
      "rouge_l": {
        "mean": 0.807909604519774,
        "std": 0.23526172670798892,
        "min": 0.5197740112994349,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.978633713722229,
        "bertscore_f1_std": 0.02616824953938451,
        "bertscore_f1_min": 0.9465842843055725,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9785504460334777,
        "bertscore_recall_mean": 0.9787171244621277
      },
      "avg_output_length_chars": 603.8,
      "avg_output_length_words": 87.0,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_014"
    },
    "gpt4_summarization_C2_abs_015": {
      "n_outputs": 5,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 287.5,
        "std": 172.28479329296593,
        "min": 9,
        "max": 431,
        "normalized_mean": 0.4346596706163197
      },
      "rouge_l": {
        "mean": 0.6320528649411276,
        "std": 0.20783289682186426,
        "min": 0.4559585492227979,
        "max": 0.9853658536585367
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9566252052783966,
        "bertscore_f1_std": 0.022708323285227948,
        "bertscore_f1_min": 0.9282573461532593,
        "bertscore_f1_max": 0.9986655712127686,
        "bertscore_precision_mean": 0.9584206461906433,
        "bertscore_recall_mean": 0.9548847675323486
      },
      "avg_output_length_chars": 621.2,
      "avg_output_length_words": 92.8,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_015"
    },
    "gpt4_summarization_C2_abs_016": {
      "n_outputs": 5,
      "exact_match_rate": 0.3,
      "edit_distance": {
        "mean": 172.8,
        "std": 116.54424052693467,
        "min": 0,
        "max": 291,
        "normalized_mean": 0.2310741935483871
      },
      "rouge_l": {
        "mean": 0.78237610061977,
        "std": 0.14284474429302518,
        "min": 0.6634146341463415,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9731513857841492,
        "bertscore_f1_std": 0.01782903467627549,
        "bertscore_f1_min": 0.9549745917320251,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9750918388366699,
        "bertscore_recall_mean": 0.9712268352508545
      },
      "avg_output_length_chars": 723.6,
      "avg_output_length_words": 99.8,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_016"
    },
    "gpt4_summarization_C2_abs_017": {
      "n_outputs": 5,
      "exact_match_rate": 0.3,
      "edit_distance": {
        "mean": 20.8,
        "std": 24.268498099388022,
        "min": 0,
        "max": 52,
        "normalized_mean": 0.04338152328914453
      },
      "rouge_l": {
        "mean": 0.9656246028720294,
        "std": 0.03295943124762863,
        "min": 0.9147286821705426,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9956763684749603,
        "bertscore_f1_std": 0.0043011973722144985,
        "bertscore_f1_min": 0.9893022775650024,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9956848382949829,
        "bertscore_recall_mean": 0.995690792798996
      },
      "avg_output_length_chars": 441.4,
      "avg_output_length_words": 62.4,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_017"
    },
    "gpt4_summarization_C2_abs_018": {
      "n_outputs": 5,
      "exact_match_rate": 0.3,
      "edit_distance": {
        "mean": 23.8,
        "std": 26.19465594353169,
        "min": 0,
        "max": 57,
        "normalized_mean": 0.03160409607038821
      },
      "rouge_l": {
        "mean": 0.9831241655540721,
        "std": 0.01550087646628754,
        "min": 0.9626168224299064,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9959127962589264,
        "bertscore_f1_std": 0.004590529633406218,
        "bertscore_f1_min": 0.9901704788208008,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9951072454452514,
        "bertscore_recall_mean": 0.9967319130897522
      },
      "avg_output_length_chars": 720.8,
      "avg_output_length_words": 105.8,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_018"
    },
    "gpt4_summarization_C2_abs_019": {
      "n_outputs": 5,
      "exact_match_rate": 0.6,
      "edit_distance": {
        "mean": 130.8,
        "std": 160.19662917801986,
        "min": 0,
        "max": 327,
        "normalized_mean": 0.21302931596091207
      },
      "rouge_l": {
        "mean": 0.82,
        "std": 0.220454076850486,
        "min": 0.55,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9847640991210938,
        "bertscore_f1_std": 0.018660091462471034,
        "bertscore_f1_min": 0.9619102478027344,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9847709178924561,
        "bertscore_recall_mean": 0.9847709178924561
      },
      "avg_output_length_chars": 574.8,
      "avg_output_length_words": 78.8,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_019"
    },
    "gpt4_summarization_C2_abs_020": {
      "n_outputs": 5,
      "exact_match_rate": 0.4,
      "edit_distance": {
        "mean": 42.6,
        "std": 34.782754347521134,
        "min": 0,
        "max": 71,
        "normalized_mean": 0.08223938223938225
      },
      "rouge_l": {
        "mean": 0.9447368421052632,
        "std": 0.04512217947232168,
        "min": 0.9078947368421053,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9938936471939087,
        "bertscore_f1_std": 0.00498581618811198,
        "bertscore_f1_min": 0.9898227453231812,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9942310452461243,
        "bertscore_recall_mean": 0.9935595750808716
      },
      "avg_output_length_chars": 497.6,
      "avg_output_length_words": 75.6,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_020"
    },
    "gpt4_summarization_C2_abs_021": {
      "n_outputs": 5,
      "exact_match_rate": 0.6,
      "edit_distance": {
        "mean": 4.8,
        "std": 5.878775382679628,
        "min": 0,
        "max": 12,
        "normalized_mean": 0.008261617900172118
      },
      "rouge_l": {
        "mean": 0.9902439024390244,
        "std": 0.01194873045260088,
        "min": 0.975609756097561,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9978891134262085,
        "bertscore_f1_std": 0.0025852975053405076,
        "bertscore_f1_min": 0.9947227835655212,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9979396820068359,
        "bertscore_recall_mean": 0.9978385448455811
      },
      "avg_output_length_chars": 571.4,
      "avg_output_length_words": 81.4,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_021"
    },
    "gpt4_summarization_C2_abs_022": {
      "n_outputs": 5,
      "exact_match_rate": 0.1,
      "edit_distance": {
        "mean": 30.8,
        "std": 18.258148865643527,
        "min": 0,
        "max": 53,
        "normalized_mean": 0.042446403526347425
      },
      "rouge_l": {
        "mean": 0.9459836769759449,
        "std": 0.03506796664104855,
        "min": 0.9062499999999999,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9951793193817139,
        "bertscore_f1_std": 0.0029641059723206045,
        "bertscore_f1_min": 0.9914920330047607,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.995275491476059,
        "bertscore_recall_mean": 0.995083749294281
      },
      "avg_output_length_chars": 716.2,
      "avg_output_length_words": 96.2,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_022"
    },
    "gpt4_summarization_C2_abs_023": {
      "n_outputs": 5,
      "exact_match_rate": 0.1,
      "edit_distance": {
        "mean": 30.2,
        "std": 29.1300532097008,
        "min": 0,
        "max": 70,
        "normalized_mean": 0.04843863888339915
      },
      "rouge_l": {
        "mean": 0.9565124394911628,
        "std": 0.042876251436762866,
        "min": 0.8994708994708994,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9943842411041259,
        "bertscore_f1_std": 0.004980279315984923,
        "bertscore_f1_min": 0.9872118830680847,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9944904506206512,
        "bertscore_recall_mean": 0.9942794561386108
      },
      "avg_output_length_chars": 621.6,
      "avg_output_length_words": 94.4,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_023"
    },
    "gpt4_summarization_C2_abs_024": {
      "n_outputs": 5,
      "exact_match_rate": 0.2,
      "edit_distance": {
        "mean": 15.0,
        "std": 9.979979959899719,
        "min": 0,
        "max": 25,
        "normalized_mean": 0.0240850627563067
      },
      "rouge_l": {
        "mean": 0.974835870427819,
        "std": 0.016669330268964316,
        "min": 0.9580838323353292,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9959732294082642,
        "bertscore_f1_std": 0.0025489016006466254,
        "bertscore_f1_min": 0.9934641122817993,
        "bertscore_f1_max": 1.0000001192092896,
        "bertscore_precision_mean": 0.9960069775581359,
        "bertscore_recall_mean": 0.9959396839141845
      },
      "avg_output_length_chars": 621.0,
      "avg_output_length_words": 83.4,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_024"
    },
    "gpt4_summarization_C2_abs_025": {
      "n_outputs": 5,
      "exact_match_rate": 0.4,
      "edit_distance": {
        "mean": 46.2,
        "std": 37.722142038860945,
        "min": 0,
        "max": 77,
        "normalized_mean": 0.07764705882352943
      },
      "rouge_l": {
        "mean": 0.9071428571428571,
        "std": 0.07581753965757451,
        "min": 0.8452380952380953,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9907597064971924,
        "bertscore_f1_std": 0.007544643718387401,
        "bertscore_f1_min": 0.9845995306968689,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9914528250694274,
        "bertscore_recall_mean": 0.9900808930397034
      },
      "avg_output_length_chars": 580.0,
      "avg_output_length_words": 83.6,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_025"
    },
    "gpt4_summarization_C2_abs_026": {
      "n_outputs": 5,
      "exact_match_rate": 0.1,
      "edit_distance": {
        "mean": 70.2,
        "std": 36.19336955852549,
        "min": 0,
        "max": 99,
        "normalized_mean": 0.12138548321721628
      },
      "rouge_l": {
        "mean": 0.8865833805228048,
        "std": 0.05393582789659841,
        "min": 0.8322981366459627,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9841129183769226,
        "bertscore_f1_std": 0.008406167133894821,
        "bertscore_f1_min": 0.9766008257865906,
        "bertscore_f1_max": 0.9999998807907104,
        "bertscore_precision_mean": 0.985355681180954,
        "bertscore_recall_mean": 0.982942670583725
      },
      "avg_output_length_chars": 560.0,
      "avg_output_length_words": 80.2,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_026"
    },
    "gpt4_summarization_C2_abs_027": {
      "n_outputs": 5,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 35.8,
        "std": 16.454786537661317,
        "min": 10,
        "max": 64,
        "normalized_mean": 0.05218743198303157
      },
      "rouge_l": {
        "mean": 0.9360808946208818,
        "std": 0.031400748504425424,
        "min": 0.8901098901098902,
        "max": 0.9888888888888889
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9932276785373688,
        "bertscore_f1_std": 0.003932230446397766,
        "bertscore_f1_min": 0.9886707663536072,
        "bertscore_f1_max": 0.9996110200881958,
        "bertscore_precision_mean": 0.9937553644180298,
        "bertscore_recall_mean": 0.9927022397518158
      },
      "avg_output_length_chars": 679.4,
      "avg_output_length_words": 90.6,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_027"
    },
    "gpt4_summarization_C2_abs_028": {
      "n_outputs": 5,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 275.5,
        "std": 141.72596798046573,
        "min": 82,
        "max": 484,
        "normalized_mean": 0.3801670998644199
      },
      "rouge_l": {
        "mean": 0.6551246870065028,
        "std": 0.17278294902613137,
        "min": 0.411214953271028,
        "max": 0.91005291005291
      },
      "bert_score": {
        "bertscore_f1_mean": 0.947914332151413,
        "bertscore_f1_std": 0.026792564408275116,
        "bertscore_f1_min": 0.9106541275978088,
        "bertscore_f1_max": 0.9844050407409668,
        "bertscore_precision_mean": 0.9494326174259186,
        "bertscore_recall_mean": 0.9465195894241333
      },
      "avg_output_length_chars": 649.2,
      "avg_output_length_words": 98.2,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_028"
    },
    "gpt4_summarization_C2_abs_029": {
      "n_outputs": 5,
      "exact_match_rate": 0.4,
      "edit_distance": {
        "mean": 144.6,
        "std": 118.0654056021492,
        "min": 0,
        "max": 241,
        "normalized_mean": 0.20746054519368723
      },
      "rouge_l": {
        "mean": 0.8125,
        "std": 0.15309310892394862,
        "min": 0.6875,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9755168855190277,
        "bertscore_f1_std": 0.019990342763941137,
        "bertscore_f1_min": 0.9591948390007019,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9748181760311126,
        "bertscore_recall_mean": 0.9762312233448028
      },
      "avg_output_length_chars": 670.6,
      "avg_output_length_words": 97.0,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_029"
    },
    "gpt4_summarization_C2_abs_030": {
      "n_outputs": 5,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 97.1,
        "std": 44.33384711481737,
        "min": 26,
        "max": 164,
        "normalized_mean": 0.19616117978083442
      },
      "rouge_l": {
        "mean": 0.8318972725610511,
        "std": 0.07313062260438372,
        "min": 0.7285714285714284,
        "max": 0.9517241379310345
      },
      "bert_score": {
        "bertscore_f1_mean": 0.981622976064682,
        "bertscore_f1_std": 0.005613550087524012,
        "bertscore_f1_min": 0.975471556186676,
        "bertscore_f1_max": 0.9946554899215698,
        "bertscore_precision_mean": 0.9813626110553741,
        "bertscore_recall_mean": 0.981887936592102
      },
      "avg_output_length_chars": 485.4,
      "avg_output_length_words": 70.8,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_030"
    },
    "gpt4_summarization_C3_t0.0_abs_001": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 59.333333333333336,
        "std": 10.274023338281626,
        "min": 46,
        "max": 71,
        "normalized_mean": 0.09416943538472479
      },
      "rouge_l": {
        "mean": 0.9078954871908408,
        "std": 0.030418127174458597,
        "min": 0.8839779005524863,
        "max": 0.9508196721311475
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9873077074686686,
        "bertscore_f1_std": 0.0037689138806241977,
        "bertscore_f1_min": 0.9825178384780884,
        "bertscore_f1_max": 0.9917274713516235,
        "bertscore_precision_mean": 0.9869740009307861,
        "bertscore_recall_mean": 0.9876486659049988
      },
      "avg_output_length_chars": 621.6666666666666,
      "avg_output_length_words": 90.66666666666667,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_001"
    },
    "gpt4_summarization_C3_t0.0_abs_002": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 71.33333333333333,
        "std": 33.229839334884275,
        "min": 27,
        "max": 107,
        "normalized_mean": 0.10775755203236119
      },
      "rouge_l": {
        "mean": 0.92891375124028,
        "std": 0.029910247250587663,
        "min": 0.8926553672316384,
        "max": 0.9659090909090908
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9914100567499796,
        "bertscore_f1_std": 0.0027028226039103407,
        "bertscore_f1_min": 0.9885302782058716,
        "bertscore_f1_max": 0.9950266480445862,
        "bertscore_precision_mean": 0.9929739832878113,
        "bertscore_recall_mean": 0.9898521502812704
      },
      "avg_output_length_chars": 650.0,
      "avg_output_length_words": 89.0,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_002"
    },
    "gpt4_summarization_C3_t0.0_abs_003": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 8.0,
        "std": 2.943920288775949,
        "min": 5,
        "max": 12,
        "normalized_mean": 0.012738885803901284
      },
      "rouge_l": {
        "mean": 0.9750000000000001,
        "std": 0.01020620726159654,
        "min": 0.9625000000000001,
        "max": 0.9875
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9989145994186401,
        "bertscore_f1_std": 0.0003793686914163327,
        "bertscore_f1_min": 0.9983924031257629,
        "bertscore_f1_max": 0.9992823004722595,
        "bertscore_precision_mean": 0.9989145994186401,
        "bertscore_recall_mean": 0.9989145994186401
      },
      "avg_output_length_chars": 627.0,
      "avg_output_length_words": 80.0,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_003"
    },
    "gpt4_summarization_C3_t0.0_abs_004": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 60.0,
        "std": 24.49489742783178,
        "min": 30,
        "max": 90,
        "normalized_mean": 0.10339317040347969
      },
      "rouge_l": {
        "mean": 0.8772460928304283,
        "std": 0.04862203497797235,
        "min": 0.8176100628930817,
        "max": 0.9367088607594937
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9885006348292033,
        "bertscore_f1_std": 0.004561315468102753,
        "bertscore_f1_min": 0.9832761287689209,
        "bertscore_f1_max": 0.9943895936012268,
        "bertscore_precision_mean": 0.9871039986610413,
        "bertscore_recall_mean": 0.9899018406867981
      },
      "avg_output_length_chars": 572.6666666666666,
      "avg_output_length_words": 78.66666666666667,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_004"
    },
    "gpt4_summarization_C3_t0.0_abs_005": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 35.333333333333336,
        "std": 15.15109090315135,
        "min": 16,
        "max": 53,
        "normalized_mean": 0.07116992560405556
      },
      "rouge_l": {
        "mean": 0.9349830805971157,
        "std": 0.023288701876036207,
        "min": 0.9022556390977443,
        "max": 0.9545454545454547
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9923827250798544,
        "bertscore_f1_std": 0.0023536809753128287,
        "bertscore_f1_min": 0.9890812635421753,
        "bertscore_f1_max": 0.9944008588790894,
        "bertscore_precision_mean": 0.9914571444193522,
        "bertscore_recall_mean": 0.9933105309804281
      },
      "avg_output_length_chars": 486.0,
      "avg_output_length_words": 66.66666666666667,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_005"
    },
    "gpt4_summarization_C3_t0.0_abs_006": {
      "n_outputs": 3,
      "exact_match_rate": 0.3333333333333333,
      "edit_distance": {
        "mean": 78.0,
        "std": 55.154328932550705,
        "min": 0,
        "max": 117,
        "normalized_mean": 0.1315345699831366
      },
      "rouge_l": {
        "mean": 0.846743295019157,
        "std": 0.10836885535426019,
        "min": 0.7701149425287356,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9771890640258789,
        "bertscore_f1_std": 0.016129767512513188,
        "bertscore_f1_min": 0.9657835960388184,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9767017364501953,
        "bertscore_recall_mean": 0.9776771465937296
      },
      "avg_output_length_chars": 585.6666666666666,
      "avg_output_length_words": 87.0,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_006"
    },
    "gpt4_summarization_C3_t0.0_abs_007": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 248.0,
        "std": 33.0252428706689,
        "min": 202,
        "max": 278,
        "normalized_mean": 0.4118065433854907
      },
      "rouge_l": {
        "mean": 0.5936110256166197,
        "std": 0.04182122033819567,
        "min": 0.5586592178770948,
        "max": 0.6524064171122995
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9489049911499023,
        "bertscore_f1_std": 0.010797277528021183,
        "bertscore_f1_min": 0.9385712146759033,
        "bertscore_f1_max": 0.9638074636459351,
        "bertscore_precision_mean": 0.9467982252438863,
        "bertscore_recall_mean": 0.9510218699773153
      },
      "avg_output_length_chars": 582.0,
      "avg_output_length_words": 89.66666666666667,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_007"
    },
    "gpt4_summarization_C3_t0.0_abs_008": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 146.66666666666666,
        "std": 52.01495511442412,
        "min": 105,
        "max": 220,
        "normalized_mean": 0.22284057998343712
      },
      "rouge_l": {
        "mean": 0.7970994186346467,
        "std": 0.06876936845301174,
        "min": 0.7011494252873562,
        "max": 0.8588235294117647
      },
      "bert_score": {
        "bertscore_f1_mean": 0.972854733467102,
        "bertscore_f1_std": 0.01012252424478485,
        "bertscore_f1_min": 0.9601431488990784,
        "bertscore_f1_max": 0.9849122762680054,
        "bertscore_precision_mean": 0.9733268022537231,
        "bertscore_recall_mean": 0.9724405606587728
      },
      "avg_output_length_chars": 635.0,
      "avg_output_length_words": 85.0,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_008"
    },
    "gpt4_summarization_C3_t0.0_abs_009": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 95.33333333333333,
        "std": 60.47772776448827,
        "min": 10,
        "max": 143,
        "normalized_mean": 0.1479616729079608
      },
      "rouge_l": {
        "mean": 0.9105588525843226,
        "std": 0.05608515992235365,
        "min": 0.8663101604278074,
        "max": 0.9896907216494846
      },
      "bert_score": {
        "bertscore_f1_mean": 0.989190141359965,
        "bertscore_f1_std": 0.006135103497151583,
        "bertscore_f1_min": 0.9841617345809937,
        "bertscore_f1_max": 0.9978277087211609,
        "bertscore_precision_mean": 0.9885732134183248,
        "bertscore_recall_mean": 0.9898401697476705
      },
      "avg_output_length_chars": 632.0,
      "avg_output_length_words": 94.33333333333333,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_009"
    },
    "gpt4_summarization_C3_t0.0_abs_010": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 272.3333333333333,
        "std": 110.55717475084505,
        "min": 120,
        "max": 379,
        "normalized_mean": 0.3958576663231421
      },
      "rouge_l": {
        "mean": 0.6314687026567639,
        "std": 0.13031703343814094,
        "min": 0.5208333333333334,
        "max": 0.8144329896907218
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9500632882118225,
        "bertscore_f1_std": 0.017341856737105237,
        "bertscore_f1_min": 0.9318055510520935,
        "bertscore_f1_max": 0.9733731746673584,
        "bertscore_precision_mean": 0.9454829692840576,
        "bertscore_recall_mean": 0.9547070066134135
      },
      "avg_output_length_chars": 652.3333333333334,
      "avg_output_length_words": 95.33333333333333,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_010"
    },
    "gpt4_summarization_C3_t0.0_abs_011": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 346.6666666666667,
        "std": 45.84272631023983,
        "min": 303,
        "max": 410,
        "normalized_mean": 0.5810247762062546
      },
      "rouge_l": {
        "mean": 0.4795524916553448,
        "std": 0.048206304054944214,
        "min": 0.41916167664670656,
        "max": 0.5371428571428571
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9354474147160848,
        "bertscore_f1_std": 0.0074235658328918016,
        "bertscore_f1_min": 0.9271041750907898,
        "bertscore_f1_max": 0.9451377987861633,
        "bertscore_precision_mean": 0.9369602998097738,
        "bertscore_recall_mean": 0.9339543779691061
      },
      "avg_output_length_chars": 591.6666666666666,
      "avg_output_length_words": 85.33333333333333,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_011"
    },
    "gpt4_summarization_C3_t0.0_abs_012": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 145.33333333333334,
        "std": 97.16423667630436,
        "min": 8,
        "max": 218,
        "normalized_mean": 0.19428671517400264
      },
      "rouge_l": {
        "mean": 0.8380799414227423,
        "std": 0.0973023392490609,
        "min": 0.7567567567567567,
        "max": 0.9748743718592964
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9797195593516032,
        "bertscore_f1_std": 0.012211472297308365,
        "bertscore_f1_min": 0.9698920845985413,
        "bertscore_f1_max": 0.9969314932823181,
        "bertscore_precision_mean": 0.9795869191487631,
        "bertscore_recall_mean": 0.9799164136250814
      },
      "avg_output_length_chars": 713.6666666666666,
      "avg_output_length_words": 94.66666666666667,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_012"
    },
    "gpt4_summarization_C3_t0.0_abs_013": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 405.0,
      "avg_output_length_words": 60.0,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_013"
    },
    "gpt4_summarization_C3_t0.0_abs_014": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 157.0,
        "std": 105.39765968306254,
        "min": 8,
        "max": 235,
        "normalized_mean": 0.24224619072076695
      },
      "rouge_l": {
        "mean": 0.7174708963594586,
        "std": 0.18746507550470715,
        "min": 0.5777777777777778,
        "max": 0.9824561403508771
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9719228347142538,
        "bertscore_f1_std": 0.017810074565207758,
        "bertscore_f1_min": 0.9585488438606262,
        "bertscore_f1_max": 0.9970936179161072,
        "bertscore_precision_mean": 0.9722863038380941,
        "bertscore_recall_mean": 0.9715609153111776
      },
      "avg_output_length_chars": 615.0,
      "avg_output_length_words": 88.33333333333333,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_014"
    },
    "gpt4_summarization_C3_t0.0_abs_015": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 320.0,
        "std": 121.1638009748236,
        "min": 149,
        "max": 415,
        "normalized_mean": 0.4903432117527862
      },
      "rouge_l": {
        "mean": 0.6239489158450605,
        "std": 0.08786499292526068,
        "min": 0.5164835164835164,
        "max": 0.7317073170731706
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9443341493606567,
        "bertscore_f1_std": 0.019079876070256643,
        "bertscore_f1_min": 0.9279100894927979,
        "bertscore_f1_max": 0.9710866808891296,
        "bertscore_precision_mean": 0.9440565506617228,
        "bertscore_recall_mean": 0.9446764787038168
      },
      "avg_output_length_chars": 594.3333333333334,
      "avg_output_length_words": 88.66666666666667,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_015"
    },
    "gpt4_summarization_C3_t0.0_abs_016": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 150.0,
        "std": 94.97719024411423,
        "min": 16,
        "max": 225,
        "normalized_mean": 0.20191409897292248
      },
      "rouge_l": {
        "mean": 0.7935529541765947,
        "std": 0.13882661359821885,
        "min": 0.6903553299492386,
        "max": 0.9897959183673469
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9749458034833273,
        "bertscore_f1_std": 0.01551995227330089,
        "bertscore_f1_min": 0.9627352356910706,
        "bertscore_f1_max": 0.9968460202217102,
        "bertscore_precision_mean": 0.9731487433115641,
        "bertscore_recall_mean": 0.9767532149950663
      },
      "avg_output_length_chars": 722.0,
      "avg_output_length_words": 98.33333333333333,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_016"
    },
    "gpt4_summarization_C3_t0.0_abs_017": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 40.0,
        "std": 9.092121131323903,
        "min": 30,
        "max": 52,
        "normalized_mean": 0.08396833450829132
      },
      "rouge_l": {
        "mean": 0.9231217578830798,
        "std": 0.02130679935094627,
        "min": 0.9022556390977443,
        "max": 0.9523809523809524
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9888396064440409,
        "bertscore_f1_std": 0.0038822752251699345,
        "bertscore_f1_min": 0.9838703870773315,
        "bertscore_f1_max": 0.9933461546897888,
        "bertscore_precision_mean": 0.9876989324887594,
        "bertscore_recall_mean": 0.9900095661481222
      },
      "avg_output_length_chars": 459.0,
      "avg_output_length_words": 64.66666666666667,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_017"
    },
    "gpt4_summarization_C3_t0.0_abs_018": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 8.666666666666666,
        "std": 3.2998316455372216,
        "min": 5,
        "max": 13,
        "normalized_mean": 0.012172284644194757
      },
      "rouge_l": {
        "mean": 0.9872100122100121,
        "std": 0.004554473674332771,
        "min": 0.9807692307692307,
        "max": 0.9904761904761905
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9977071285247803,
        "bertscore_f1_std": 0.0011597936322813585,
        "bertscore_f1_min": 0.9965885877609253,
        "bertscore_f1_max": 0.9993053078651428,
        "bertscore_precision_mean": 0.9973333875338236,
        "bertscore_recall_mean": 0.9980812470118204
      },
      "avg_output_length_chars": 709.3333333333334,
      "avg_output_length_words": 104.33333333333333,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_018"
    },
    "gpt4_summarization_C3_t0.0_abs_019": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 158.66666666666666,
        "std": 100.91030120304313,
        "min": 16,
        "max": 233,
        "normalized_mean": 0.26584978443385526
      },
      "rouge_l": {
        "mean": 0.7494419946649246,
        "std": 0.16818621843555373,
        "min": 0.6242038216560509,
        "max": 0.9871794871794872
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9763846000035604,
        "bertscore_f1_std": 0.016531574471562494,
        "bertscore_f1_min": 0.9645847678184509,
        "bertscore_f1_max": 0.9997634291648865,
        "bertscore_precision_mean": 0.9738742709159851,
        "bertscore_recall_mean": 0.9789146582285563
      },
      "avg_output_length_chars": 576.0,
      "avg_output_length_words": 78.33333333333333,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_019"
    },
    "gpt4_summarization_C3_t0.0_abs_020": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 484.0,
      "avg_output_length_words": 74.0,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_020"
    },
    "gpt4_summarization_C3_t0.0_abs_021": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 569.0,
      "avg_output_length_words": 81.0,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_021"
    },
    "gpt4_summarization_C3_t0.0_abs_022": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 35.333333333333336,
        "std": 21.545816814923082,
        "min": 5,
        "max": 53,
        "normalized_mean": 0.048534798534798536
      },
      "rouge_l": {
        "mean": 0.9375357961053837,
        "std": 0.037123478015887675,
        "min": 0.9062499999999999,
        "max": 0.9896907216494846
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9943760832150778,
        "bertscore_f1_std": 0.003383207253741092,
        "bertscore_f1_min": 0.9914920330047607,
        "bertscore_f1_max": 0.99912428855896,
        "bertscore_precision_mean": 0.994406263033549,
        "bertscore_recall_mean": 0.9943466981252035
      },
      "avg_output_length_chars": 720.6666666666666,
      "avg_output_length_words": 96.33333333333333,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_022"
    },
    "gpt4_summarization_C3_t0.0_abs_023": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 66.0,
        "std": 41.817061908587824,
        "min": 7,
        "max": 99,
        "normalized_mean": 0.10320872091317235
      },
      "rouge_l": {
        "mean": 0.8961209091714273,
        "std": 0.06987024110657786,
        "min": 0.84375,
        "max": 0.9948717948717948
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9883931279182434,
        "bertscore_f1_std": 0.006502210072434868,
        "bertscore_f1_min": 0.9829667806625366,
        "bertscore_f1_max": 0.9975354671478271,
        "bertscore_precision_mean": 0.9888781507809957,
        "bertscore_recall_mean": 0.987908681233724
      },
      "avg_output_length_chars": 635.0,
      "avg_output_length_words": 96.66666666666667,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_023"
    },
    "gpt4_summarization_C3_t0.0_abs_024": {
      "n_outputs": 3,
      "exact_match_rate": 0.3333333333333333,
      "edit_distance": {
        "mean": 4.666666666666667,
        "std": 3.299831645537222,
        "min": 0,
        "max": 7,
        "normalized_mean": 0.007478632478632479
      },
      "rouge_l": {
        "mean": 0.9919678714859437,
        "std": 0.005679572539651002,
        "min": 0.9879518072289156,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.99848739306132,
        "bertscore_f1_std": 0.0010696589173074833,
        "bertscore_f1_min": 0.9977310299873352,
        "bertscore_f1_max": 1.0000001192092896,
        "bertscore_precision_mean": 0.9981539646784464,
        "bertscore_recall_mean": 0.9988211393356323
      },
      "avg_output_length_chars": 620.6666666666666,
      "avg_output_length_words": 83.0,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_024"
    },
    "gpt4_summarization_C3_t0.0_abs_025": {
      "n_outputs": 3,
      "exact_match_rate": 0.3333333333333333,
      "edit_distance": {
        "mean": 51.333333333333336,
        "std": 36.29814810090944,
        "min": 0,
        "max": 77,
        "normalized_mean": 0.08627450980392158
      },
      "rouge_l": {
        "mean": 0.8968253968253969,
        "std": 0.07295546155099294,
        "min": 0.8452380952380953,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9897330204645792,
        "bertscore_f1_std": 0.007259850851799511,
        "bertscore_f1_min": 0.9845995306968689,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9874544143676758,
        "bertscore_recall_mean": 0.9920275211334229
      },
      "avg_output_length_chars": 578.3333333333334,
      "avg_output_length_words": 83.33333333333333,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_025"
    },
    "gpt4_summarization_C3_t0.0_abs_026": {
      "n_outputs": 3,
      "exact_match_rate": 0.3333333333333333,
      "edit_distance": {
        "mean": 66.0,
        "std": 46.66904755831214,
        "min": 0,
        "max": 99,
        "normalized_mean": 0.11320754716981131
      },
      "rouge_l": {
        "mean": 0.8881987577639752,
        "std": 0.079055416530173,
        "min": 0.8322981366459627,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9844005505243937,
        "bertscore_f1_std": 0.011030392213281113,
        "bertscore_f1_min": 0.9766008853912354,
        "bertscore_f1_max": 0.9999998807907104,
        "bertscore_precision_mean": 0.9898120164871216,
        "bertscore_recall_mean": 0.9790776173273722
      },
      "avg_output_length_chars": 551.0,
      "avg_output_length_words": 79.66666666666667,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_026"
    },
    "gpt4_summarization_C3_t0.0_abs_027": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 110.0,
        "std": 4.96655480858378,
        "min": 106,
        "max": 117,
        "normalized_mean": 0.15730221028713076
      },
      "rouge_l": {
        "mean": 0.8491545939665569,
        "std": 0.02808308091023349,
        "min": 0.813953488372093,
        "max": 0.8826815642458101
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9733073314030966,
        "bertscore_f1_std": 0.005549027609446078,
        "bertscore_f1_min": 0.9692895412445068,
        "bertscore_f1_max": 0.9811540842056274,
        "bertscore_precision_mean": 0.9703095157941183,
        "bertscore_recall_mean": 0.9764037132263184
      },
      "avg_output_length_chars": 670.0,
      "avg_output_length_words": 88.66666666666667,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_027"
    },
    "gpt4_summarization_C3_t0.0_abs_028": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 208.0,
        "std": 66.87799837515075,
        "min": 143,
        "max": 300,
        "normalized_mean": 0.34106921330430917
      },
      "rouge_l": {
        "mean": 0.6946798450031083,
        "std": 0.09931023663257513,
        "min": 0.5744680851063831,
        "max": 0.8176795580110497
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9505597949028015,
        "bertscore_f1_std": 0.018333799315723424,
        "bertscore_f1_min": 0.9309062957763672,
        "bertscore_f1_max": 0.9750322699546814,
        "bertscore_precision_mean": 0.9499925176302592,
        "bertscore_recall_mean": 0.9511303901672363
      },
      "avg_output_length_chars": 603.6666666666666,
      "avg_output_length_words": 92.33333333333333,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_028"
    },
    "gpt4_summarization_C3_t0.0_abs_029": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 11.333333333333334,
        "std": 7.318166133366716,
        "min": 1,
        "max": 17,
        "normalized_mean": 0.016260162601626018
      },
      "rouge_l": {
        "mean": 0.9670452303667245,
        "std": 0.01289418419952343,
        "min": 0.9556650246305418,
        "max": 0.9850746268656716
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9966552058855692,
        "bertscore_f1_std": 0.0012499332334118703,
        "bertscore_f1_min": 0.9952794313430786,
        "bertscore_f1_max": 0.9983043074607849,
        "bertscore_precision_mean": 0.9968223770459493,
        "bertscore_recall_mean": 0.9964882334073385
      },
      "avg_output_length_chars": 696.6666666666666,
      "avg_output_length_words": 101.0,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_029"
    },
    "gpt4_summarization_C3_t0.0_abs_030": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 113.66666666666667,
        "std": 10.656244908763854,
        "min": 100,
        "max": 126,
        "normalized_mean": 0.225975265471111
      },
      "rouge_l": {
        "mean": 0.7982439391441348,
        "std": 0.012795069433941975,
        "min": 0.7808219178082192,
        "max": 0.8111888111888113
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9712936480840048,
        "bertscore_f1_std": 0.0042083647954021565,
        "bertscore_f1_min": 0.9664939641952515,
        "bertscore_f1_max": 0.9767410755157471,
        "bertscore_precision_mean": 0.9693353970845541,
        "bertscore_recall_mean": 0.9732784628868103
      },
      "avg_output_length_chars": 494.3333333333333,
      "avg_output_length_words": 72.66666666666667,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_030"
    },
    "gpt4_summarization_C3_t0.3_abs_001": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 108.66666666666667,
        "std": 23.697163449568293,
        "min": 79,
        "max": 137,
        "normalized_mean": 0.1737749057619817
      },
      "rouge_l": {
        "mean": 0.8399065494575116,
        "std": 0.04264437468566383,
        "min": 0.7868852459016393,
        "max": 0.8913043478260869
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9764006733894348,
        "bertscore_f1_std": 0.006309191579862049,
        "bertscore_f1_min": 0.9702041745185852,
        "bertscore_f1_max": 0.9850587248802185,
        "bertscore_precision_mean": 0.9730482498804728,
        "bertscore_recall_mean": 0.9797781904538473
      },
      "avg_output_length_chars": 618.6666666666666,
      "avg_output_length_words": 91.66666666666667,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_001"
    },
    "gpt4_summarization_C3_t0.3_abs_002": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 132.66666666666666,
        "std": 41.84362423223984,
        "min": 75,
        "max": 173,
        "normalized_mean": 0.18671319102353587
      },
      "rouge_l": {
        "mean": 0.7250293351273558,
        "std": 0.06315732028238917,
        "min": 0.6598984771573604,
        "max": 0.8105263157894737
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9652149279912313,
        "bertscore_f1_std": 0.009587984359254013,
        "bertscore_f1_min": 0.9571111798286438,
        "bertscore_f1_max": 0.978681743144989,
        "bertscore_precision_mean": 0.9641685684521993,
        "bertscore_recall_mean": 0.9662638703982035
      },
      "avg_output_length_chars": 698.3333333333334,
      "avg_output_length_words": 96.66666666666667,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_002"
    },
    "gpt4_summarization_C3_t0.3_abs_003": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 55.0,
        "std": 13.92838827718412,
        "min": 41,
        "max": 74,
        "normalized_mean": 0.08496302356233403
      },
      "rouge_l": {
        "mean": 0.9057789896235704,
        "std": 0.024584952846643755,
        "min": 0.8711656441717791,
        "max": 0.9259259259259259
      },
      "bert_score": {
        "bertscore_f1_mean": 0.990312655766805,
        "bertscore_f1_std": 0.0029287968493856165,
        "bertscore_f1_min": 0.9862573146820068,
        "bertscore_f1_max": 0.9930700063705444,
        "bertscore_precision_mean": 0.9909801681836446,
        "bertscore_recall_mean": 0.9896475076675415
      },
      "avg_output_length_chars": 634.0,
      "avg_output_length_words": 81.33333333333333,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_003"
    },
    "gpt4_summarization_C3_t0.3_abs_004": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 109.0,
        "std": 48.33218389437829,
        "min": 41,
        "max": 149,
        "normalized_mean": 0.18106312292358803
      },
      "rouge_l": {
        "mean": 0.7588986259872336,
        "std": 0.07151500999527885,
        "min": 0.6962025316455697,
        "max": 0.8589743589743589
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9826671282450358,
        "bertscore_f1_std": 0.006033335584098789,
        "bertscore_f1_min": 0.9783991575241089,
        "bertscore_f1_max": 0.9911995530128479,
        "bertscore_precision_mean": 0.9821099042892456,
        "bertscore_recall_mean": 0.9832252065340678
      },
      "avg_output_length_chars": 600.3333333333334,
      "avg_output_length_words": 78.66666666666667,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_004"
    },
    "gpt4_summarization_C3_t0.3_abs_005": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 117.33333333333333,
        "std": 39.04128868547018,
        "min": 63,
        "max": 153,
        "normalized_mean": 0.23413811420982733
      },
      "rouge_l": {
        "mean": 0.7965252529620787,
        "std": 0.07232779874238399,
        "min": 0.7299270072992701,
        "max": 0.8970588235294118
      },
      "bert_score": {
        "bertscore_f1_mean": 0.974419375260671,
        "bertscore_f1_std": 0.005638919531724003,
        "bertscore_f1_min": 0.9678794145584106,
        "bertscore_f1_max": 0.9816412925720215,
        "bertscore_precision_mean": 0.9715907573699951,
        "bertscore_recall_mean": 0.9772689541180929
      },
      "avg_output_length_chars": 497.3333333333333,
      "avg_output_length_words": 68.66666666666667,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_005"
    },
    "gpt4_summarization_C3_t0.3_abs_006": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 339.6666666666667,
        "std": 100.30730559413685,
        "min": 200,
        "max": 431,
        "normalized_mean": 0.4624624624624625
      },
      "rouge_l": {
        "mean": 0.56024020055469,
        "std": 0.11217705884550884,
        "min": 0.4723618090452262,
        "max": 0.718562874251497
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9439856211344401,
        "bertscore_f1_std": 0.014366062001424528,
        "bertscore_f1_min": 0.9327288866043091,
        "bertscore_f1_max": 0.9642611742019653,
        "bertscore_precision_mean": 0.9306488633155823,
        "bertscore_recall_mean": 0.9577820499738058
      },
      "avg_output_length_chars": 643.3333333333334,
      "avg_output_length_words": 93.66666666666667,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_006"
    },
    "gpt4_summarization_C3_t0.3_abs_007": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 186.66666666666666,
        "std": 101.68688323585408,
        "min": 43,
        "max": 264,
        "normalized_mean": 0.33622382889478314
      },
      "rouge_l": {
        "mean": 0.6646031245779119,
        "std": 0.18672471582911657,
        "min": 0.5263157894736843,
        "max": 0.9285714285714286
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9598183433214823,
        "bertscore_f1_std": 0.022229700457846887,
        "bertscore_f1_min": 0.9436303377151489,
        "bertscore_f1_max": 0.9912511706352234,
        "bertscore_precision_mean": 0.9562675356864929,
        "bertscore_recall_mean": 0.9634007612864176
      },
      "avg_output_length_chars": 543.6666666666666,
      "avg_output_length_words": 84.33333333333333,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_007"
    },
    "gpt4_summarization_C3_t0.3_abs_008": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 275.6666666666667,
        "std": 25.223445883190152,
        "min": 240,
        "max": 294,
        "normalized_mean": 0.41235394224758987
      },
      "rouge_l": {
        "mean": 0.6420332355816227,
        "std": 0.06219098734525177,
        "min": 0.5806451612903225,
        "max": 0.7272727272727274
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9506762822469076,
        "bertscore_f1_std": 0.009844375348640081,
        "bertscore_f1_min": 0.9370300769805908,
        "bertscore_f1_max": 0.9598875641822815,
        "bertscore_precision_mean": 0.9491143226623535,
        "bertscore_recall_mean": 0.9523333708445231
      },
      "avg_output_length_chars": 624.3333333333334,
      "avg_output_length_words": 82.66666666666667,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_008"
    },
    "gpt4_summarization_C3_t0.3_abs_009": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 279.6666666666667,
        "std": 18.517259216441534,
        "min": 254,
        "max": 297,
        "normalized_mean": 0.36966357111269854
      },
      "rouge_l": {
        "mean": 0.6142983347317706,
        "std": 0.04893819704663477,
        "min": 0.5567010309278351,
        "max": 0.6763285024154589
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9438060522079468,
        "bertscore_f1_std": 0.004039453712577864,
        "bertscore_f1_min": 0.9403561353683472,
        "bertscore_f1_max": 0.9494742751121521,
        "bertscore_precision_mean": 0.9492815136909485,
        "bertscore_recall_mean": 0.9384134610493978
      },
      "avg_output_length_chars": 700.6666666666666,
      "avg_output_length_words": 104.0,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_009"
    },
    "gpt4_summarization_C3_t0.3_abs_010": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 299.0,
        "std": 7.483314773547883,
        "min": 291,
        "max": 309,
        "normalized_mean": 0.4551530323068203
      },
      "rouge_l": {
        "mean": 0.5706362127659784,
        "std": 0.008850518367541832,
        "min": 0.5583756345177665,
        "max": 0.5789473684210527
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9433873494466146,
        "bertscore_f1_std": 0.003656994726499727,
        "bertscore_f1_min": 0.9382178783416748,
        "bertscore_f1_max": 0.9461056590080261,
        "bertscore_precision_mean": 0.9391826192537943,
        "bertscore_recall_mean": 0.9476321736971537
      },
      "avg_output_length_chars": 627.3333333333334,
      "avg_output_length_words": 94.66666666666667,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_010"
    },
    "gpt4_summarization_C3_t0.3_abs_011": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 266.6666666666667,
        "std": 63.46827729048759,
        "min": 177,
        "max": 315,
        "normalized_mean": 0.4067056530214425
      },
      "rouge_l": {
        "mean": 0.6292587735031238,
        "std": 0.10204223137153265,
        "min": 0.5423728813559322,
        "max": 0.7724867724867724
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9525306820869446,
        "bertscore_f1_std": 0.011788996841908162,
        "bertscore_f1_min": 0.9396346211433411,
        "bertscore_f1_max": 0.9681296348571777,
        "bertscore_precision_mean": 0.9518374601999918,
        "bertscore_recall_mean": 0.9533309936523438
      },
      "avg_output_length_chars": 631.3333333333334,
      "avg_output_length_words": 93.0,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_011"
    },
    "gpt4_summarization_C3_t0.3_abs_012": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 321.6666666666667,
        "std": 11.61416759345623,
        "min": 312,
        "max": 338,
        "normalized_mean": 0.4201002052916434
      },
      "rouge_l": {
        "mean": 0.6069958799655879,
        "std": 0.01820614278175143,
        "min": 0.5869565217391305,
        "max": 0.6310160427807486
      },
      "bert_score": {
        "bertscore_f1_mean": 0.957228422164917,
        "bertscore_f1_std": 0.0015858991724704726,
        "bertscore_f1_min": 0.9555519819259644,
        "bertscore_f1_max": 0.9593569040298462,
        "bertscore_precision_mean": 0.9575852950414022,
        "bertscore_recall_mean": 0.9569341937700907
      },
      "avg_output_length_chars": 723.0,
      "avg_output_length_words": 95.0,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_012"
    },
    "gpt4_summarization_C3_t0.3_abs_013": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 78.0,
        "std": 31.822423959633664,
        "min": 36,
        "max": 113,
        "normalized_mean": 0.18031579133489326
      },
      "rouge_l": {
        "mean": 0.8440671350507416,
        "std": 0.05294055342614231,
        "min": 0.7704918032786886,
        "max": 0.8928571428571429
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9836233456929525,
        "bertscore_f1_std": 0.005609396329281139,
        "bertscore_f1_min": 0.9763435125350952,
        "bertscore_f1_max": 0.9899929165840149,
        "bertscore_precision_mean": 0.9816045165061951,
        "bertscore_recall_mean": 0.9856522083282471
      },
      "avg_output_length_chars": 402.3333333333333,
      "avg_output_length_words": 59.333333333333336,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_013"
    },
    "gpt4_summarization_C3_t0.3_abs_014": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 258.0,
        "std": 105.53672346628922,
        "min": 109,
        "max": 340,
        "normalized_mean": 0.4287535688397757
      },
      "rouge_l": {
        "mean": 0.5548074777210904,
        "std": 0.1784902632458855,
        "min": 0.42774566473988446,
        "max": 0.8072289156626505
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9437627593676249,
        "bertscore_f1_std": 0.016913561020038242,
        "bertscore_f1_min": 0.9260962009429932,
        "bertscore_f1_max": 0.9665610790252686,
        "bertscore_precision_mean": 0.9415523608525594,
        "bertscore_recall_mean": 0.9460244576136271
      },
      "avg_output_length_chars": 593.0,
      "avg_output_length_words": 83.66666666666667,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_014"
    },
    "gpt4_summarization_C3_t0.3_abs_015": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 197.33333333333334,
        "std": 35.11251755270318,
        "min": 154,
        "max": 240,
        "normalized_mean": 0.3190011229753136
      },
      "rouge_l": {
        "mean": 0.7011999430527008,
        "std": 0.058465999658033965,
        "min": 0.6206896551724138,
        "max": 0.7577639751552796
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9653571446736654,
        "bertscore_f1_std": 0.005574270308824728,
        "bertscore_f1_min": 0.9606224298477173,
        "bertscore_f1_max": 0.9731830358505249,
        "bertscore_precision_mean": 0.9608195821444193,
        "bertscore_recall_mean": 0.9699482123057047
      },
      "avg_output_length_chars": 585.3333333333334,
      "avg_output_length_words": 84.33333333333333,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_015"
    },
    "gpt4_summarization_C3_t0.3_abs_016": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 151.66666666666666,
        "std": 39.160637833870325,
        "min": 108,
        "max": 203,
        "normalized_mean": 0.20342458452794535
      },
      "rouge_l": {
        "mean": 0.7860333041851194,
        "std": 0.044792744695333286,
        "min": 0.7227722772277229,
        "max": 0.8205128205128205
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9752076268196106,
        "bertscore_f1_std": 0.00789996379809924,
        "bertscore_f1_min": 0.9660546183586121,
        "bertscore_f1_max": 0.985332190990448,
        "bertscore_precision_mean": 0.9730492234230042,
        "bertscore_recall_mean": 0.9774093230565389
      },
      "avg_output_length_chars": 721.6666666666666,
      "avg_output_length_words": 97.66666666666667,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_016"
    },
    "gpt4_summarization_C3_t0.3_abs_017": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 261.6666666666667,
        "std": 55.99007848618261,
        "min": 195,
        "max": 332,
        "normalized_mean": 0.5336763336763336
      },
      "rouge_l": {
        "mean": 0.4831464664236805,
        "std": 0.09161937708010415,
        "min": 0.36496350364963503,
        "max": 0.588235294117647
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9525062243143717,
        "bertscore_f1_std": 0.008440596479241795,
        "bertscore_f1_min": 0.9406051635742188,
        "bertscore_f1_max": 0.9592561721801758,
        "bertscore_precision_mean": 0.9536816875139872,
        "bertscore_recall_mean": 0.9513435165087382
      },
      "avg_output_length_chars": 475.0,
      "avg_output_length_words": 67.66666666666667,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_017"
    },
    "gpt4_summarization_C3_t0.3_abs_018": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 96.66666666666667,
        "std": 32.82614134429381,
        "min": 59,
        "max": 139,
        "normalized_mean": 0.13044477724160458
      },
      "rouge_l": {
        "mean": 0.880777075392526,
        "std": 0.03628104186030371,
        "min": 0.8363636363636364,
        "max": 0.9252336448598131
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9832075436909994,
        "bertscore_f1_std": 0.004114331067221993,
        "bertscore_f1_min": 0.9778169393539429,
        "bertscore_f1_max": 0.9877995848655701,
        "bertscore_precision_mean": 0.984997550646464,
        "bertscore_recall_mean": 0.9814260601997375
      },
      "avg_output_length_chars": 724.0,
      "avg_output_length_words": 108.66666666666667,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_018"
    },
    "gpt4_summarization_C3_t0.3_abs_019": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 201.33333333333334,
        "std": 86.58072662101088,
        "min": 79,
        "max": 267,
        "normalized_mean": 0.34052092834701525
      },
      "rouge_l": {
        "mean": 0.6690804802759263,
        "std": 0.12770737905910629,
        "min": 0.576923076923077,
        "max": 0.8496732026143792
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9728450179100037,
        "bertscore_f1_std": 0.009803825571875415,
        "bertscore_f1_min": 0.9653360247612,
        "bertscore_f1_max": 0.986693263053894,
        "bertscore_precision_mean": 0.9735440413157145,
        "bertscore_recall_mean": 0.9721497495969137
      },
      "avg_output_length_chars": 564.0,
      "avg_output_length_words": 77.33333333333333,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_019"
    },
    "gpt4_summarization_C3_t0.3_abs_020": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 186.66666666666666,
        "std": 60.00740695021648,
        "min": 102,
        "max": 234,
        "normalized_mean": 0.3895236256632561
      },
      "rouge_l": {
        "mean": 0.6389971932829075,
        "std": 0.12343082072299193,
        "min": 0.5442176870748299,
        "max": 0.8133333333333332
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9500310222307841,
        "bertscore_f1_std": 0.022430407420097862,
        "bertscore_f1_min": 0.9332284927368164,
        "bertscore_f1_max": 0.9817333817481995,
        "bertscore_precision_mean": 0.9491872588793436,
        "bertscore_recall_mean": 0.9508801102638245
      },
      "avg_output_length_chars": 472.0,
      "avg_output_length_words": 73.33333333333333,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_020"
    },
    "gpt4_summarization_C3_t0.3_abs_021": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 52.666666666666664,
        "std": 18.62495339293199,
        "min": 39,
        "max": 79,
        "normalized_mean": 0.09346553096690757
      },
      "rouge_l": {
        "mean": 0.9331146444375835,
        "std": 0.03190453727278952,
        "min": 0.8987341772151898,
        "max": 0.975609756097561
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9850161472956339,
        "bertscore_f1_std": 0.005381085823912291,
        "bertscore_f1_min": 0.9780416488647461,
        "bertscore_f1_max": 0.9911399483680725,
        "bertscore_precision_mean": 0.9829757213592529,
        "bertscore_recall_mean": 0.9870670040448507
      },
      "avg_output_length_chars": 557.3333333333334,
      "avg_output_length_words": 80.33333333333333,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_021"
    },
    "gpt4_summarization_C3_t0.3_abs_022": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 51.333333333333336,
        "std": 6.649979114420002,
        "min": 42,
        "max": 57,
        "normalized_mean": 0.07176893103610159
      },
      "rouge_l": {
        "mean": 0.9100217590728062,
        "std": 0.002769231446103752,
        "min": 0.90625,
        "max": 0.9128205128205128
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9871180256207784,
        "bertscore_f1_std": 0.0011578874288598,
        "bertscore_f1_min": 0.9855177402496338,
        "bertscore_f1_max": 0.9882187843322754,
        "bertscore_precision_mean": 0.9892858465512594,
        "bertscore_recall_mean": 0.9849613110224406
      },
      "avg_output_length_chars": 708.0,
      "avg_output_length_words": 96.33333333333333,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_022"
    },
    "gpt4_summarization_C3_t0.3_abs_023": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 69.33333333333333,
        "std": 21.452790546272116,
        "min": 49,
        "max": 99,
        "normalized_mean": 0.10907208733295691
      },
      "rouge_l": {
        "mean": 0.8700366570903775,
        "std": 0.03458161873354972,
        "min": 0.8272251308900525,
        "max": 0.9119170984455959
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9884332815806071,
        "bertscore_f1_std": 0.0036313714053309142,
        "bertscore_f1_min": 0.9841406345367432,
        "bertscore_f1_max": 0.9930209517478943,
        "bertscore_precision_mean": 0.9888566931088766,
        "bertscore_recall_mean": 0.9880171616872152
      },
      "avg_output_length_chars": 623.6666666666666,
      "avg_output_length_words": 95.0,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_023"
    },
    "gpt4_summarization_C3_t0.3_abs_024": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 37.333333333333336,
        "std": 4.109609335312651,
        "min": 32,
        "max": 42,
        "normalized_mean": 0.06054638692669673
      },
      "rouge_l": {
        "mean": 0.9397590361445783,
        "std": 0.0,
        "min": 0.9397590361445783,
        "max": 0.9397590361445783
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9915726780891418,
        "bertscore_f1_std": 0.0008948534126493666,
        "bertscore_f1_min": 0.9904200434684753,
        "bertscore_f1_max": 0.9926014542579651,
        "bertscore_precision_mean": 0.9913293123245239,
        "bertscore_recall_mean": 0.9918182492256165
      },
      "avg_output_length_chars": 613.6666666666666,
      "avg_output_length_words": 83.0,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_024"
    },
    "gpt4_summarization_C3_t0.3_abs_025": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 79.66666666666667,
        "std": 12.918548250050733,
        "min": 66,
        "max": 97,
        "normalized_mean": 0.13482045040754612
      },
      "rouge_l": {
        "mean": 0.8407658632799345,
        "std": 0.01031757556744845,
        "min": 0.8284023668639052,
        "max": 0.8536585365853658
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9827663898468018,
        "bertscore_f1_std": 0.003461752072817973,
        "bertscore_f1_min": 0.9797028303146362,
        "bertscore_f1_max": 0.9876052141189575,
        "bertscore_precision_mean": 0.9846828778584799,
        "bertscore_recall_mean": 0.9808599154154459
      },
      "avg_output_length_chars": 577.0,
      "avg_output_length_words": 83.66666666666667,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_025"
    },
    "gpt4_summarization_C3_t0.3_abs_026": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 187.33333333333334,
        "std": 36.444783196257625,
        "min": 136,
        "max": 217,
        "normalized_mean": 0.32296289150427676
      },
      "rouge_l": {
        "mean": 0.6639668684962386,
        "std": 0.04729989169521279,
        "min": 0.6114649681528662,
        "max": 0.7261146496815287
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9625668327013651,
        "bertscore_f1_std": 0.0014986155743556139,
        "bertscore_f1_min": 0.961287260055542,
        "bertscore_f1_max": 0.9646697640419006,
        "bertscore_precision_mean": 0.9612996578216553,
        "bertscore_recall_mean": 0.9638388554255167
      },
      "avg_output_length_chars": 572.0,
      "avg_output_length_words": 79.33333333333333,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_026"
    },
    "gpt4_summarization_C3_t0.3_abs_027": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 175.0,
        "std": 55.93448548674303,
        "min": 107,
        "max": 244,
        "normalized_mean": 0.27203580828951873
      },
      "rouge_l": {
        "mean": 0.7065261753648965,
        "std": 0.08595167170869326,
        "min": 0.6012269938650308,
        "max": 0.8117647058823529
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9555996259053549,
        "bertscore_f1_std": 0.014506068115858282,
        "bertscore_f1_min": 0.9372484683990479,
        "bertscore_f1_max": 0.9727165699005127,
        "bertscore_precision_mean": 0.9556116064389547,
        "bertscore_recall_mean": 0.9556187391281128
      },
      "avg_output_length_chars": 634.0,
      "avg_output_length_words": 83.33333333333333,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_027"
    },
    "gpt4_summarization_C3_t0.3_abs_028": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 184.66666666666666,
        "std": 28.015868519267592,
        "min": 148,
        "max": 216,
        "normalized_mean": 0.2788929858890607
      },
      "rouge_l": {
        "mean": 0.7331809227156274,
        "std": 0.049924073737322036,
        "min": 0.6633663366336634,
        "max": 0.77720207253886
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9600185751914978,
        "bertscore_f1_std": 0.008461916484872312,
        "bertscore_f1_min": 0.949806272983551,
        "bertscore_f1_max": 0.9705272912979126,
        "bertscore_precision_mean": 0.9580256342887878,
        "bertscore_recall_mean": 0.9620383580525717
      },
      "avg_output_length_chars": 641.6666666666666,
      "avg_output_length_words": 98.33333333333333,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_028"
    },
    "gpt4_summarization_C3_t0.3_abs_029": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 359.3333333333333,
        "std": 19.770910168449223,
        "min": 342,
        "max": 387,
        "normalized_mean": 0.477142772975212
      },
      "rouge_l": {
        "mean": 0.6029644071579555,
        "std": 0.005065857435295271,
        "min": 0.5961538461538461,
        "max": 0.6082949308755762
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9473325411478678,
        "bertscore_f1_std": 0.01098462034574633,
        "bertscore_f1_min": 0.9378795623779297,
        "bertscore_f1_max": 0.9627348780632019,
        "bertscore_precision_mean": 0.9429060220718384,
        "bertscore_recall_mean": 0.951805830001831
      },
      "avg_output_length_chars": 723.3333333333334,
      "avg_output_length_words": 108.33333333333333,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_029"
    },
    "gpt4_summarization_C3_t0.3_abs_030": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 210.0,
        "std": 50.2858495669176,
        "min": 139,
        "max": 249,
        "normalized_mean": 0.42666721981372774
      },
      "rouge_l": {
        "mean": 0.6405393143172762,
        "std": 0.05766061320590748,
        "min": 0.5735294117647058,
        "max": 0.7142857142857143
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9575939774513245,
        "bertscore_f1_std": 0.004877243236688382,
        "bertscore_f1_min": 0.953236997127533,
        "bertscore_f1_max": 0.9644032120704651,
        "bertscore_precision_mean": 0.958048423131307,
        "bertscore_recall_mean": 0.9571410218874613
      },
      "avg_output_length_chars": 483.3333333333333,
      "avg_output_length_words": 69.66666666666667,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_030"
    },
    "gpt4_summarization_C3_t0.7_abs_001": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 258.3333333333333,
        "std": 24.997777679003566,
        "min": 235,
        "max": 293,
        "normalized_mean": 0.40656829009835976
      },
      "rouge_l": {
        "mean": 0.6038148038148039,
        "std": 0.03971861708588944,
        "min": 0.5604395604395604,
        "max": 0.6564102564102564
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9553948044776917,
        "bertscore_f1_std": 0.005728511365660788,
        "bertscore_f1_min": 0.9474099278450012,
        "bertscore_f1_max": 0.9605726003646851,
        "bertscore_precision_mean": 0.9596510529518127,
        "bertscore_recall_mean": 0.9511876900990804
      },
      "avg_output_length_chars": 619.6666666666666,
      "avg_output_length_words": 93.66666666666667,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_001"
    },
    "gpt4_summarization_C3_t0.7_abs_002": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 267.0,
        "std": 35.59026084010437,
        "min": 217,
        "max": 297,
        "normalized_mean": 0.3647928385879011
      },
      "rouge_l": {
        "mean": 0.616170308608417,
        "std": 0.04064159973126923,
        "min": 0.5812807881773399,
        "max": 0.6731707317073171
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9518647789955139,
        "bertscore_f1_std": 0.004832667376626702,
        "bertscore_f1_min": 0.9479776620864868,
        "bertscore_f1_max": 0.9586765766143799,
        "bertscore_precision_mean": 0.9505723317464193,
        "bertscore_recall_mean": 0.9531953533490499
      },
      "avg_output_length_chars": 723.6666666666666,
      "avg_output_length_words": 101.66666666666667,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_002"
    },
    "gpt4_summarization_C3_t0.7_abs_003": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 241.66666666666666,
        "std": 18.83849486792639,
        "min": 225,
        "max": 268,
        "normalized_mean": 0.35177309908904997
      },
      "rouge_l": {
        "mean": 0.6202829080313924,
        "std": 0.039160818934959575,
        "min": 0.5664739884393063,
        "max": 0.6585365853658537
      },
      "bert_score": {
        "bertscore_f1_mean": 0.955872635046641,
        "bertscore_f1_std": 0.004876895634265973,
        "bertscore_f1_min": 0.9501563906669617,
        "bertscore_f1_max": 0.9620728492736816,
        "bertscore_precision_mean": 0.9568946162859598,
        "bertscore_recall_mean": 0.9548559784889221
      },
      "avg_output_length_chars": 662.3333333333334,
      "avg_output_length_words": 85.0,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_003"
    },
    "gpt4_summarization_C3_t0.7_abs_004": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 204.33333333333334,
        "std": 18.624953392931992,
        "min": 184,
        "max": 229,
        "normalized_mean": 0.34025167769618103
      },
      "rouge_l": {
        "mean": 0.6101237314940701,
        "std": 0.01969001874634432,
        "min": 0.5853658536585366,
        "max": 0.6335403726708074
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9620199799537659,
        "bertscore_f1_std": 0.0017502727017931232,
        "bertscore_f1_min": 0.9603562355041504,
        "bertscore_f1_max": 0.9644390344619751,
        "bertscore_precision_mean": 0.9612269997596741,
        "bertscore_recall_mean": 0.9628240863482157
      },
      "avg_output_length_chars": 592.6666666666666,
      "avg_output_length_words": 80.33333333333333,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_004"
    },
    "gpt4_summarization_C3_t0.7_abs_005": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 171.33333333333334,
        "std": 57.418541333691934,
        "min": 92,
        "max": 226,
        "normalized_mean": 0.33579265331021396
      },
      "rouge_l": {
        "mean": 0.620881420812883,
        "std": 0.12057315219592529,
        "min": 0.510948905109489,
        "max": 0.7887323943661971
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9561504324277242,
        "bertscore_f1_std": 0.013369096149591716,
        "bertscore_f1_min": 0.9443444609642029,
        "bertscore_f1_max": 0.9748426079750061,
        "bertscore_precision_mean": 0.9555661678314209,
        "bertscore_recall_mean": 0.956735372543335
      },
      "avg_output_length_chars": 501.3333333333333,
      "avg_output_length_words": 69.0,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_005"
    },
    "gpt4_summarization_C3_t0.7_abs_006": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 352.0,
        "std": 75.66152699137565,
        "min": 245,
        "max": 406,
        "normalized_mean": 0.5096326123779299
      },
      "rouge_l": {
        "mean": 0.4307587106384225,
        "std": 0.05526821307982243,
        "min": 0.3756906077348066,
        "max": 0.5063291139240507
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9242114027341207,
        "bertscore_f1_std": 0.007906693616956504,
        "bertscore_f1_min": 0.9173325300216675,
        "bertscore_f1_max": 0.9352852702140808,
        "bertscore_precision_mean": 0.9254031578699747,
        "bertscore_recall_mean": 0.9230848948160807
      },
      "avg_output_length_chars": 612.6666666666666,
      "avg_output_length_words": 89.0,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_006"
    },
    "gpt4_summarization_C3_t0.7_abs_007": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 254.0,
        "std": 54.57716250105594,
        "min": 180,
        "max": 310,
        "normalized_mean": 0.4041018865847233
      },
      "rouge_l": {
        "mean": 0.5734952919301225,
        "std": 0.07626604082965582,
        "min": 0.5130890052356021,
        "max": 0.6810810810810811
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9427345395088196,
        "bertscore_f1_std": 0.009683903367186346,
        "bertscore_f1_min": 0.9340496063232422,
        "bertscore_f1_max": 0.9562473893165588,
        "bertscore_precision_mean": 0.940987249215444,
        "bertscore_recall_mean": 0.9445034861564636
      },
      "avg_output_length_chars": 605.6666666666666,
      "avg_output_length_words": 94.33333333333333,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_007"
    },
    "gpt4_summarization_C3_t0.7_abs_008": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 283.3333333333333,
        "std": 53.08065142361721,
        "min": 219,
        "max": 349,
        "normalized_mean": 0.4392753909290654
      },
      "rouge_l": {
        "mean": 0.5274884137718598,
        "std": 0.08374848689506556,
        "min": 0.4210526315789474,
        "max": 0.6256983240223464
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9434998830159506,
        "bertscore_f1_std": 0.005821588128076046,
        "bertscore_f1_min": 0.9383518695831299,
        "bertscore_f1_max": 0.9516380429267883,
        "bertscore_precision_mean": 0.9463711579640707,
        "bertscore_recall_mean": 0.9406864047050476
      },
      "avg_output_length_chars": 638.3333333333334,
      "avg_output_length_words": 86.33333333333333,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_008"
    },
    "gpt4_summarization_C3_t0.7_abs_009": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 473.6666666666667,
        "std": 47.316898555261304,
        "min": 412,
        "max": 527,
        "normalized_mean": 0.5471918235893392
      },
      "rouge_l": {
        "mean": 0.44651777699722905,
        "std": 0.08208935366095911,
        "min": 0.34703196347031967,
        "max": 0.5480769230769231
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9306558966636658,
        "bertscore_f1_std": 0.010711281994565358,
        "bertscore_f1_min": 0.9159618616104126,
        "bertscore_f1_max": 0.9411906003952026,
        "bertscore_precision_mean": 0.9382087786992391,
        "bertscore_recall_mean": 0.9232638080914816
      },
      "avg_output_length_chars": 780.6666666666666,
      "avg_output_length_words": 111.66666666666667,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_009"
    },
    "gpt4_summarization_C3_t0.7_abs_010": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 358.0,
        "std": 43.54308211415448,
        "min": 298,
        "max": 400,
        "normalized_mean": 0.5112105670963955
      },
      "rouge_l": {
        "mean": 0.4679774263925877,
        "std": 0.034758857961122804,
        "min": 0.44131455399061037,
        "max": 0.5170731707317072
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9347366293271383,
        "bertscore_f1_std": 0.009529304293350338,
        "bertscore_f1_min": 0.9233638048171997,
        "bertscore_f1_max": 0.9466844797134399,
        "bertscore_precision_mean": 0.9360287586847941,
        "bertscore_recall_mean": 0.9334527850151062
      },
      "avg_output_length_chars": 681.0,
      "avg_output_length_words": 103.33333333333333,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_010"
    },
    "gpt4_summarization_C3_t0.7_abs_011": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 340.3333333333333,
        "std": 60.933479212079206,
        "min": 257,
        "max": 401,
        "normalized_mean": 0.5248029301695994
      },
      "rouge_l": {
        "mean": 0.43923923923923924,
        "std": 0.06063653933904977,
        "min": 0.35555555555555557,
        "max": 0.4972972972972973
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9339221119880676,
        "bertscore_f1_std": 0.003337184241887993,
        "bertscore_f1_min": 0.9303805828094482,
        "bertscore_f1_max": 0.9383944272994995,
        "bertscore_precision_mean": 0.9311712185541788,
        "bertscore_recall_mean": 0.9367008805274963
      },
      "avg_output_length_chars": 636.3333333333334,
      "avg_output_length_words": 91.66666666666667,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_011"
    },
    "gpt4_summarization_C3_t0.7_abs_012": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 262.0,
        "std": 50.25932749251625,
        "min": 199,
        "max": 322,
        "normalized_mean": 0.3560812942533373
      },
      "rouge_l": {
        "mean": 0.5799886017891421,
        "std": 0.06997465687387341,
        "min": 0.48677248677248675,
        "max": 0.655367231638418
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9511778553326925,
        "bertscore_f1_std": 0.00895834465006251,
        "bertscore_f1_min": 0.938768208026886,
        "bertscore_f1_max": 0.9595913887023926,
        "bertscore_precision_mean": 0.9493779142697653,
        "bertscore_recall_mean": 0.9530224402745565
      },
      "avg_output_length_chars": 705.3333333333334,
      "avg_output_length_words": 91.66666666666667,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_012"
    },
    "gpt4_summarization_C3_t0.7_abs_013": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 234.33333333333334,
        "std": 68.11917661145225,
        "min": 139,
        "max": 294,
        "normalized_mean": 0.5267404190791288
      },
      "rouge_l": {
        "mean": 0.5657990604927621,
        "std": 0.11027879872207491,
        "min": 0.456140350877193,
        "max": 0.7166666666666668
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9564676880836487,
        "bertscore_f1_std": 0.011077213682637813,
        "bertscore_f1_min": 0.9477893710136414,
        "bertscore_f1_max": 0.972101628780365,
        "bertscore_precision_mean": 0.9575626850128174,
        "bertscore_recall_mean": 0.9553832014401754
      },
      "avg_output_length_chars": 431.0,
      "avg_output_length_words": 59.333333333333336,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_013"
    },
    "gpt4_summarization_C3_t0.7_abs_014": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 346.3333333333333,
        "std": 32.8870119584549,
        "min": 300,
        "max": 373,
        "normalized_mean": 0.5135710084163693
      },
      "rouge_l": {
        "mean": 0.4373588863871218,
        "std": 0.07384134194234925,
        "min": 0.37569060773480667,
        "max": 0.5411764705882353
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9340858856836954,
        "bertscore_f1_std": 0.005017138932105888,
        "bertscore_f1_min": 0.9293301701545715,
        "bertscore_f1_max": 0.9410238862037659,
        "bertscore_precision_mean": 0.9277259906133016,
        "bertscore_recall_mean": 0.9405472278594971
      },
      "avg_output_length_chars": 638.3333333333334,
      "avg_output_length_words": 86.33333333333333,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_014"
    },
    "gpt4_summarization_C3_t0.7_abs_015": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 377.6666666666667,
        "std": 49.16186417223099,
        "min": 329,
        "max": 445,
        "normalized_mean": 0.559862758366872
      },
      "rouge_l": {
        "mean": 0.41473490965809484,
        "std": 0.052159334631632685,
        "min": 0.3578947368421052,
        "max": 0.4838709677419355
      },
      "bert_score": {
        "bertscore_f1_mean": 0.92222527662913,
        "bertscore_f1_std": 0.007670702907968963,
        "bertscore_f1_min": 0.9164096713066101,
        "bertscore_f1_max": 0.9330636262893677,
        "bertscore_precision_mean": 0.9229054649670919,
        "bertscore_recall_mean": 0.9217552741368612
      },
      "avg_output_length_chars": 624.6666666666666,
      "avg_output_length_words": 90.0,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_015"
    },
    "gpt4_summarization_C3_t0.7_abs_016": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 344.0,
        "std": 17.90716802475106,
        "min": 320,
        "max": 363,
        "normalized_mean": 0.4412906454579884
      },
      "rouge_l": {
        "mean": 0.536442392078969,
        "std": 0.035320128990914926,
        "min": 0.4867724867724868,
        "max": 0.5658536585365854
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9398465951283773,
        "bertscore_f1_std": 0.005446572583232251,
        "bertscore_f1_min": 0.9339497089385986,
        "bertscore_f1_max": 0.9470866322517395,
        "bertscore_precision_mean": 0.9436917702356974,
        "bertscore_recall_mean": 0.9360467990239462
      },
      "avg_output_length_chars": 737.3333333333334,
      "avg_output_length_words": 98.0,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_016"
    },
    "gpt4_summarization_C3_t0.7_abs_017": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 244.0,
        "std": 53.23532661682466,
        "min": 170,
        "max": 293,
        "normalized_mean": 0.510808767951625
      },
      "rouge_l": {
        "mean": 0.47755685740773623,
        "std": 0.11563037017517071,
        "min": 0.3565891472868217,
        "max": 0.6333333333333334
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9548117717107137,
        "bertscore_f1_std": 0.008950741054991867,
        "bertscore_f1_min": 0.9450563192367554,
        "bertscore_f1_max": 0.9666749835014343,
        "bertscore_precision_mean": 0.9512260556221008,
        "bertscore_recall_mean": 0.9584477345148722
      },
      "avg_output_length_chars": 455.6666666666667,
      "avg_output_length_words": 63.333333333333336,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_017"
    },
    "gpt4_summarization_C3_t0.7_abs_018": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 268.3333333333333,
        "std": 43.083897481799646,
        "min": 209,
        "max": 310,
        "normalized_mean": 0.3536564451443424
      },
      "rouge_l": {
        "mean": 0.7106412814511122,
        "std": 0.02486140518689621,
        "min": 0.6757990867579909,
        "max": 0.7321428571428571
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9592076738675436,
        "bertscore_f1_std": 0.006968218533312708,
        "bertscore_f1_min": 0.9525929093360901,
        "bertscore_f1_max": 0.968841016292572,
        "bertscore_precision_mean": 0.9589246312777201,
        "bertscore_recall_mean": 0.9594909946123759
      },
      "avg_output_length_chars": 751.3333333333334,
      "avg_output_length_words": 110.66666666666667,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_018"
    },
    "gpt4_summarization_C3_t0.7_abs_019": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 240.66666666666666,
        "std": 19.344824171395878,
        "min": 226,
        "max": 268,
        "normalized_mean": 0.4276313424472145
      },
      "rouge_l": {
        "mean": 0.5744785881772184,
        "std": 0.07205600993592234,
        "min": 0.5135135135135136,
        "max": 0.6756756756756757
      },
      "bert_score": {
        "bertscore_f1_mean": 0.959860622882843,
        "bertscore_f1_std": 0.008011746492942833,
        "bertscore_f1_min": 0.9517554044723511,
        "bertscore_f1_max": 0.9707697033882141,
        "bertscore_precision_mean": 0.9608680208524069,
        "bertscore_recall_mean": 0.9588588873545328
      },
      "avg_output_length_chars": 557.3333333333334,
      "avg_output_length_words": 73.66666666666667,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_019"
    },
    "gpt4_summarization_C3_t0.7_abs_020": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 233.0,
        "std": 41.697322056298376,
        "min": 189,
        "max": 289,
        "normalized_mean": 0.4580039602122426
      },
      "rouge_l": {
        "mean": 0.5398902590683412,
        "std": 0.09226524251893149,
        "min": 0.423076923076923,
        "max": 0.6486486486486486
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9417024254798889,
        "bertscore_f1_std": 0.010202907540205926,
        "bertscore_f1_min": 0.9273111820220947,
        "bertscore_f1_max": 0.9498025178909302,
        "bertscore_precision_mean": 0.939268946647644,
        "bertscore_recall_mean": 0.9441526532173157
      },
      "avg_output_length_chars": 487.0,
      "avg_output_length_words": 75.0,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_020"
    },
    "gpt4_summarization_C3_t0.7_abs_021": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 240.0,
        "std": 7.874007874011811,
        "min": 229,
        "max": 247,
        "normalized_mean": 0.38535286188715795
      },
      "rouge_l": {
        "mean": 0.6598243710312675,
        "std": 0.007536143910679272,
        "min": 0.6538461538461539,
        "max": 0.6704545454545453
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9511847297350565,
        "bertscore_f1_std": 0.003967701037542318,
        "bertscore_f1_min": 0.9468618631362915,
        "bertscore_f1_max": 0.9564443230628967,
        "bertscore_precision_mean": 0.95883576075236,
        "bertscore_recall_mean": 0.9436663190523783
      },
      "avg_output_length_chars": 585.0,
      "avg_output_length_words": 84.33333333333333,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_021"
    },
    "gpt4_summarization_C3_t0.7_abs_022": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 318.6666666666667,
        "std": 16.110727964792762,
        "min": 296,
        "max": 332,
        "normalized_mean": 0.49020500274980555
      },
      "rouge_l": {
        "mean": 0.529013123435328,
        "std": 0.02254976267525451,
        "min": 0.4971751412429378,
        "max": 0.5465116279069768
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9476044575373331,
        "bertscore_f1_std": 0.008632179379269975,
        "bertscore_f1_min": 0.9376176595687866,
        "bertscore_f1_max": 0.958678126335144,
        "bertscore_precision_mean": 0.9497580130894979,
        "bertscore_recall_mean": 0.9454614520072937
      },
      "avg_output_length_chars": 641.0,
      "avg_output_length_words": 87.0,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_022"
    },
    "gpt4_summarization_C3_t0.7_abs_023": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 421.6666666666667,
        "std": 92.43496212052138,
        "min": 293,
        "max": 506,
        "normalized_mean": 0.5798105866038251
      },
      "rouge_l": {
        "mean": 0.519536784210248,
        "std": 0.0868755315634949,
        "min": 0.4397905759162304,
        "max": 0.6403508771929824
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9491658210754395,
        "bertscore_f1_std": 0.0061828250551725,
        "bertscore_f1_min": 0.9441857933998108,
        "bertscore_f1_max": 0.9578800201416016,
        "bertscore_precision_mean": 0.9429706533749899,
        "bertscore_recall_mean": 0.9555975596110026
      },
      "avg_output_length_chars": 661.6666666666666,
      "avg_output_length_words": 104.66666666666667,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_023"
    },
    "gpt4_summarization_C3_t0.7_abs_024": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 83.0,
        "std": 2.943920288775949,
        "min": 80,
        "max": 87,
        "normalized_mean": 0.1344301218161683
      },
      "rouge_l": {
        "mean": 0.8475001217078039,
        "std": 0.016249409404602396,
        "min": 0.8263473053892215,
        "max": 0.8658536585365854
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9818831086158752,
        "bertscore_f1_std": 0.0013850930618959669,
        "bertscore_f1_min": 0.9799372553825378,
        "bertscore_f1_max": 0.9830508828163147,
        "bertscore_precision_mean": 0.9828023115793864,
        "bertscore_recall_mean": 0.9809662699699402
      },
      "avg_output_length_chars": 609.3333333333334,
      "avg_output_length_words": 83.0,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_024"
    },
    "gpt4_summarization_C3_t0.7_abs_025": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 174.0,
        "std": 27.17842281418601,
        "min": 136,
        "max": 198,
        "normalized_mean": 0.29686435837998465
      },
      "rouge_l": {
        "mean": 0.6552835754765082,
        "std": 0.051923561968083815,
        "min": 0.5853658536585367,
        "max": 0.7096774193548387
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9633239905039469,
        "bertscore_f1_std": 0.005695687601150407,
        "bertscore_f1_min": 0.9571852087974548,
        "bertscore_f1_max": 0.9709097743034363,
        "bertscore_precision_mean": 0.9641738931337992,
        "bertscore_recall_mean": 0.962497611840566
      },
      "avg_output_length_chars": 554.0,
      "avg_output_length_words": 80.0,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_025"
    },
    "gpt4_summarization_C3_t0.7_abs_026": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 237.33333333333334,
        "std": 39.96943276499625,
        "min": 183,
        "max": 278,
        "normalized_mean": 0.4247315307385979
      },
      "rouge_l": {
        "mean": 0.5068082788671024,
        "std": 0.06955595728677846,
        "min": 0.41830065359477125,
        "max": 0.5882352941176471
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9463572899500529,
        "bertscore_f1_std": 0.0047142922645648485,
        "bertscore_f1_min": 0.9400237798690796,
        "bertscore_f1_max": 0.9513273239135742,
        "bertscore_precision_mean": 0.9466511209805807,
        "bertscore_recall_mean": 0.9460681478182474
      },
      "avg_output_length_chars": 549.6666666666666,
      "avg_output_length_words": 75.0,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_026"
    },
    "gpt4_summarization_C3_t0.7_abs_027": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 241.0,
        "std": 31.496031496047245,
        "min": 197,
        "max": 269,
        "normalized_mean": 0.3888534515661844
      },
      "rouge_l": {
        "mean": 0.5840042364251089,
        "std": 0.05810012647035885,
        "min": 0.5066666666666666,
        "max": 0.6467065868263472
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9453430573145548,
        "bertscore_f1_std": 0.011080801974826167,
        "bertscore_f1_min": 0.9327954649925232,
        "bertscore_f1_max": 0.9597467184066772,
        "bertscore_precision_mean": 0.9367532134056091,
        "bertscore_recall_mean": 0.9541499018669128
      },
      "avg_output_length_chars": 575.3333333333334,
      "avg_output_length_words": 77.33333333333333,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_027"
    },
    "gpt4_summarization_C3_t0.7_abs_028": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 266.3333333333333,
        "std": 54.93228154332892,
        "min": 190,
        "max": 317,
        "normalized_mean": 0.4178592338296541
      },
      "rouge_l": {
        "mean": 0.5996158752856401,
        "std": 0.06418086730229715,
        "min": 0.5197740112994351,
        "max": 0.6769230769230768
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9550560712814331,
        "bertscore_f1_std": 0.010404095569741072,
        "bertscore_f1_min": 0.9461674690246582,
        "bertscore_f1_max": 0.9696547985076904,
        "bertscore_precision_mean": 0.9566354751586914,
        "bertscore_recall_mean": 0.9534910519917806
      },
      "avg_output_length_chars": 606.0,
      "avg_output_length_words": 93.0,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_028"
    },
    "gpt4_summarization_C3_t0.7_abs_029": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 350.6666666666667,
        "std": 55.10192575783738,
        "min": 273,
        "max": 395,
        "normalized_mean": 0.4979328307862187
      },
      "rouge_l": {
        "mean": 0.5219570760204966,
        "std": 0.0514139472292602,
        "min": 0.4729064039408867,
        "max": 0.592964824120603
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9454125563303629,
        "bertscore_f1_std": 0.004348213951542136,
        "bertscore_f1_min": 0.9407660961151123,
        "bertscore_f1_max": 0.9512240886688232,
        "bertscore_precision_mean": 0.9464584390322367,
        "bertscore_recall_mean": 0.9443862438201904
      },
      "avg_output_length_chars": 675.6666666666666,
      "avg_output_length_words": 99.0,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_029"
    },
    "gpt4_summarization_C3_t0.7_abs_030": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 307.3333333333333,
        "std": 30.57595278791634,
        "min": 266,
        "max": 339,
        "normalized_mean": 0.5978378777069447
      },
      "rouge_l": {
        "mean": 0.44363334134422816,
        "std": 0.010531346925791196,
        "min": 0.43421052631578944,
        "max": 0.4583333333333333
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9351893663406372,
        "bertscore_f1_std": 0.007920931432108843,
        "bertscore_f1_min": 0.9279156923294067,
        "bertscore_f1_max": 0.9462040066719055,
        "bertscore_precision_mean": 0.9366840720176697,
        "bertscore_recall_mean": 0.9337817033131918
      },
      "avg_output_length_chars": 496.0,
      "avg_output_length_words": 73.66666666666667,
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_030"
    },
    "llama3_8b_extraction_C1_abs_001": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 529.0,
      "avg_output_length_words": 63.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C1",
      "abstract": "abs_001",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C1_abs_002": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 569.0,
      "avg_output_length_words": 71.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C1",
      "abstract": "abs_002",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C1_abs_003": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 426.0,
      "avg_output_length_words": 53.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C1",
      "abstract": "abs_003",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C1_abs_004": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 339.0,
      "avg_output_length_words": 42.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C1",
      "abstract": "abs_004",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C1_abs_005": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 365.0,
      "avg_output_length_words": 42.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C1",
      "abstract": "abs_005",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C1_abs_006": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 344.0,
      "avg_output_length_words": 45.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C1",
      "abstract": "abs_006",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C1_abs_007": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 333.0,
      "avg_output_length_words": 43.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C1",
      "abstract": "abs_007",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C1_abs_008": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 408.0,
      "avg_output_length_words": 47.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C1",
      "abstract": "abs_008",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C1_abs_009": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 430.0,
      "avg_output_length_words": 55.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C1",
      "abstract": "abs_009",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C1_abs_010": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 505.0,
      "avg_output_length_words": 65.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C1",
      "abstract": "abs_010",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C1_abs_011": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9999998807907104,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9999998807907104,
        "bertscore_f1_max": 0.9999998807907104,
        "bertscore_precision_mean": 0.9999998807907104,
        "bertscore_recall_mean": 0.9999998807907104
      },
      "avg_output_length_chars": 475.0,
      "avg_output_length_words": 57.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C1",
      "abstract": "abs_011",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C1_abs_012": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 486.0,
      "avg_output_length_words": 63.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C1",
      "abstract": "abs_012",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C1_abs_013": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0000001192092896,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0000001192092896,
        "bertscore_f1_max": 1.0000001192092896,
        "bertscore_precision_mean": 1.0000001192092896,
        "bertscore_recall_mean": 1.0000001192092896
      },
      "avg_output_length_chars": 454.0,
      "avg_output_length_words": 57.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C1",
      "abstract": "abs_013",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C1_abs_014": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9999998807907104,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9999998807907104,
        "bertscore_f1_max": 0.9999998807907104,
        "bertscore_precision_mean": 0.9999998807907104,
        "bertscore_recall_mean": 0.9999998807907104
      },
      "avg_output_length_chars": 449.0,
      "avg_output_length_words": 54.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C1",
      "abstract": "abs_014",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C1_abs_015": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 476.0,
      "avg_output_length_words": 62.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C1",
      "abstract": "abs_015",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C1_abs_016": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9999998807907104,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9999998807907104,
        "bertscore_f1_max": 0.9999998807907104,
        "bertscore_precision_mean": 0.9999998807907104,
        "bertscore_recall_mean": 0.9999998807907104
      },
      "avg_output_length_chars": 479.0,
      "avg_output_length_words": 58.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C1",
      "abstract": "abs_016",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C1_abs_017": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 464.0,
      "avg_output_length_words": 59.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C1",
      "abstract": "abs_017",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C1_abs_018": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 445.0,
      "avg_output_length_words": 48.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C1",
      "abstract": "abs_018",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C1_abs_019": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 668.0,
      "avg_output_length_words": 84.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C1",
      "abstract": "abs_019",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C1_abs_020": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9999999403953552,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9999999403953552,
        "bertscore_f1_max": 0.9999999403953552,
        "bertscore_precision_mean": 0.9999999403953552,
        "bertscore_recall_mean": 0.9999999403953552
      },
      "avg_output_length_chars": 381.0,
      "avg_output_length_words": 50.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C1",
      "abstract": "abs_020",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C1_abs_021": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9999999403953552,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9999999403953552,
        "bertscore_f1_max": 0.9999999403953552,
        "bertscore_precision_mean": 0.9999999403953552,
        "bertscore_recall_mean": 0.9999999403953552
      },
      "avg_output_length_chars": 503.0,
      "avg_output_length_words": 62.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C1",
      "abstract": "abs_021",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C1_abs_022": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 597.0,
      "avg_output_length_words": 73.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C1",
      "abstract": "abs_022",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C1_abs_023": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9999998807907104,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9999998807907104,
        "bertscore_f1_max": 0.9999998807907104,
        "bertscore_precision_mean": 0.9999998807907104,
        "bertscore_recall_mean": 0.9999998807907104
      },
      "avg_output_length_chars": 482.0,
      "avg_output_length_words": 68.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C1",
      "abstract": "abs_023",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C1_abs_024": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 567.0,
      "avg_output_length_words": 68.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C1",
      "abstract": "abs_024",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C1_abs_025": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0000001192092896,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0000001192092896,
        "bertscore_f1_max": 1.0000001192092896,
        "bertscore_precision_mean": 1.0000001192092896,
        "bertscore_recall_mean": 1.0000001192092896
      },
      "avg_output_length_chars": 340.0,
      "avg_output_length_words": 42.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C1",
      "abstract": "abs_025",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C1_abs_026": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 622.0,
      "avg_output_length_words": 80.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C1",
      "abstract": "abs_026",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C1_abs_027": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9999998807907104,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9999998807907104,
        "bertscore_f1_max": 0.9999998807907104,
        "bertscore_precision_mean": 0.9999998807907104,
        "bertscore_recall_mean": 0.9999998807907104
      },
      "avg_output_length_chars": 433.0,
      "avg_output_length_words": 54.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C1",
      "abstract": "abs_027",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C1_abs_028": {
      "n_outputs": 5,
      "exact_match_rate": 0.6,
      "edit_distance": {
        "mean": 40.4,
        "std": 49.4796928042202,
        "min": 0,
        "max": 101,
        "normalized_mean": 0.09351851851851853
      },
      "rouge_l": {
        "mean": 0.8969072164948454,
        "std": 0.12626235787542156,
        "min": 0.7422680412371134,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9920141935348511,
        "bertscore_f1_std": 0.009780575512116942,
        "bertscore_f1_min": 0.9800354838371277,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.990880012512207,
        "bertscore_recall_mean": 0.9931549310684205
      },
      "avg_output_length_chars": 424.0,
      "avg_output_length_words": 50.6,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C1",
      "abstract": "abs_028",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C1_abs_029": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 557.0,
      "avg_output_length_words": 73.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C1",
      "abstract": "abs_029",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C1_abs_030": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0000001192092896,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0000001192092896,
        "bertscore_f1_max": 1.0000001192092896,
        "bertscore_precision_mean": 1.0000001192092896,
        "bertscore_recall_mean": 1.0000001192092896
      },
      "avg_output_length_chars": 338.0,
      "avg_output_length_words": 41.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C1",
      "abstract": "abs_030",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C2_abs_001": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 529.0,
      "avg_output_length_words": 63.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_001",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C2_abs_002": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 569.0,
      "avg_output_length_words": 71.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_002",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C2_abs_003": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 426.0,
      "avg_output_length_words": 53.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_003",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C2_abs_004": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 339.0,
      "avg_output_length_words": 42.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_004",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C2_abs_005": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 365.0,
      "avg_output_length_words": 42.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_005",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C2_abs_006": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 344.0,
      "avg_output_length_words": 45.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_006",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C2_abs_007": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 333.0,
      "avg_output_length_words": 43.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_007",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C2_abs_008": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 408.0,
      "avg_output_length_words": 47.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_008",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C2_abs_009": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 430.0,
      "avg_output_length_words": 55.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_009",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C2_abs_010": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 505.0,
      "avg_output_length_words": 65.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_010",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C2_abs_011": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9999998807907104,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9999998807907104,
        "bertscore_f1_max": 0.9999998807907104,
        "bertscore_precision_mean": 0.9999998807907104,
        "bertscore_recall_mean": 0.9999998807907104
      },
      "avg_output_length_chars": 475.0,
      "avg_output_length_words": 57.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_011",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C2_abs_012": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 486.0,
      "avg_output_length_words": 63.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_012",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C2_abs_013": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0000001192092896,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0000001192092896,
        "bertscore_f1_max": 1.0000001192092896,
        "bertscore_precision_mean": 1.0000001192092896,
        "bertscore_recall_mean": 1.0000001192092896
      },
      "avg_output_length_chars": 454.0,
      "avg_output_length_words": 57.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_013",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C2_abs_014": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9999998807907104,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9999998807907104,
        "bertscore_f1_max": 0.9999998807907104,
        "bertscore_precision_mean": 0.9999998807907104,
        "bertscore_recall_mean": 0.9999998807907104
      },
      "avg_output_length_chars": 449.0,
      "avg_output_length_words": 54.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_014",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C2_abs_015": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 476.0,
      "avg_output_length_words": 62.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_015",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C2_abs_016": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9999998807907104,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9999998807907104,
        "bertscore_f1_max": 0.9999998807907104,
        "bertscore_precision_mean": 0.9999998807907104,
        "bertscore_recall_mean": 0.9999998807907104
      },
      "avg_output_length_chars": 479.0,
      "avg_output_length_words": 58.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_016",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C2_abs_017": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 464.0,
      "avg_output_length_words": 59.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_017",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C2_abs_018": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 445.0,
      "avg_output_length_words": 48.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_018",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C2_abs_019": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 668.0,
      "avg_output_length_words": 84.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_019",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C2_abs_020": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9999999403953552,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9999999403953552,
        "bertscore_f1_max": 0.9999999403953552,
        "bertscore_precision_mean": 0.9999999403953552,
        "bertscore_recall_mean": 0.9999999403953552
      },
      "avg_output_length_chars": 381.0,
      "avg_output_length_words": 50.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_020",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C2_abs_021": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9999999403953552,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9999999403953552,
        "bertscore_f1_max": 0.9999999403953552,
        "bertscore_precision_mean": 0.9999999403953552,
        "bertscore_recall_mean": 0.9999999403953552
      },
      "avg_output_length_chars": 503.0,
      "avg_output_length_words": 62.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_021",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C2_abs_022": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 597.0,
      "avg_output_length_words": 73.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_022",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C2_abs_023": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9999998807907104,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9999998807907104,
        "bertscore_f1_max": 0.9999998807907104,
        "bertscore_precision_mean": 0.9999998807907104,
        "bertscore_recall_mean": 0.9999998807907104
      },
      "avg_output_length_chars": 482.0,
      "avg_output_length_words": 68.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_023",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C2_abs_024": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 567.0,
      "avg_output_length_words": 68.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_024",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C2_abs_025": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0000001192092896,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0000001192092896,
        "bertscore_f1_max": 1.0000001192092896,
        "bertscore_precision_mean": 1.0000001192092896,
        "bertscore_recall_mean": 1.0000001192092896
      },
      "avg_output_length_chars": 340.0,
      "avg_output_length_words": 42.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_025",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C2_abs_026": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 622.0,
      "avg_output_length_words": 80.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_026",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C2_abs_027": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9999998807907104,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9999998807907104,
        "bertscore_f1_max": 0.9999998807907104,
        "bertscore_precision_mean": 0.9999998807907104,
        "bertscore_recall_mean": 0.9999998807907104
      },
      "avg_output_length_chars": 433.0,
      "avg_output_length_words": 54.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_027",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C2_abs_028": {
      "n_outputs": 5,
      "exact_match_rate": 0.6,
      "edit_distance": {
        "mean": 40.4,
        "std": 49.4796928042202,
        "min": 0,
        "max": 101,
        "normalized_mean": 0.09351851851851853
      },
      "rouge_l": {
        "mean": 0.8969072164948454,
        "std": 0.12626235787542156,
        "min": 0.7422680412371134,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9920141935348511,
        "bertscore_f1_std": 0.009780575512116942,
        "bertscore_f1_min": 0.9800354838371277,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.990880012512207,
        "bertscore_recall_mean": 0.9931549310684205
      },
      "avg_output_length_chars": 424.0,
      "avg_output_length_words": 50.6,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_028",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C2_abs_029": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 557.0,
      "avg_output_length_words": 73.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_029",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C2_abs_030": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0000001192092896,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0000001192092896,
        "bertscore_f1_max": 1.0000001192092896,
        "bertscore_precision_mean": 1.0000001192092896,
        "bertscore_recall_mean": 1.0000001192092896
      },
      "avg_output_length_chars": 338.0,
      "avg_output_length_words": 41.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C2",
      "abstract": "abs_030",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 5,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.0_abs_001": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 529.0,
      "avg_output_length_words": 63.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_001",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.0_abs_002": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 569.0,
      "avg_output_length_words": 71.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_002",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.0_abs_003": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 426.0,
      "avg_output_length_words": 53.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_003",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.0_abs_004": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 339.0,
      "avg_output_length_words": 42.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_004",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.0_abs_005": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 365.0,
      "avg_output_length_words": 42.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_005",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.0_abs_006": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 344.0,
      "avg_output_length_words": 45.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_006",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.0_abs_007": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 333.0,
      "avg_output_length_words": 43.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_007",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.0_abs_008": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 408.0,
      "avg_output_length_words": 47.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_008",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.0_abs_009": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 430.0,
      "avg_output_length_words": 55.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_009",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.0_abs_010": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 505.0,
      "avg_output_length_words": 65.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_010",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.0_abs_011": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9999998807907104,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9999998807907104,
        "bertscore_f1_max": 0.9999998807907104,
        "bertscore_precision_mean": 0.9999998807907104,
        "bertscore_recall_mean": 0.9999998807907104
      },
      "avg_output_length_chars": 475.0,
      "avg_output_length_words": 57.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_011",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.0_abs_012": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 486.0,
      "avg_output_length_words": 63.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_012",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.0_abs_013": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0000001192092896,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0000001192092896,
        "bertscore_f1_max": 1.0000001192092896,
        "bertscore_precision_mean": 1.0000001192092896,
        "bertscore_recall_mean": 1.0000001192092896
      },
      "avg_output_length_chars": 454.0,
      "avg_output_length_words": 57.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_013",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.0_abs_014": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9999998807907104,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9999998807907104,
        "bertscore_f1_max": 0.9999998807907104,
        "bertscore_precision_mean": 0.9999998807907104,
        "bertscore_recall_mean": 0.9999998807907104
      },
      "avg_output_length_chars": 449.0,
      "avg_output_length_words": 54.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_014",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.0_abs_015": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 476.0,
      "avg_output_length_words": 62.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_015",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.0_abs_016": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9999998807907104,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9999998807907104,
        "bertscore_f1_max": 0.9999998807907104,
        "bertscore_precision_mean": 0.9999998807907104,
        "bertscore_recall_mean": 0.9999998807907104
      },
      "avg_output_length_chars": 479.0,
      "avg_output_length_words": 58.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_016",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.0_abs_017": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 464.0,
      "avg_output_length_words": 59.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_017",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.0_abs_018": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 445.0,
      "avg_output_length_words": 48.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_018",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.0_abs_019": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 668.0,
      "avg_output_length_words": 84.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_019",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.0_abs_020": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9999999403953552,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9999999403953552,
        "bertscore_f1_max": 0.9999999403953552,
        "bertscore_precision_mean": 0.9999999403953552,
        "bertscore_recall_mean": 0.9999999403953552
      },
      "avg_output_length_chars": 381.0,
      "avg_output_length_words": 50.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_020",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.0_abs_021": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9999999403953552,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9999999403953552,
        "bertscore_f1_max": 0.9999999403953552,
        "bertscore_precision_mean": 0.9999999403953552,
        "bertscore_recall_mean": 0.9999999403953552
      },
      "avg_output_length_chars": 503.0,
      "avg_output_length_words": 62.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_021",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.0_abs_022": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 597.0,
      "avg_output_length_words": 73.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_022",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.0_abs_023": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9999998807907104,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9999998807907104,
        "bertscore_f1_max": 0.9999998807907104,
        "bertscore_precision_mean": 0.9999998807907104,
        "bertscore_recall_mean": 0.9999998807907104
      },
      "avg_output_length_chars": 482.0,
      "avg_output_length_words": 68.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_023",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.0_abs_024": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 567.0,
      "avg_output_length_words": 68.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_024",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.0_abs_025": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0000001192092896,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0000001192092896,
        "bertscore_f1_max": 1.0000001192092896,
        "bertscore_precision_mean": 1.0000001192092896,
        "bertscore_recall_mean": 1.0000001192092896
      },
      "avg_output_length_chars": 340.0,
      "avg_output_length_words": 42.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_025",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.0_abs_026": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 622.0,
      "avg_output_length_words": 80.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_026",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.0_abs_027": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9999998807907104,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9999998807907104,
        "bertscore_f1_max": 0.9999998807907104,
        "bertscore_precision_mean": 0.9999998807907104,
        "bertscore_recall_mean": 0.9999998807907104
      },
      "avg_output_length_chars": 433.0,
      "avg_output_length_words": 54.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_027",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.0_abs_028": {
      "n_outputs": 3,
      "exact_match_rate": 0.3333333333333333,
      "edit_distance": {
        "mean": 67.33333333333333,
        "std": 47.6118565998942,
        "min": 0,
        "max": 101,
        "normalized_mean": 0.1558641975308642
      },
      "rouge_l": {
        "mean": 0.8281786941580757,
        "std": 0.12149601051315248,
        "min": 0.7422680412371134,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9866903225580851,
        "bertscore_f1_std": 0.00941136317458363,
        "bertscore_f1_min": 0.9800354838371277,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9848000208536783,
        "bertscore_recall_mean": 0.9885915517807007
      },
      "avg_output_length_chars": 418.6666666666667,
      "avg_output_length_words": 49.666666666666664,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_028",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.0_abs_029": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 557.0,
      "avg_output_length_words": 73.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_029",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.0_abs_030": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0000001192092896,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0000001192092896,
        "bertscore_f1_max": 1.0000001192092896,
        "bertscore_precision_mean": 1.0000001192092896,
        "bertscore_recall_mean": 1.0000001192092896
      },
      "avg_output_length_chars": 338.0,
      "avg_output_length_words": 41.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.0",
      "abstract": "abs_030",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.3_abs_001": {
      "n_outputs": 3,
      "exact_match_rate": 0.3333333333333333,
      "edit_distance": {
        "mean": 62.666666666666664,
        "std": 44.31202495435698,
        "min": 0,
        "max": 94,
        "normalized_mean": 0.13305024769992924
      },
      "rouge_l": {
        "mean": 0.903030303030303,
        "std": 0.0685679302968773,
        "min": 0.8545454545454546,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9865075349807739,
        "bertscore_f1_std": 0.009540613510017039,
        "bertscore_f1_min": 0.9797613024711609,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9879358609517416,
        "bertscore_recall_mean": 0.9850854078928629
      },
      "avg_output_length_chars": 454.3333333333333,
      "avg_output_length_words": 55.333333333333336,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_001",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.3_abs_002": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 186.33333333333334,
        "std": 95.68815089770635,
        "min": 54,
        "max": 277,
        "normalized_mean": 0.2840082594906413
      },
      "rouge_l": {
        "mean": 0.7536340304882234,
        "std": 0.11621811146505623,
        "min": 0.6357615894039734,
        "max": 0.9117647058823529
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9710322419802347,
        "bertscore_f1_std": 0.011226497116065699,
        "bertscore_f1_min": 0.9584444165229797,
        "bertscore_f1_max": 0.9857053756713867,
        "bertscore_precision_mean": 0.963713010152181,
        "bertscore_recall_mean": 0.9784926176071167
      },
      "avg_output_length_chars": 585.6666666666666,
      "avg_output_length_words": 74.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_002",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.3_abs_003": {
      "n_outputs": 3,
      "exact_match_rate": 0.3333333333333333,
      "edit_distance": {
        "mean": 29.333333333333332,
        "std": 20.741798914805393,
        "min": 0,
        "max": 44,
        "normalized_mean": 0.062411347517730496
      },
      "rouge_l": {
        "mean": 0.9523809523809522,
        "std": 0.03367175148507373,
        "min": 0.9285714285714285,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9930144349733988,
        "bertscore_f1_std": 0.004939498253880751,
        "bertscore_f1_min": 0.9895216822624207,
        "bertscore_f1_max": 0.9999999403953552,
        "bertscore_precision_mean": 0.9975089033444723,
        "bertscore_recall_mean": 0.9885803659756979
      },
      "avg_output_length_chars": 440.6666666666667,
      "avg_output_length_words": 55.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_003",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.3_abs_004": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 133.66666666666666,
        "std": 82.75398614098431,
        "min": 17,
        "max": 200,
        "normalized_mean": 0.26603925048059657
      },
      "rouge_l": {
        "mean": 0.8057729732801113,
        "std": 0.10938913594809221,
        "min": 0.7128712871287128,
        "max": 0.959349593495935
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9715081453323364,
        "bertscore_f1_std": 0.014423510591322523,
        "bertscore_f1_min": 0.9583240151405334,
        "bertscore_f1_max": 0.9915794730186462,
        "bertscore_precision_mean": 0.9582603971163431,
        "bertscore_recall_mean": 0.9852678378423055
      },
      "avg_output_length_chars": 443.3333333333333,
      "avg_output_length_words": 54.333333333333336,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_004",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.3_abs_005": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 90.66666666666667,
        "std": 35.31131389355102,
        "min": 45,
        "max": 131,
        "normalized_mean": 0.19588361146769362
      },
      "rouge_l": {
        "mean": 0.8143097643097642,
        "std": 0.06967053289337216,
        "min": 0.75,
        "max": 0.9111111111111111
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9812541802724203,
        "bertscore_f1_std": 0.0054575879187275635,
        "bertscore_f1_min": 0.9757859110832214,
        "bertscore_f1_max": 0.9887054562568665,
        "bertscore_precision_mean": 0.9841821789741516,
        "bertscore_recall_mean": 0.9784537156422933
      },
      "avg_output_length_chars": 416.6666666666667,
      "avg_output_length_words": 50.666666666666664,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_005",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.3_abs_006": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 97.0,
        "std": 34.215006454283575,
        "min": 61,
        "max": 143,
        "normalized_mean": 0.25487017749743285
      },
      "rouge_l": {
        "mean": 0.7390873015873015,
        "std": 0.08803131171221833,
        "min": 0.6458333333333333,
        "max": 0.8571428571428571
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9676581621170044,
        "bertscore_f1_std": 0.010410403924615313,
        "bertscore_f1_min": 0.9577940106391907,
        "bertscore_f1_max": 0.9820553660392761,
        "bertscore_precision_mean": 0.9729105432828268,
        "bertscore_recall_mean": 0.9624811212221781
      },
      "avg_output_length_chars": 359.0,
      "avg_output_length_words": 46.333333333333336,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_006",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.3_abs_007": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 333.0,
      "avg_output_length_words": 43.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_007",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.3_abs_008": {
      "n_outputs": 3,
      "exact_match_rate": 0.3333333333333333,
      "edit_distance": {
        "mean": 9.333333333333334,
        "std": 6.599663291074444,
        "min": 0,
        "max": 14,
        "normalized_mean": 0.022875816993464054
      },
      "rouge_l": {
        "mean": 0.9633699633699634,
        "std": 0.025901347296210544,
        "min": 0.945054945054945,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.996385375658671,
        "bertscore_f1_std": 0.0025559253831956502,
        "bertscore_f1_min": 0.9945780634880066,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9963879982630411,
        "bertscore_recall_mean": 0.9963879982630411
      },
      "avg_output_length_chars": 403.3333333333333,
      "avg_output_length_words": 46.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_008",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.3_abs_009": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 430.0,
      "avg_output_length_words": 55.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_009",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.3_abs_010": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 211.66666666666666,
        "std": 83.49184923625113,
        "min": 103,
        "max": 306,
        "normalized_mean": 0.3081579327232668
      },
      "rouge_l": {
        "mean": 0.7080293620567543,
        "std": 0.0922057535081809,
        "min": 0.6024096385542169,
        "max": 0.8270676691729324
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9591375192006429,
        "bertscore_f1_std": 0.013195845780036356,
        "bertscore_f1_min": 0.9443575143814087,
        "bertscore_f1_max": 0.9763945937156677,
        "bertscore_precision_mean": 0.9478124181429545,
        "bertscore_recall_mean": 0.970811148484548
      },
      "avg_output_length_chars": 591.6666666666666,
      "avg_output_length_words": 77.66666666666667,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_010",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.3_abs_011": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 29.333333333333332,
        "std": 11.145502331533658,
        "min": 17,
        "max": 44,
        "normalized_mean": 0.060327915904280126
      },
      "rouge_l": {
        "mean": 0.9386251899526236,
        "std": 0.02654014157424935,
        "min": 0.9090909090909092,
        "max": 0.9734513274336283
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9888753890991211,
        "bertscore_f1_std": 0.004355692581130391,
        "bertscore_f1_min": 0.9839179515838623,
        "bertscore_f1_max": 0.9945204854011536,
        "bertscore_precision_mean": 0.9879422783851624,
        "bertscore_recall_mean": 0.9898175994555155
      },
      "avg_output_length_chars": 475.3333333333333,
      "avg_output_length_words": 59.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_011",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.3_abs_012": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 92.0,
        "std": 52.839379254491625,
        "min": 18,
        "max": 138,
        "normalized_mean": 0.15425987043148762
      },
      "rouge_l": {
        "mean": 0.8867249197785535,
        "std": 0.04571504293391965,
        "min": 0.832116788321168,
        "max": 0.944
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9846938451131185,
        "bertscore_f1_std": 0.006164927683030662,
        "bertscore_f1_min": 0.9775938987731934,
        "bertscore_f1_max": 0.992625892162323,
        "bertscore_precision_mean": 0.9791747530301412,
        "bertscore_recall_mean": 0.9903076688448588
      },
      "avg_output_length_chars": 521.3333333333334,
      "avg_output_length_words": 66.66666666666667,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_012",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.3_abs_013": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0000001192092896,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0000001192092896,
        "bertscore_f1_max": 1.0000001192092896,
        "bertscore_precision_mean": 1.0000001192092896,
        "bertscore_recall_mean": 1.0000001192092896
      },
      "avg_output_length_chars": 454.0,
      "avg_output_length_words": 57.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_013",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.3_abs_014": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 52.666666666666664,
        "std": 19.362047641943473,
        "min": 33,
        "max": 79,
        "normalized_mean": 0.12185179132392732
      },
      "rouge_l": {
        "mean": 0.8672361271933732,
        "std": 0.05491738610774725,
        "min": 0.8073394495412844,
        "max": 0.9400000000000001
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9830081264177958,
        "bertscore_f1_std": 0.004646479085615776,
        "bertscore_f1_min": 0.9766581654548645,
        "bertscore_f1_max": 0.9876470565795898,
        "bertscore_precision_mean": 0.9851584037144979,
        "bertscore_recall_mean": 0.9809252421061198
      },
      "avg_output_length_chars": 415.3333333333333,
      "avg_output_length_words": 52.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_014",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.3_abs_015": {
      "n_outputs": 3,
      "exact_match_rate": 0.3333333333333333,
      "edit_distance": {
        "mean": 6.666666666666667,
        "std": 4.714045207910317,
        "min": 0,
        "max": 10,
        "normalized_mean": 0.013774104683195593
      },
      "rouge_l": {
        "mean": 0.9733333333333333,
        "std": 0.018856180831641284,
        "min": 0.96,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9959033330281576,
        "bertscore_f1_std": 0.002896780996052754,
        "bertscore_f1_min": 0.9938549995422363,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.994553009668986,
        "bertscore_recall_mean": 0.9972591002782186
      },
      "avg_output_length_chars": 478.6666666666667,
      "avg_output_length_words": 62.333333333333336,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_015",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.3_abs_016": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 10.666666666666666,
        "std": 4.109609335312651,
        "min": 6,
        "max": 16,
        "normalized_mean": 0.022204957486794647
      },
      "rouge_l": {
        "mean": 0.9655172413793104,
        "std": 0.014077527257374569,
        "min": 0.9482758620689655,
        "max": 0.9827586206896551
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9918701450030009,
        "bertscore_f1_std": 0.003126492903403398,
        "bertscore_f1_min": 0.9877261519432068,
        "bertscore_f1_max": 0.9952774047851562,
        "bertscore_precision_mean": 0.9926963249842325,
        "bertscore_recall_mean": 0.9910520911216736
      },
      "avg_output_length_chars": 477.3333333333333,
      "avg_output_length_words": 58.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_016",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.3_abs_017": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 7.333333333333333,
        "std": 2.8674417556808756,
        "min": 4,
        "max": 11,
        "normalized_mean": 0.015639520665199313
      },
      "rouge_l": {
        "mean": 0.9774482007782108,
        "std": 0.010518932697495306,
        "min": 0.9661016949152542,
        "max": 0.9914529914529915
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9967639247576395,
        "bertscore_f1_std": 0.001325405674201889,
        "bertscore_f1_min": 0.9950149059295654,
        "bertscore_f1_max": 0.9982221722602844,
        "bertscore_precision_mean": 0.996192455291748,
        "bertscore_recall_mean": 0.9973363081614176
      },
      "avg_output_length_chars": 464.6666666666667,
      "avg_output_length_words": 59.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_017",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.3_abs_018": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 42.666666666666664,
        "std": 7.586537784494029,
        "min": 35,
        "max": 53,
        "normalized_mean": 0.08639368470445356
      },
      "rouge_l": {
        "mean": 0.9062656641604009,
        "std": 0.030086324866425717,
        "min": 0.8761904761904762,
        "max": 0.9473684210526315
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9860116839408875,
        "bertscore_f1_std": 0.002672318354357434,
        "bertscore_f1_min": 0.9823139905929565,
        "bertscore_f1_max": 0.9885367155075073,
        "bertscore_precision_mean": 0.9797060688336691,
        "bertscore_recall_mean": 0.9924144148826599
      },
      "avg_output_length_chars": 476.0,
      "avg_output_length_words": 54.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_018",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.3_abs_019": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 139.0,
        "std": 86.28248180637055,
        "min": 17,
        "max": 202,
        "normalized_mean": 0.23667225013958682
      },
      "rouge_l": {
        "mean": 0.8023848238482384,
        "std": 0.111648275212744,
        "min": 0.7154471544715447,
        "max": 0.96
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9765787323315939,
        "bertscore_f1_std": 0.012816370922486416,
        "bertscore_f1_min": 0.966841995716095,
        "bertscore_f1_max": 0.9946866631507874,
        "bertscore_precision_mean": 0.9672947923342387,
        "bertscore_recall_mean": 0.986120879650116
      },
      "avg_output_length_chars": 482.0,
      "avg_output_length_words": 57.666666666666664,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_019",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.3_abs_020": {
      "n_outputs": 3,
      "exact_match_rate": 0.3333333333333333,
      "edit_distance": {
        "mean": 9.333333333333334,
        "std": 6.599663291074444,
        "min": 0,
        "max": 14,
        "normalized_mean": 0.023628691983122365
      },
      "rouge_l": {
        "mean": 0.9869281045751634,
        "std": 0.009243225897863385,
        "min": 0.9803921568627451,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9968759814898173,
        "bertscore_f1_std": 0.0022089725262539705,
        "bertscore_f1_min": 0.9953140020370483,
        "bertscore_f1_max": 0.9999999403953552,
        "bertscore_precision_mean": 0.9958667556444804,
        "bertscore_recall_mean": 0.9978883067766825
      },
      "avg_output_length_chars": 385.6666666666667,
      "avg_output_length_words": 50.666666666666664,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_020",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.3_abs_021": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 159.33333333333334,
        "std": 49.88208317845419,
        "min": 111,
        "max": 228,
        "normalized_mean": 0.2971918481876826
      },
      "rouge_l": {
        "mean": 0.695659460550937,
        "std": 0.08130804004363848,
        "min": 0.6229508196721311,
        "max": 0.8091603053435115
      },
      "bert_score": {
        "bertscore_f1_mean": 0.964078684647878,
        "bertscore_f1_std": 0.010196016004925198,
        "bertscore_f1_min": 0.9540461301803589,
        "bertscore_f1_max": 0.9780642986297607,
        "bertscore_precision_mean": 0.9709862470626831,
        "bertscore_recall_mean": 0.9572938879330953
      },
      "avg_output_length_chars": 487.6666666666667,
      "avg_output_length_words": 61.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_021",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.3_abs_022": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 35.333333333333336,
        "std": 13.912424503139471,
        "min": 19,
        "max": 53,
        "normalized_mean": 0.05945823044201534
      },
      "rouge_l": {
        "mean": 0.9581003424968167,
        "std": 0.014569236495993995,
        "min": 0.9379310344827586,
        "max": 0.971830985915493
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9936749935150146,
        "bertscore_f1_std": 0.0014891922280072195,
        "bertscore_f1_min": 0.9918966293334961,
        "bertscore_f1_max": 0.9955412149429321,
        "bertscore_precision_mean": 0.9936008254686991,
        "bertscore_recall_mean": 0.993753969669342
      },
      "avg_output_length_chars": 580.6666666666666,
      "avg_output_length_words": 71.33333333333333,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_022",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.3_abs_023": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 28.0,
        "std": 13.92838827718412,
        "min": 9,
        "max": 42,
        "normalized_mean": 0.058160790089231564
      },
      "rouge_l": {
        "mean": 0.935430351477823,
        "std": 0.03569281169106616,
        "min": 0.9037037037037037,
        "max": 0.9852941176470589
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9932932456334432,
        "bertscore_f1_std": 0.0032697417805407445,
        "bertscore_f1_min": 0.9904245734214783,
        "bertscore_f1_max": 0.9978684186935425,
        "bertscore_precision_mean": 0.9925257762273153,
        "bertscore_recall_mean": 0.9940620263417562
      },
      "avg_output_length_chars": 475.3333333333333,
      "avg_output_length_words": 67.33333333333333,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_023",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.3_abs_024": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 44.0,
        "std": 29.709706606876257,
        "min": 2,
        "max": 66,
        "normalized_mean": 0.07774576399438886
      },
      "rouge_l": {
        "mean": 0.9370774884757053,
        "std": 0.03866077519632291,
        "min": 0.90625,
        "max": 0.9915966386554621
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9916136662165324,
        "bertscore_f1_std": 0.005000092953682832,
        "bertscore_f1_min": 0.9873822331428528,
        "bertscore_f1_max": 0.9986357688903809,
        "bertscore_precision_mean": 0.9919469157854716,
        "bertscore_recall_mean": 0.9912958343823751
      },
      "avg_output_length_chars": 525.0,
      "avg_output_length_words": 62.333333333333336,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_024",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.3_abs_025": {
      "n_outputs": 3,
      "exact_match_rate": 0.3333333333333333,
      "edit_distance": {
        "mean": 38.0,
        "std": 26.870057685088806,
        "min": 0,
        "max": 57,
        "normalized_mean": 0.09571788413098237
      },
      "rouge_l": {
        "mean": 0.9194139194139194,
        "std": 0.05698296405166318,
        "min": 0.8791208791208791,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9894271294275919,
        "bertscore_f1_std": 0.007476232772054442,
        "bertscore_f1_min": 0.9841406345367432,
        "bertscore_f1_max": 1.0000001192092896,
        "bertscore_precision_mean": 0.9869586626688639,
        "bertscore_recall_mean": 0.9919142325719198
      },
      "avg_output_length_chars": 359.0,
      "avg_output_length_words": 44.333333333333336,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_025",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.3_abs_026": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 89.33333333333333,
        "std": 38.029228525204424,
        "min": 36,
        "max": 122,
        "normalized_mean": 0.16158207638242758
      },
      "rouge_l": {
        "mean": 0.8609022556390977,
        "std": 0.04939950712888976,
        "min": 0.81203007518797,
        "max": 0.9285714285714286
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9843543767929077,
        "bertscore_f1_std": 0.004859131313385274,
        "bertscore_f1_min": 0.9805445075035095,
        "bertscore_f1_max": 0.9912121295928955,
        "bertscore_precision_mean": 0.983969489733378,
        "bertscore_recall_mean": 0.9847412705421448
      },
      "avg_output_length_chars": 539.3333333333334,
      "avg_output_length_words": 67.66666666666667,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_026",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.3_abs_027": {
      "n_outputs": 3,
      "exact_match_rate": 0.3333333333333333,
      "edit_distance": {
        "mean": 10.666666666666666,
        "std": 7.542472332656507,
        "min": 0,
        "max": 16,
        "normalized_mean": 0.024634334103156273
      },
      "rouge_l": {
        "mean": 0.9813084112149534,
        "std": 0.013216949181056965,
        "min": 0.9719626168224299,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9954273700714111,
        "bertscore_f1_std": 0.003233253336664725,
        "bertscore_f1_min": 0.9931411147117615,
        "bertscore_f1_max": 0.9999998807907104,
        "bertscore_precision_mean": 0.9939512411753336,
        "bertscore_recall_mean": 0.9969102144241333
      },
      "avg_output_length_chars": 428.0,
      "avg_output_length_words": 53.666666666666664,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_027",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.3_abs_028": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 175.66666666666666,
        "std": 17.98765008430939,
        "min": 161,
        "max": 201,
        "normalized_mean": 0.37032164016506536
      },
      "rouge_l": {
        "mean": 0.5448765592141273,
        "std": 0.04928474902007395,
        "min": 0.47619047619047616,
        "max": 0.5894736842105263
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9505470395088196,
        "bertscore_f1_std": 0.006422101461002346,
        "bertscore_f1_min": 0.9438450336456299,
        "bertscore_f1_max": 0.9592062830924988,
        "bertscore_precision_mean": 0.9455326000849406,
        "bertscore_recall_mean": 0.9557008743286133
      },
      "avg_output_length_chars": 431.6666666666667,
      "avg_output_length_words": 52.666666666666664,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_028",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.3_abs_029": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 155.33333333333334,
        "std": 40.43375927228247,
        "min": 99,
        "max": 192,
        "normalized_mean": 0.2644324659819572
      },
      "rouge_l": {
        "mean": 0.7366557734204792,
        "std": 0.07084903173087746,
        "min": 0.6805555555555556,
        "max": 0.8366013071895425
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9631119171778361,
        "bertscore_f1_std": 0.01491430544528306,
        "bertscore_f1_min": 0.9517710208892822,
        "bertscore_f1_max": 0.9841834306716919,
        "bertscore_precision_mean": 0.9662490288416544,
        "bertscore_recall_mean": 0.9600143035252889
      },
      "avg_output_length_chars": 570.6666666666666,
      "avg_output_length_words": 75.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_029",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.3_abs_030": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0000001192092896,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0000001192092896,
        "bertscore_f1_max": 1.0000001192092896,
        "bertscore_precision_mean": 1.0000001192092896,
        "bertscore_recall_mean": 1.0000001192092896
      },
      "avg_output_length_chars": 338.0,
      "avg_output_length_words": 41.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.3",
      "abstract": "abs_030",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.7_abs_001": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 206.33333333333334,
        "std": 59.78479925718763,
        "min": 125,
        "max": 267,
        "normalized_mean": 0.37762124524330015
      },
      "rouge_l": {
        "mean": 0.6499216555801922,
        "std": 0.10210752378657263,
        "min": 0.544,
        "max": 0.787878787878788
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9493987758954366,
        "bertscore_f1_std": 0.017220692931394492,
        "bertscore_f1_min": 0.9310142397880554,
        "bertscore_f1_max": 0.9724234342575073,
        "bertscore_precision_mean": 0.9534786144892374,
        "bertscore_recall_mean": 0.9453581174214681
      },
      "avg_output_length_chars": 515.3333333333334,
      "avg_output_length_words": 63.333333333333336,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_001",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.7_abs_002": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 211.66666666666666,
        "std": 38.96437118987322,
        "min": 173,
        "max": 265,
        "normalized_mean": 0.33304048555716237
      },
      "rouge_l": {
        "mean": 0.6592783224295953,
        "std": 0.07178103627428536,
        "min": 0.6056338028169013,
        "max": 0.7607361963190185
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9498536984125773,
        "bertscore_f1_std": 0.011016704404797115,
        "bertscore_f1_min": 0.9418807029724121,
        "bertscore_f1_max": 0.9654322266578674,
        "bertscore_precision_mean": 0.953819195429484,
        "bertscore_recall_mean": 0.946020225683848
      },
      "avg_output_length_chars": 588.0,
      "avg_output_length_words": 77.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_002",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.7_abs_003": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 131.33333333333334,
        "std": 64.31346843564124,
        "min": 44,
        "max": 197,
        "normalized_mean": 0.21847159136186148
      },
      "rouge_l": {
        "mean": 0.847292803814543,
        "std": 0.06379994486756835,
        "min": 0.7727272727272728,
        "max": 0.9285714285714285
      },
      "bert_score": {
        "bertscore_f1_mean": 0.975163996219635,
        "bertscore_f1_std": 0.010862620723066289,
        "bertscore_f1_min": 0.9632536172866821,
        "bertscore_f1_max": 0.9895216822624207,
        "bertscore_precision_mean": 0.9647974173227946,
        "bertscore_recall_mean": 0.9860874017079672
      },
      "avg_output_length_chars": 506.3333333333333,
      "avg_output_length_words": 63.666666666666664,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_003",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.7_abs_004": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 155.66666666666666,
        "std": 71.25696847014723,
        "min": 55,
        "max": 210,
        "normalized_mean": 0.3286035592899938
      },
      "rouge_l": {
        "mean": 0.7903192159535686,
        "std": 0.062271288814495625,
        "min": 0.7238095238095238,
        "max": 0.8735632183908046
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9736709793408712,
        "bertscore_f1_std": 0.011412589999142925,
        "bertscore_f1_min": 0.964309573173523,
        "bertscore_f1_max": 0.9897378087043762,
        "bertscore_precision_mean": 0.9737410744031271,
        "bertscore_recall_mean": 0.9740217924118042
      },
      "avg_output_length_chars": 394.3333333333333,
      "avg_output_length_words": 49.666666666666664,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_004",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.7_abs_005": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 112.66666666666667,
        "std": 36.261396675926434,
        "min": 79,
        "max": 163,
        "normalized_mean": 0.25778979904563715
      },
      "rouge_l": {
        "mean": 0.7767136526736786,
        "std": 0.06036692619760035,
        "min": 0.693069306930693,
        "max": 0.8333333333333333
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9655154148737589,
        "bertscore_f1_std": 0.012655477064651125,
        "bertscore_f1_min": 0.9546791315078735,
        "bertscore_f1_max": 0.9832693934440613,
        "bertscore_precision_mean": 0.9656258424123129,
        "bertscore_recall_mean": 0.9654306173324585
      },
      "avg_output_length_chars": 423.0,
      "avg_output_length_words": 52.666666666666664,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_005",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.7_abs_006": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 73.33333333333333,
        "std": 21.85304453744502,
        "min": 49,
        "max": 102,
        "normalized_mean": 0.19864649477509058
      },
      "rouge_l": {
        "mean": 0.8161860031132105,
        "std": 0.049123040617144044,
        "min": 0.7526881720430109,
        "max": 0.8723404255319148
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9780787626902262,
        "bertscore_f1_std": 0.005174874907285142,
        "bertscore_f1_min": 0.9711639881134033,
        "bertscore_f1_max": 0.9836118221282959,
        "bertscore_precision_mean": 0.9834481279055277,
        "bertscore_recall_mean": 0.9727843205134074
      },
      "avg_output_length_chars": 349.3333333333333,
      "avg_output_length_words": 45.333333333333336,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_006",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.7_abs_007": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 141.33333333333334,
        "std": 58.20271089524572,
        "min": 62,
        "max": 200,
        "normalized_mean": 0.30758392377722604
      },
      "rouge_l": {
        "mean": 0.7027862725110432,
        "std": 0.07721295172945027,
        "min": 0.6111111111111112,
        "max": 0.8
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9525774121284485,
        "bertscore_f1_std": 0.01472859375597582,
        "bertscore_f1_min": 0.9337781071662903,
        "bertscore_f1_max": 0.9697447419166565,
        "bertscore_precision_mean": 0.9568230112393697,
        "bertscore_recall_mean": 0.9495288332303365
      },
      "avg_output_length_chars": 386.6666666666667,
      "avg_output_length_words": 50.333333333333336,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_007",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.7_abs_008": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 40.0,
        "std": 19.252705437591537,
        "min": 14,
        "max": 60,
        "normalized_mean": 0.09937460601838029
      },
      "rouge_l": {
        "mean": 0.9233926992547682,
        "std": 0.020137113957940003,
        "min": 0.896551724137931,
        "max": 0.945054945054945
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9882446924845377,
        "bertscore_f1_std": 0.004579788502154943,
        "bertscore_f1_min": 0.9839041233062744,
        "bertscore_f1_max": 0.9945780634880066,
        "bertscore_precision_mean": 0.9942762653032938,
        "bertscore_recall_mean": 0.9823029239972433
      },
      "avg_output_length_chars": 383.6666666666667,
      "avg_output_length_words": 43.666666666666664,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_008",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.7_abs_009": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 51.666666666666664,
        "std": 11.55662388223981,
        "min": 43,
        "max": 68,
        "normalized_mean": 0.12050120166372762
      },
      "rouge_l": {
        "mean": 0.8337504732134486,
        "std": 0.014994668246144151,
        "min": 0.8148148148148149,
        "max": 0.8514851485148515
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9798867305119833,
        "bertscore_f1_std": 0.0015398734683514377,
        "bertscore_f1_min": 0.9787404537200928,
        "bertscore_f1_max": 0.9820634126663208,
        "bertscore_precision_mean": 0.9860560099283854,
        "bertscore_recall_mean": 0.9738021095593771
      },
      "avg_output_length_chars": 407.0,
      "avg_output_length_words": 52.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_009",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.7_abs_010": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 170.66666666666666,
        "std": 6.018490028422596,
        "min": 165,
        "max": 179,
        "normalized_mean": 0.3270624303232999
      },
      "rouge_l": {
        "mean": 0.6660900762380678,
        "std": 0.032147785173244264,
        "min": 0.6363636363636364,
        "max": 0.7107438016528925
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9611577788988749,
        "bertscore_f1_std": 0.0014277123678883992,
        "bertscore_f1_min": 0.9591490626335144,
        "bertscore_f1_max": 0.9623391628265381,
        "bertscore_precision_mean": 0.9619810581207275,
        "bertscore_recall_mean": 0.9604723652203878
      },
      "avg_output_length_chars": 477.0,
      "avg_output_length_words": 60.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_010",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.7_abs_011": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 141.33333333333334,
        "std": 71.76040381405024,
        "min": 42,
        "max": 209,
        "normalized_mean": 0.32589356346637904
      },
      "rouge_l": {
        "mean": 0.7461549243727462,
        "std": 0.08564957696504345,
        "min": 0.6486486486486487,
        "max": 0.8571428571428571
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9583590825398763,
        "bertscore_f1_std": 0.015550536180181344,
        "bertscore_f1_min": 0.9433018565177917,
        "bertscore_f1_max": 0.9797689318656921,
        "bertscore_precision_mean": 0.9613050023714701,
        "bertscore_recall_mean": 0.9554370045661926
      },
      "avg_output_length_chars": 423.0,
      "avg_output_length_words": 54.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_011",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.7_abs_012": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 147.33333333333334,
        "std": 16.110727964792765,
        "min": 134,
        "max": 170,
        "normalized_mean": 0.2475055197827475
      },
      "rouge_l": {
        "mean": 0.8160442681123947,
        "std": 0.04336231224012354,
        "min": 0.7567567567567568,
        "max": 0.8592592592592593
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9743259946505228,
        "bertscore_f1_std": 0.0032260202535776394,
        "bertscore_f1_min": 0.9699349999427795,
        "bertscore_f1_max": 0.9775939583778381,
        "bertscore_precision_mean": 0.9751851161321005,
        "bertscore_recall_mean": 0.9735761483510336
      },
      "avg_output_length_chars": 550.0,
      "avg_output_length_words": 70.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_012",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.7_abs_013": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 101.33333333333333,
        "std": 36.160137659521645,
        "min": 70,
        "max": 152,
        "normalized_mean": 0.20651461673138286
      },
      "rouge_l": {
        "mean": 0.8345929088321675,
        "std": 0.061472528267991634,
        "min": 0.7522935779816514,
        "max": 0.8999999999999999
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9775388439496359,
        "bertscore_f1_std": 0.009429753716190047,
        "bertscore_f1_min": 0.9667173027992249,
        "bertscore_f1_max": 0.9896987676620483,
        "bertscore_precision_mean": 0.980158785978953,
        "bertscore_recall_mean": 0.975253701210022
      },
      "avg_output_length_chars": 439.0,
      "avg_output_length_words": 55.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_013",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.7_abs_014": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 123.0,
        "std": 52.42772803266099,
        "min": 52,
        "max": 177,
        "normalized_mean": 0.270566027902371
      },
      "rouge_l": {
        "mean": 0.7514493967553221,
        "std": 0.0703956982134527,
        "min": 0.6734693877551021,
        "max": 0.8440366972477065
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9713663458824158,
        "bertscore_f1_std": 0.007094254463085484,
        "bertscore_f1_min": 0.9653246998786926,
        "bertscore_f1_max": 0.981323778629303,
        "bertscore_precision_mean": 0.9716500242551168,
        "bertscore_recall_mean": 0.9711862603823344
      },
      "avg_output_length_chars": 415.3333333333333,
      "avg_output_length_words": 50.333333333333336,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_014",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.7_abs_015": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 151.66666666666666,
        "std": 66.13790306792484,
        "min": 59,
        "max": 209,
        "normalized_mean": 0.33055270611157345
      },
      "rouge_l": {
        "mean": 0.7431138975966561,
        "std": 0.09134864848260654,
        "min": 0.64,
        "max": 0.8620689655172413
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9639391104380289,
        "bertscore_f1_std": 0.009939441848118513,
        "bertscore_f1_min": 0.9541571140289307,
        "bertscore_f1_max": 0.9775721430778503,
        "bertscore_precision_mean": 0.9603567918141683,
        "bertscore_recall_mean": 0.9677077134450277
      },
      "avg_output_length_chars": 418.6666666666667,
      "avg_output_length_words": 54.333333333333336,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_015",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.7_abs_016": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 159.33333333333334,
        "std": 99.53335565974298,
        "min": 19,
        "max": 239,
        "normalized_mean": 0.24091672964832456
      },
      "rouge_l": {
        "mean": 0.8021758348729563,
        "std": 0.1222115622358011,
        "min": 0.7083333333333334,
        "max": 0.9747899159663865
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9651715556780497,
        "bertscore_f1_std": 0.0156860928069693,
        "bertscore_f1_min": 0.9508672952651978,
        "bertscore_f1_max": 0.987007737159729,
        "bertscore_precision_mean": 0.9744899670282999,
        "bertscore_recall_mean": 0.9561781684557596
      },
      "avg_output_length_chars": 544.6666666666666,
      "avg_output_length_words": 67.33333333333333,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_016",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.7_abs_017": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 90.0,
        "std": 38.505410875183074,
        "min": 38,
        "max": 130,
        "normalized_mean": 0.1840009998664951
      },
      "rouge_l": {
        "mean": 0.8655150384640407,
        "std": 0.043287877667058904,
        "min": 0.8253968253968254,
        "max": 0.9256198347107438
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9821199178695679,
        "bertscore_f1_std": 0.008152778445979525,
        "bertscore_f1_min": 0.9758829474449158,
        "bertscore_f1_max": 0.993636429309845,
        "bertscore_precision_mean": 0.9839713176091512,
        "bertscore_recall_mean": 0.9802865982055664
      },
      "avg_output_length_chars": 477.6666666666667,
      "avg_output_length_words": 61.666666666666664,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_017",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.7_abs_018": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 66.33333333333333,
        "std": 16.21384867602041,
        "min": 52,
        "max": 89,
        "normalized_mean": 0.13099335802733808
      },
      "rouge_l": {
        "mean": 0.8468507520915515,
        "std": 0.018798937432673652,
        "min": 0.823529411764706,
        "max": 0.8695652173913043
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9800669550895691,
        "bertscore_f1_std": 0.003419722848225054,
        "bertscore_f1_min": 0.976944625377655,
        "bertscore_f1_max": 0.9848265647888184,
        "bertscore_precision_mean": 0.9818643728892008,
        "bertscore_recall_mean": 0.9782863656679789
      },
      "avg_output_length_chars": 497.0,
      "avg_output_length_words": 58.666666666666664,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_018",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.7_abs_019": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 178.66666666666666,
        "std": 95.72646214895626,
        "min": 44,
        "max": 258,
        "normalized_mean": 0.27065565654038076
      },
      "rouge_l": {
        "mean": 0.7630673805918503,
        "std": 0.11208911299685308,
        "min": 0.6666666666666667,
        "max": 0.9202453987730062
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9667693575223287,
        "bertscore_f1_std": 0.013684067750147275,
        "bertscore_f1_min": 0.9528142213821411,
        "bertscore_f1_max": 0.9853581786155701,
        "bertscore_precision_mean": 0.9680509765942892,
        "bertscore_recall_mean": 0.9658288359642029
      },
      "avg_output_length_chars": 581.6666666666666,
      "avg_output_length_words": 71.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_019",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.7_abs_020": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 97.66666666666667,
        "std": 59.39884024307396,
        "min": 14,
        "max": 146,
        "normalized_mean": 0.19274819813163901
      },
      "rouge_l": {
        "mean": 0.8447390978249224,
        "std": 0.0960643553155049,
        "min": 0.7704918032786885,
        "max": 0.9803921568627451
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9774591525395712,
        "bertscore_f1_std": 0.012629400759127215,
        "bertscore_f1_min": 0.9681388735771179,
        "bertscore_f1_max": 0.9953140616416931,
        "bertscore_precision_mean": 0.976665198802948,
        "bertscore_recall_mean": 0.9785678188006083
      },
      "avg_output_length_chars": 430.0,
      "avg_output_length_words": 57.333333333333336,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_020",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.7_abs_021": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 141.0,
        "std": 23.423634787681152,
        "min": 108,
        "max": 160,
        "normalized_mean": 0.25625528393862534
      },
      "rouge_l": {
        "mean": 0.7532345221553188,
        "std": 0.033365415651596564,
        "min": 0.7244094488188977,
        "max": 0.7999999999999999
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9656586647033691,
        "bertscore_f1_std": 0.012420341138667175,
        "bertscore_f1_min": 0.9538995623588562,
        "bertscore_f1_max": 0.9828382134437561,
        "bertscore_precision_mean": 0.9597514073053995,
        "bertscore_recall_mean": 0.9716485937436422
      },
      "avg_output_length_chars": 532.6666666666666,
      "avg_output_length_words": 66.33333333333333,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_021",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.7_abs_022": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 122.66666666666667,
        "std": 29.261275129806325,
        "min": 90,
        "max": 161,
        "normalized_mean": 0.23235929798213184
      },
      "rouge_l": {
        "mean": 0.8145453982410505,
        "std": 0.04550389087583252,
        "min": 0.765217391304348,
        "max": 0.8750000000000001
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9678055047988892,
        "bertscore_f1_std": 0.008355983447408798,
        "bertscore_f1_min": 0.9565407037734985,
        "bertscore_f1_max": 0.976530134677887,
        "bertscore_precision_mean": 0.9728759328524271,
        "bertscore_recall_mean": 0.9629115462303162
      },
      "avg_output_length_chars": 496.6666666666667,
      "avg_output_length_words": 60.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_022",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.7_abs_023": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 50.333333333333336,
        "std": 12.684198393626966,
        "min": 33,
        "max": 63,
        "normalized_mean": 0.10665341405379357
      },
      "rouge_l": {
        "mean": 0.8651886236259941,
        "std": 0.040379631480409496,
        "min": 0.8188976377952757,
        "max": 0.9172932330827067
      },
      "bert_score": {
        "bertscore_f1_mean": 0.987077514330546,
        "bertscore_f1_std": 0.0032126608547953296,
        "bertscore_f1_min": 0.9843413233757019,
        "bertscore_f1_max": 0.9915867447853088,
        "bertscore_precision_mean": 0.9886943896611532,
        "bertscore_recall_mean": 0.9854715466499329
      },
      "avg_output_length_chars": 460.0,
      "avg_output_length_words": 64.66666666666667,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_023",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.7_abs_024": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 145.66666666666666,
        "std": 26.23398982660133,
        "min": 112,
        "max": 176,
        "normalized_mean": 0.28567435694720617
      },
      "rouge_l": {
        "mean": 0.7526092836308976,
        "std": 0.03123233495587031,
        "min": 0.7272727272727272,
        "max": 0.7966101694915254
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9687211910883585,
        "bertscore_f1_std": 0.006230492079117669,
        "bertscore_f1_min": 0.9628846645355225,
        "bertscore_f1_max": 0.9773560762405396,
        "bertscore_precision_mean": 0.9743581612904867,
        "bertscore_recall_mean": 0.9632601936658224
      },
      "avg_output_length_chars": 465.0,
      "avg_output_length_words": 54.333333333333336,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_024",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.7_abs_025": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 137.66666666666666,
        "std": 32.293790252754306,
        "min": 106,
        "max": 182,
        "normalized_mean": 0.3348305897231912
      },
      "rouge_l": {
        "mean": 0.7006907184705519,
        "std": 0.04433736062919612,
        "min": 0.64,
        "max": 0.7446808510638299
      },
      "bert_score": {
        "bertscore_f1_mean": 0.972813884417216,
        "bertscore_f1_std": 0.008045602805291348,
        "bertscore_f1_min": 0.9623852968215942,
        "bertscore_f1_max": 0.9819691181182861,
        "bertscore_precision_mean": 0.9737344980239868,
        "bertscore_recall_mean": 0.9719307820002238
      },
      "avg_output_length_chars": 384.3333333333333,
      "avg_output_length_words": 47.666666666666664,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_025",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.7_abs_026": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 165.0,
        "std": 12.328828005937952,
        "min": 149,
        "max": 179,
        "normalized_mean": 0.28734099448502864
      },
      "rouge_l": {
        "mean": 0.6433419891552286,
        "std": 0.05547932512815314,
        "min": 0.596774193548387,
        "max": 0.721311475409836
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9630492726961771,
        "bertscore_f1_std": 0.006429499866652176,
        "bertscore_f1_min": 0.9555820226669312,
        "bertscore_f1_max": 0.9712759852409363,
        "bertscore_precision_mean": 0.9633703231811523,
        "bertscore_recall_mean": 0.9627653360366821
      },
      "avg_output_length_chars": 549.6666666666666,
      "avg_output_length_words": 63.333333333333336,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_026",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.7_abs_027": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 113.0,
        "std": 20.607442021431645,
        "min": 90,
        "max": 140,
        "normalized_mean": 0.21728638171357964
      },
      "rouge_l": {
        "mean": 0.8346063122182046,
        "std": 0.041339310744004834,
        "min": 0.7786259541984734,
        "max": 0.8771929824561403
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9736230572064718,
        "bertscore_f1_std": 0.0069805166610431854,
        "bertscore_f1_min": 0.9637613892555237,
        "bertscore_f1_max": 0.9789438247680664,
        "bertscore_precision_mean": 0.9681731263796488,
        "bertscore_recall_mean": 0.9792781472206116
      },
      "avg_output_length_chars": 484.0,
      "avg_output_length_words": 61.666666666666664,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_027",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.7_abs_028": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 204.0,
        "std": 48.194052191807515,
        "min": 136,
        "max": 242,
        "normalized_mean": 0.41361925765006635
      },
      "rouge_l": {
        "mean": 0.568871302053891,
        "std": 0.07301573696819826,
        "min": 0.5172413793103448,
        "max": 0.6721311475409836
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9524614016215006,
        "bertscore_f1_std": 0.008159019729918388,
        "bertscore_f1_min": 0.9426344633102417,
        "bertscore_f1_max": 0.9626120328903198,
        "bertscore_precision_mean": 0.953262984752655,
        "bertscore_recall_mean": 0.9516708254814148
      },
      "avg_output_length_chars": 481.3333333333333,
      "avg_output_length_words": 59.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_028",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.7_abs_029": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 226.66666666666666,
        "std": 49.492984912566705,
        "min": 164,
        "max": 285,
        "normalized_mean": 0.40652826993295904
      },
      "rouge_l": {
        "mean": 0.6229304792661919,
        "std": 0.08321072380443748,
        "min": 0.5210084033613445,
        "max": 0.7248322147651005
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9477067788441976,
        "bertscore_f1_std": 0.011469849937289956,
        "bertscore_f1_min": 0.931492030620575,
        "bertscore_f1_max": 0.9561983942985535,
        "bertscore_precision_mean": 0.9394034345944723,
        "bertscore_recall_mean": 0.9561892747879028
      },
      "avg_output_length_chars": 492.0,
      "avg_output_length_words": 65.0,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_029",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_extraction_C3_t0.7_abs_030": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 28.666666666666668,
        "std": 14.817407180595245,
        "min": 8,
        "max": 42,
        "normalized_mean": 0.08189368951590957
      },
      "rouge_l": {
        "mean": 0.921645899739206,
        "std": 0.01567538250458848,
        "min": 0.9047619047619047,
        "max": 0.942528735632184
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9887279868125916,
        "bertscore_f1_std": 0.0052190176922478895,
        "bertscore_f1_min": 0.9843648076057434,
        "bertscore_f1_max": 0.996065080165863,
        "bertscore_precision_mean": 0.991215705871582,
        "bertscore_recall_mean": 0.9862554470698038
      },
      "avg_output_length_chars": 340.0,
      "avg_output_length_words": 42.666666666666664,
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.7",
      "abstract": "abs_030",
      "json_validity": {
        "json_valid_count": 0,
        "json_valid_total": 3,
        "json_validity_rate": 0.0
      },
      "schema_compliance": {
        "schema_compliant_count": 0,
        "schema_compliant_of_valid": 0.0,
        "schema_compliance_rate": 0.0
      },
      "field_accuracy": {
        "field_accuracy": {
          "objective": null,
          "method": null,
          "key_result": null,
          "model_or_system": null,
          "benchmark": null
        },
        "overall_field_emr": null
      }
    },
    "llama3_8b_summarization_C1_abs_001": {
      "n_outputs": 5,
      "exact_match_rate": 0.6,
      "edit_distance": {
        "mean": 17.2,
        "std": 21.06561178793533,
        "min": 0,
        "max": 43,
        "normalized_mean": 0.03214953271028038
      },
      "rouge_l": {
        "mean": 0.9578947368421051,
        "std": 0.051568205111224855,
        "min": 0.894736842105263,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9926137924194336,
        "bertscore_f1_std": 0.009046365854298383,
        "bertscore_f1_min": 0.9815343022346497,
        "bertscore_f1_max": 1.0000001192092896,
        "bertscore_precision_mean": 0.9931611537933349,
        "bertscore_recall_mean": 0.9920679330825806
      },
      "avg_output_length_chars": 532.2,
      "avg_output_length_words": 75.4,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C1",
      "abstract": "abs_001"
    },
    "llama3_8b_summarization_C1_abs_002": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 654.0,
      "avg_output_length_words": 96.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C1",
      "abstract": "abs_002"
    },
    "llama3_8b_summarization_C1_abs_003": {
      "n_outputs": 5,
      "exact_match_rate": 0.6,
      "edit_distance": {
        "mean": 26.0,
        "std": 31.843366656181317,
        "min": 0,
        "max": 65,
        "normalized_mean": 0.04186795491143318
      },
      "rouge_l": {
        "mean": 0.9538461538461538,
        "std": 0.05652668637191951,
        "min": 0.8846153846153846,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9930226802825928,
        "bertscore_f1_std": 0.008545436539953911,
        "bertscore_f1_min": 0.9825567007064819,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9946743011474609,
        "bertscore_recall_mean": 0.9913848400115967
      },
      "avg_output_length_chars": 581.0,
      "avg_output_length_words": 76.2,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C1",
      "abstract": "abs_003"
    },
    "llama3_8b_summarization_C1_abs_004": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 522.0,
      "avg_output_length_words": 73.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C1",
      "abstract": "abs_004"
    },
    "llama3_8b_summarization_C1_abs_005": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 500.0,
      "avg_output_length_words": 68.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C1",
      "abstract": "abs_005"
    },
    "llama3_8b_summarization_C1_abs_006": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 496.0,
      "avg_output_length_words": 70.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C1",
      "abstract": "abs_006"
    },
    "llama3_8b_summarization_C1_abs_007": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 559.0,
      "avg_output_length_words": 87.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C1",
      "abstract": "abs_007"
    },
    "llama3_8b_summarization_C1_abs_008": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9999999403953552,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9999999403953552,
        "bertscore_f1_max": 0.9999999403953552,
        "bertscore_precision_mean": 0.9999999403953552,
        "bertscore_recall_mean": 0.9999999403953552
      },
      "avg_output_length_chars": 580.0,
      "avg_output_length_words": 77.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C1",
      "abstract": "abs_008"
    },
    "llama3_8b_summarization_C1_abs_009": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 532.0,
      "avg_output_length_words": 78.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C1",
      "abstract": "abs_009"
    },
    "llama3_8b_summarization_C1_abs_010": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 506.0,
      "avg_output_length_words": 74.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C1",
      "abstract": "abs_010"
    },
    "llama3_8b_summarization_C1_abs_011": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 446.0,
      "avg_output_length_words": 64.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C1",
      "abstract": "abs_011"
    },
    "llama3_8b_summarization_C1_abs_012": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 616.0,
      "avg_output_length_words": 78.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C1",
      "abstract": "abs_012"
    },
    "llama3_8b_summarization_C1_abs_013": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9999999403953552,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9999999403953552,
        "bertscore_f1_max": 0.9999999403953552,
        "bertscore_precision_mean": 0.9999999403953552,
        "bertscore_recall_mean": 0.9999999403953552
      },
      "avg_output_length_chars": 537.0,
      "avg_output_length_words": 71.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C1",
      "abstract": "abs_013"
    },
    "llama3_8b_summarization_C1_abs_014": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 582.0,
      "avg_output_length_words": 86.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C1",
      "abstract": "abs_014"
    },
    "llama3_8b_summarization_C1_abs_015": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 531.0,
      "avg_output_length_words": 81.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C1",
      "abstract": "abs_015"
    },
    "llama3_8b_summarization_C1_abs_016": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 674.0,
      "avg_output_length_words": 93.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C1",
      "abstract": "abs_016"
    },
    "llama3_8b_summarization_C1_abs_017": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 529.0,
      "avg_output_length_words": 80.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C1",
      "abstract": "abs_017"
    },
    "llama3_8b_summarization_C1_abs_018": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 560.0,
      "avg_output_length_words": 76.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C1",
      "abstract": "abs_018"
    },
    "llama3_8b_summarization_C1_abs_019": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9999999403953552,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9999999403953552,
        "bertscore_f1_max": 0.9999999403953552,
        "bertscore_precision_mean": 0.9999999403953552,
        "bertscore_recall_mean": 0.9999999403953552
      },
      "avg_output_length_chars": 567.0,
      "avg_output_length_words": 77.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C1",
      "abstract": "abs_019"
    },
    "llama3_8b_summarization_C1_abs_020": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 527.0,
      "avg_output_length_words": 82.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C1",
      "abstract": "abs_020"
    },
    "llama3_8b_summarization_C1_abs_021": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 535.0,
      "avg_output_length_words": 74.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C1",
      "abstract": "abs_021"
    },
    "llama3_8b_summarization_C1_abs_022": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9999998807907104,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9999998807907104,
        "bertscore_f1_max": 0.9999998807907104,
        "bertscore_precision_mean": 0.9999998807907104,
        "bertscore_recall_mean": 0.9999998807907104
      },
      "avg_output_length_chars": 561.0,
      "avg_output_length_words": 78.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C1",
      "abstract": "abs_022"
    },
    "llama3_8b_summarization_C1_abs_023": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9999998807907104,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9999998807907104,
        "bertscore_f1_max": 0.9999998807907104,
        "bertscore_precision_mean": 0.9999998807907104,
        "bertscore_recall_mean": 0.9999998807907104
      },
      "avg_output_length_chars": 580.0,
      "avg_output_length_words": 91.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C1",
      "abstract": "abs_023"
    },
    "llama3_8b_summarization_C1_abs_024": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 595.0,
      "avg_output_length_words": 83.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C1",
      "abstract": "abs_024"
    },
    "llama3_8b_summarization_C1_abs_025": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0000001192092896,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0000001192092896,
        "bertscore_f1_max": 1.0000001192092896,
        "bertscore_precision_mean": 1.0000001192092896,
        "bertscore_recall_mean": 1.0000001192092896
      },
      "avg_output_length_chars": 493.0,
      "avg_output_length_words": 72.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C1",
      "abstract": "abs_025"
    },
    "llama3_8b_summarization_C1_abs_026": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 496.0,
      "avg_output_length_words": 64.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C1",
      "abstract": "abs_026"
    },
    "llama3_8b_summarization_C1_abs_027": {
      "n_outputs": 5,
      "exact_match_rate": 0.6,
      "edit_distance": {
        "mean": 5.2,
        "std": 6.368673331236264,
        "min": 0,
        "max": 13,
        "normalized_mean": 0.009219858156028368
      },
      "rouge_l": {
        "mean": 0.9863945578231291,
        "std": 0.016663195529137285,
        "min": 0.9659863945578231,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9955590248107911,
        "bertscore_f1_std": 0.005439061586960996,
        "bertscore_f1_min": 0.9888975620269775,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9970999956130981,
        "bertscore_recall_mean": 0.9940299749374389
      },
      "avg_output_length_chars": 555.2,
      "avg_output_length_words": 73.2,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C1",
      "abstract": "abs_027"
    },
    "llama3_8b_summarization_C1_abs_028": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 585.0,
      "avg_output_length_words": 86.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C1",
      "abstract": "abs_028"
    },
    "llama3_8b_summarization_C1_abs_029": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 538.0,
      "avg_output_length_words": 81.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C1",
      "abstract": "abs_029"
    },
    "llama3_8b_summarization_C1_abs_030": {
      "n_outputs": 5,
      "exact_match_rate": 0.6,
      "edit_distance": {
        "mean": 30.8,
        "std": 37.72214203886094,
        "min": 0,
        "max": 77,
        "normalized_mean": 0.0653927813163482
      },
      "rouge_l": {
        "mean": 0.9375,
        "std": 0.07654655446197431,
        "min": 0.84375,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9902472496032715,
        "bertscore_f1_std": 0.011944631030355535,
        "bertscore_f1_min": 0.9756181240081787,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9861576080322265,
        "bertscore_recall_mean": 0.9944244384765625
      },
      "avg_output_length_chars": 461.6,
      "avg_output_length_words": 66.4,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C1",
      "abstract": "abs_030"
    },
    "llama3_8b_summarization_C2_abs_001": {
      "n_outputs": 5,
      "exact_match_rate": 0.6,
      "edit_distance": {
        "mean": 17.2,
        "std": 21.06561178793533,
        "min": 0,
        "max": 43,
        "normalized_mean": 0.03214953271028038
      },
      "rouge_l": {
        "mean": 0.9578947368421051,
        "std": 0.051568205111224855,
        "min": 0.894736842105263,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9926137924194336,
        "bertscore_f1_std": 0.009046365854298383,
        "bertscore_f1_min": 0.9815343022346497,
        "bertscore_f1_max": 1.0000001192092896,
        "bertscore_precision_mean": 0.9931611537933349,
        "bertscore_recall_mean": 0.9920679330825806
      },
      "avg_output_length_chars": 532.2,
      "avg_output_length_words": 75.4,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_001"
    },
    "llama3_8b_summarization_C2_abs_002": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 654.0,
      "avg_output_length_words": 96.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_002"
    },
    "llama3_8b_summarization_C2_abs_003": {
      "n_outputs": 5,
      "exact_match_rate": 0.6,
      "edit_distance": {
        "mean": 26.0,
        "std": 31.843366656181317,
        "min": 0,
        "max": 65,
        "normalized_mean": 0.04186795491143318
      },
      "rouge_l": {
        "mean": 0.9538461538461538,
        "std": 0.05652668637191951,
        "min": 0.8846153846153846,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9930226802825928,
        "bertscore_f1_std": 0.008545436539953911,
        "bertscore_f1_min": 0.9825567007064819,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9946743011474609,
        "bertscore_recall_mean": 0.9913848400115967
      },
      "avg_output_length_chars": 581.0,
      "avg_output_length_words": 76.2,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_003"
    },
    "llama3_8b_summarization_C2_abs_004": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 522.0,
      "avg_output_length_words": 73.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_004"
    },
    "llama3_8b_summarization_C2_abs_005": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 500.0,
      "avg_output_length_words": 68.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_005"
    },
    "llama3_8b_summarization_C2_abs_006": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 496.0,
      "avg_output_length_words": 70.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_006"
    },
    "llama3_8b_summarization_C2_abs_007": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 559.0,
      "avg_output_length_words": 87.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_007"
    },
    "llama3_8b_summarization_C2_abs_008": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9999999403953552,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9999999403953552,
        "bertscore_f1_max": 0.9999999403953552,
        "bertscore_precision_mean": 0.9999999403953552,
        "bertscore_recall_mean": 0.9999999403953552
      },
      "avg_output_length_chars": 580.0,
      "avg_output_length_words": 77.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_008"
    },
    "llama3_8b_summarization_C2_abs_009": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 532.0,
      "avg_output_length_words": 78.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_009"
    },
    "llama3_8b_summarization_C2_abs_010": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 506.0,
      "avg_output_length_words": 74.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_010"
    },
    "llama3_8b_summarization_C2_abs_011": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 446.0,
      "avg_output_length_words": 64.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_011"
    },
    "llama3_8b_summarization_C2_abs_012": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 616.0,
      "avg_output_length_words": 78.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_012"
    },
    "llama3_8b_summarization_C2_abs_013": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9999999403953552,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9999999403953552,
        "bertscore_f1_max": 0.9999999403953552,
        "bertscore_precision_mean": 0.9999999403953552,
        "bertscore_recall_mean": 0.9999999403953552
      },
      "avg_output_length_chars": 537.0,
      "avg_output_length_words": 71.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_013"
    },
    "llama3_8b_summarization_C2_abs_014": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 582.0,
      "avg_output_length_words": 86.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_014"
    },
    "llama3_8b_summarization_C2_abs_015": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 531.0,
      "avg_output_length_words": 81.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_015"
    },
    "llama3_8b_summarization_C2_abs_016": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 674.0,
      "avg_output_length_words": 93.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_016"
    },
    "llama3_8b_summarization_C2_abs_017": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 529.0,
      "avg_output_length_words": 80.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_017"
    },
    "llama3_8b_summarization_C2_abs_018": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 560.0,
      "avg_output_length_words": 76.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_018"
    },
    "llama3_8b_summarization_C2_abs_019": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9999999403953552,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9999999403953552,
        "bertscore_f1_max": 0.9999999403953552,
        "bertscore_precision_mean": 0.9999999403953552,
        "bertscore_recall_mean": 0.9999999403953552
      },
      "avg_output_length_chars": 567.0,
      "avg_output_length_words": 77.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_019"
    },
    "llama3_8b_summarization_C2_abs_020": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 527.0,
      "avg_output_length_words": 82.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_020"
    },
    "llama3_8b_summarization_C2_abs_021": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 535.0,
      "avg_output_length_words": 74.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_021"
    },
    "llama3_8b_summarization_C2_abs_022": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9999998807907104,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9999998807907104,
        "bertscore_f1_max": 0.9999998807907104,
        "bertscore_precision_mean": 0.9999998807907104,
        "bertscore_recall_mean": 0.9999998807907104
      },
      "avg_output_length_chars": 561.0,
      "avg_output_length_words": 78.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_022"
    },
    "llama3_8b_summarization_C2_abs_023": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9999998807907104,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9999998807907104,
        "bertscore_f1_max": 0.9999998807907104,
        "bertscore_precision_mean": 0.9999998807907104,
        "bertscore_recall_mean": 0.9999998807907104
      },
      "avg_output_length_chars": 580.0,
      "avg_output_length_words": 91.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_023"
    },
    "llama3_8b_summarization_C2_abs_024": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 595.0,
      "avg_output_length_words": 83.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_024"
    },
    "llama3_8b_summarization_C2_abs_025": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0000001192092896,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0000001192092896,
        "bertscore_f1_max": 1.0000001192092896,
        "bertscore_precision_mean": 1.0000001192092896,
        "bertscore_recall_mean": 1.0000001192092896
      },
      "avg_output_length_chars": 493.0,
      "avg_output_length_words": 72.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_025"
    },
    "llama3_8b_summarization_C2_abs_026": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 496.0,
      "avg_output_length_words": 64.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_026"
    },
    "llama3_8b_summarization_C2_abs_027": {
      "n_outputs": 5,
      "exact_match_rate": 0.6,
      "edit_distance": {
        "mean": 5.2,
        "std": 6.368673331236264,
        "min": 0,
        "max": 13,
        "normalized_mean": 0.009219858156028368
      },
      "rouge_l": {
        "mean": 0.9863945578231291,
        "std": 0.016663195529137285,
        "min": 0.9659863945578231,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9955590248107911,
        "bertscore_f1_std": 0.005439061586960996,
        "bertscore_f1_min": 0.9888975620269775,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9970999956130981,
        "bertscore_recall_mean": 0.9940299749374389
      },
      "avg_output_length_chars": 555.2,
      "avg_output_length_words": 73.2,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_027"
    },
    "llama3_8b_summarization_C2_abs_028": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 585.0,
      "avg_output_length_words": 86.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_028"
    },
    "llama3_8b_summarization_C2_abs_029": {
      "n_outputs": 5,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 538.0,
      "avg_output_length_words": 81.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_029"
    },
    "llama3_8b_summarization_C2_abs_030": {
      "n_outputs": 5,
      "exact_match_rate": 0.6,
      "edit_distance": {
        "mean": 30.8,
        "std": 37.72214203886094,
        "min": 0,
        "max": 77,
        "normalized_mean": 0.0653927813163482
      },
      "rouge_l": {
        "mean": 0.9375,
        "std": 0.07654655446197431,
        "min": 0.84375,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9902472496032715,
        "bertscore_f1_std": 0.011944631030355535,
        "bertscore_f1_min": 0.9756181240081787,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9861576080322265,
        "bertscore_recall_mean": 0.9944244384765625
      },
      "avg_output_length_chars": 461.6,
      "avg_output_length_words": 66.4,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C2",
      "abstract": "abs_030"
    },
    "llama3_8b_summarization_C3_t0.0_abs_001": {
      "n_outputs": 3,
      "exact_match_rate": 0.3333333333333333,
      "edit_distance": {
        "mean": 28.666666666666668,
        "std": 20.27039439401436,
        "min": 0,
        "max": 43,
        "normalized_mean": 0.05358255451713396
      },
      "rouge_l": {
        "mean": 0.9298245614035087,
        "std": 0.049621528504319175,
        "min": 0.894736842105263,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9876895745595297,
        "bertscore_f1_std": 0.008704869601945017,
        "bertscore_f1_min": 0.9815343022346497,
        "bertscore_f1_max": 1.0000001192092896,
        "bertscore_precision_mean": 0.9886018435160319,
        "bertscore_recall_mean": 0.9867798089981079
      },
      "avg_output_length_chars": 530.3333333333334,
      "avg_output_length_words": 75.66666666666667,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_001"
    },
    "llama3_8b_summarization_C3_t0.0_abs_002": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 654.0,
      "avg_output_length_words": 96.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_002"
    },
    "llama3_8b_summarization_C3_t0.0_abs_003": {
      "n_outputs": 3,
      "exact_match_rate": 0.3333333333333333,
      "edit_distance": {
        "mean": 43.333333333333336,
        "std": 30.64129385141706,
        "min": 0,
        "max": 65,
        "normalized_mean": 0.06977992485238863
      },
      "rouge_l": {
        "mean": 0.923076923076923,
        "std": 0.05439282932204213,
        "min": 0.8846153846153846,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9883711338043213,
        "bertscore_f1_std": 0.008222850144475425,
        "bertscore_f1_min": 0.9825567007064819,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9911238352457682,
        "bertscore_recall_mean": 0.9856414000193278
      },
      "avg_output_length_chars": 587.6666666666666,
      "avg_output_length_words": 77.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_003"
    },
    "llama3_8b_summarization_C3_t0.0_abs_004": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 522.0,
      "avg_output_length_words": 73.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_004"
    },
    "llama3_8b_summarization_C3_t0.0_abs_005": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 500.0,
      "avg_output_length_words": 68.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_005"
    },
    "llama3_8b_summarization_C3_t0.0_abs_006": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 496.0,
      "avg_output_length_words": 70.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_006"
    },
    "llama3_8b_summarization_C3_t0.0_abs_007": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 559.0,
      "avg_output_length_words": 87.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_007"
    },
    "llama3_8b_summarization_C3_t0.0_abs_008": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9999999403953552,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9999999403953552,
        "bertscore_f1_max": 0.9999999403953552,
        "bertscore_precision_mean": 0.9999999403953552,
        "bertscore_recall_mean": 0.9999999403953552
      },
      "avg_output_length_chars": 580.0,
      "avg_output_length_words": 77.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_008"
    },
    "llama3_8b_summarization_C3_t0.0_abs_009": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 532.0,
      "avg_output_length_words": 78.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_009"
    },
    "llama3_8b_summarization_C3_t0.0_abs_010": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 506.0,
      "avg_output_length_words": 74.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_010"
    },
    "llama3_8b_summarization_C3_t0.0_abs_011": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 446.0,
      "avg_output_length_words": 64.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_011"
    },
    "llama3_8b_summarization_C3_t0.0_abs_012": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 616.0,
      "avg_output_length_words": 78.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_012"
    },
    "llama3_8b_summarization_C3_t0.0_abs_013": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9999999403953552,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9999999403953552,
        "bertscore_f1_max": 0.9999999403953552,
        "bertscore_precision_mean": 0.9999999403953552,
        "bertscore_recall_mean": 0.9999999403953552
      },
      "avg_output_length_chars": 537.0,
      "avg_output_length_words": 71.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_013"
    },
    "llama3_8b_summarization_C3_t0.0_abs_014": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 582.0,
      "avg_output_length_words": 86.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_014"
    },
    "llama3_8b_summarization_C3_t0.0_abs_015": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 531.0,
      "avg_output_length_words": 81.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_015"
    },
    "llama3_8b_summarization_C3_t0.0_abs_016": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 674.0,
      "avg_output_length_words": 93.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_016"
    },
    "llama3_8b_summarization_C3_t0.0_abs_017": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 529.0,
      "avg_output_length_words": 80.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_017"
    },
    "llama3_8b_summarization_C3_t0.0_abs_018": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 560.0,
      "avg_output_length_words": 76.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_018"
    },
    "llama3_8b_summarization_C3_t0.0_abs_019": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9999999403953552,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9999999403953552,
        "bertscore_f1_max": 0.9999999403953552,
        "bertscore_precision_mean": 0.9999999403953552,
        "bertscore_recall_mean": 0.9999999403953552
      },
      "avg_output_length_chars": 567.0,
      "avg_output_length_words": 77.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_019"
    },
    "llama3_8b_summarization_C3_t0.0_abs_020": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 527.0,
      "avg_output_length_words": 82.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_020"
    },
    "llama3_8b_summarization_C3_t0.0_abs_021": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 535.0,
      "avg_output_length_words": 74.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_021"
    },
    "llama3_8b_summarization_C3_t0.0_abs_022": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9999998807907104,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9999998807907104,
        "bertscore_f1_max": 0.9999998807907104,
        "bertscore_precision_mean": 0.9999998807907104,
        "bertscore_recall_mean": 0.9999998807907104
      },
      "avg_output_length_chars": 561.0,
      "avg_output_length_words": 78.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_022"
    },
    "llama3_8b_summarization_C3_t0.0_abs_023": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9999998807907104,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 0.9999998807907104,
        "bertscore_f1_max": 0.9999998807907104,
        "bertscore_precision_mean": 0.9999998807907104,
        "bertscore_recall_mean": 0.9999998807907104
      },
      "avg_output_length_chars": 580.0,
      "avg_output_length_words": 91.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_023"
    },
    "llama3_8b_summarization_C3_t0.0_abs_024": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 595.0,
      "avg_output_length_words": 83.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_024"
    },
    "llama3_8b_summarization_C3_t0.0_abs_025": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0000001192092896,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0000001192092896,
        "bertscore_f1_max": 1.0000001192092896,
        "bertscore_precision_mean": 1.0000001192092896,
        "bertscore_recall_mean": 1.0000001192092896
      },
      "avg_output_length_chars": 493.0,
      "avg_output_length_words": 72.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_025"
    },
    "llama3_8b_summarization_C3_t0.0_abs_026": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 496.0,
      "avg_output_length_words": 64.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_026"
    },
    "llama3_8b_summarization_C3_t0.0_abs_027": {
      "n_outputs": 3,
      "exact_match_rate": 0.3333333333333333,
      "edit_distance": {
        "mean": 8.666666666666666,
        "std": 6.128258770283411,
        "min": 0,
        "max": 13,
        "normalized_mean": 0.015366430260047281
      },
      "rouge_l": {
        "mean": 0.9773242630385487,
        "std": 0.01603416737384463,
        "min": 0.9659863945578231,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9925983746846517,
        "bertscore_f1_std": 0.005233739452284806,
        "bertscore_f1_min": 0.9888975620269775,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9951666593551636,
        "bertscore_recall_mean": 0.9900499582290649
      },
      "avg_output_length_chars": 556.6666666666666,
      "avg_output_length_words": 73.33333333333333,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_027"
    },
    "llama3_8b_summarization_C3_t0.0_abs_028": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 585.0,
      "avg_output_length_words": 86.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_028"
    },
    "llama3_8b_summarization_C3_t0.0_abs_029": {
      "n_outputs": 3,
      "exact_match_rate": 1.0,
      "edit_distance": {
        "mean": 0.0,
        "std": 0.0,
        "min": 0,
        "max": 0,
        "normalized_mean": 0.0
      },
      "rouge_l": {
        "mean": 1.0,
        "std": 0.0,
        "min": 1.0,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 1.0,
        "bertscore_f1_std": 0.0,
        "bertscore_f1_min": 1.0,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 1.0,
        "bertscore_recall_mean": 1.0
      },
      "avg_output_length_chars": 538.0,
      "avg_output_length_words": 81.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_029"
    },
    "llama3_8b_summarization_C3_t0.0_abs_030": {
      "n_outputs": 3,
      "exact_match_rate": 0.3333333333333333,
      "edit_distance": {
        "mean": 51.333333333333336,
        "std": 36.29814810090944,
        "min": 0,
        "max": 77,
        "normalized_mean": 0.10898796886058032
      },
      "rouge_l": {
        "mean": 0.8958333333333334,
        "std": 0.0736569563735987,
        "min": 0.84375,
        "max": 1.0
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9837454160054525,
        "bertscore_f1_std": 0.011493726567910876,
        "bertscore_f1_min": 0.9756181240081787,
        "bertscore_f1_max": 1.0,
        "bertscore_precision_mean": 0.9769293467203776,
        "bertscore_recall_mean": 0.9907073974609375
      },
      "avg_output_length_chars": 455.3333333333333,
      "avg_output_length_words": 65.33333333333333,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.0",
      "abstract": "abs_030"
    },
    "llama3_8b_summarization_C3_t0.3_abs_001": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 102.66666666666667,
        "std": 50.075498555237125,
        "min": 32,
        "max": 142,
        "normalized_mean": 0.17602346194061722
      },
      "rouge_l": {
        "mean": 0.8089164941338854,
        "std": 0.1163708197037625,
        "min": 0.7204968944099379,
        "max": 0.9733333333333334
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9724922776222229,
        "bertscore_f1_std": 0.016581749343728743,
        "bertscore_f1_min": 0.9597756266593933,
        "bertscore_f1_max": 0.9959136247634888,
        "bertscore_precision_mean": 0.9766301115353903,
        "bertscore_recall_mean": 0.968405524889628
      },
      "avg_output_length_chars": 554.0,
      "avg_output_length_words": 78.66666666666667,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_001"
    },
    "llama3_8b_summarization_C3_t0.3_abs_002": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 311.3333333333333,
        "std": 119.73953213352537,
        "min": 142,
        "max": 397,
        "normalized_mean": 0.4837938578361171
      },
      "rouge_l": {
        "mean": 0.5734968734968735,
        "std": 0.14623030100208537,
        "min": 0.4523809523809524,
        "max": 0.7792207792207793
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9428825179735819,
        "bertscore_f1_std": 0.007394087584166931,
        "bertscore_f1_min": 0.9361740350723267,
        "bertscore_f1_max": 0.9531834125518799,
        "bertscore_precision_mean": 0.9336480498313904,
        "bertscore_recall_mean": 0.9524401227633158
      },
      "avg_output_length_chars": 578.3333333333334,
      "avg_output_length_words": 83.66666666666667,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_002"
    },
    "llama3_8b_summarization_C3_t0.3_abs_003": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 48.0,
        "std": 14.165686240583852,
        "min": 28,
        "max": 59,
        "normalized_mean": 0.07265990344153116
      },
      "rouge_l": {
        "mean": 0.9494423892197171,
        "std": 0.012490457522616417,
        "min": 0.935672514619883,
        "max": 0.9659090909090908
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9919313589731852,
        "bertscore_f1_std": 0.0021069373932453075,
        "bertscore_f1_min": 0.9896857738494873,
        "bertscore_f1_max": 0.9947502613067627,
        "bertscore_precision_mean": 0.9923070271809896,
        "bertscore_recall_mean": 0.9915639758110046
      },
      "avg_output_length_chars": 648.6666666666666,
      "avg_output_length_words": 86.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_003"
    },
    "llama3_8b_summarization_C3_t0.3_abs_004": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 122.66666666666667,
        "std": 48.66438350808754,
        "min": 54,
        "max": 161,
        "normalized_mean": 0.2077321593592233
      },
      "rouge_l": {
        "mean": 0.7768203572141227,
        "std": 0.09099603594064318,
        "min": 0.7080745341614907,
        "max": 0.9054054054054055
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9740530053774515,
        "bertscore_f1_std": 0.010952131363432274,
        "bertscore_f1_min": 0.9651429057121277,
        "bertscore_f1_max": 0.9894798994064331,
        "bertscore_precision_mean": 0.9766845703125,
        "bertscore_recall_mean": 0.9714457988739014
      },
      "avg_output_length_chars": 550.6666666666666,
      "avg_output_length_words": 78.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_004"
    },
    "llama3_8b_summarization_C3_t0.3_abs_005": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 112.0,
        "std": 36.359317925395686,
        "min": 74,
        "max": 161,
        "normalized_mean": 0.20453986358045398
      },
      "rouge_l": {
        "mean": 0.80127777354884,
        "std": 0.06089044049957273,
        "min": 0.7152317880794702,
        "max": 0.8472222222222223
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9670338829358419,
        "bertscore_f1_std": 0.008237159471411612,
        "bertscore_f1_min": 0.9555837512016296,
        "bertscore_f1_max": 0.9746155738830566,
        "bertscore_precision_mean": 0.9639048377672831,
        "bertscore_recall_mean": 0.9702643354733785
      },
      "avg_output_length_chars": 533.0,
      "avg_output_length_words": 73.33333333333333,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_005"
    },
    "llama3_8b_summarization_C3_t0.3_abs_006": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 43.333333333333336,
        "std": 11.440668201153676,
        "min": 29,
        "max": 57,
        "normalized_mean": 0.08880809375984162
      },
      "rouge_l": {
        "mean": 0.9364473751229277,
        "std": 0.031019779045172325,
        "min": 0.9037037037037037,
        "max": 0.9781021897810218
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9926947156588236,
        "bertscore_f1_std": 0.0022104064422106377,
        "bertscore_f1_min": 0.9901617169380188,
        "bertscore_f1_max": 0.9955476522445679,
        "bertscore_precision_mean": 0.9917516708374023,
        "bertscore_recall_mean": 0.9936451117197672
      },
      "avg_output_length_chars": 480.3333333333333,
      "avg_output_length_words": 68.33333333333333,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_006"
    },
    "llama3_8b_summarization_C3_t0.3_abs_007": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 168.0,
        "std": 79.01054781905178,
        "min": 58,
        "max": 240,
        "normalized_mean": 0.2909151080559415
      },
      "rouge_l": {
        "mean": 0.7229492911061803,
        "std": 0.15169900022496252,
        "min": 0.5977011494252873,
        "max": 0.9364161849710982
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9628873070081075,
        "bertscore_f1_std": 0.01799629890127588,
        "bertscore_f1_min": 0.9486353993415833,
        "bertscore_f1_max": 0.9882742166519165,
        "bertscore_precision_mean": 0.9662478367487589,
        "bertscore_recall_mean": 0.9595614274342855
      },
      "avg_output_length_chars": 563.6666666666666,
      "avg_output_length_words": 85.66666666666667,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_007"
    },
    "llama3_8b_summarization_C3_t0.3_abs_008": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 140.33333333333334,
        "std": 20.531818125912658,
        "min": 112,
        "max": 160,
        "normalized_mean": 0.23861992209237617
      },
      "rouge_l": {
        "mean": 0.7072257596935341,
        "std": 0.0432119060994601,
        "min": 0.6533333333333333,
        "max": 0.7591240875912408
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9618930220603943,
        "bertscore_f1_std": 0.008526191401418767,
        "bertscore_f1_min": 0.9499515295028687,
        "bertscore_f1_max": 0.969310998916626,
        "bertscore_precision_mean": 0.9554038643836975,
        "bertscore_recall_mean": 0.9684938589731852
      },
      "avg_output_length_chars": 559.0,
      "avg_output_length_words": 71.33333333333333,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_008"
    },
    "llama3_8b_summarization_C3_t0.3_abs_009": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 227.33333333333334,
        "std": 57.56349151637308,
        "min": 151,
        "max": 290,
        "normalized_mean": 0.3444455628721536
      },
      "rouge_l": {
        "mean": 0.6666192937970259,
        "std": 0.10501458844797572,
        "min": 0.5775401069518715,
        "max": 0.8140703517587939
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9506763219833374,
        "bertscore_f1_std": 0.011383009597606015,
        "bertscore_f1_min": 0.9402774572372437,
        "bertscore_f1_max": 0.966517984867096,
        "bertscore_precision_mean": 0.955543061097463,
        "bertscore_recall_mean": 0.9458758036295573
      },
      "avg_output_length_chars": 641.3333333333334,
      "avg_output_length_words": 96.66666666666667,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_009"
    },
    "llama3_8b_summarization_C3_t0.3_abs_010": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 65.33333333333333,
        "std": 43.36921591277491,
        "min": 4,
        "max": 96,
        "normalized_mean": 0.12885790461963006
      },
      "rouge_l": {
        "mean": 0.8947760710350638,
        "std": 0.06484905667113647,
        "min": 0.8489208633093526,
        "max": 0.9864864864864865
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9830946127573649,
        "bertscore_f1_std": 0.010607961726970537,
        "bertscore_f1_min": 0.9749870300292969,
        "bertscore_f1_max": 0.9980797171592712,
        "bertscore_precision_mean": 0.9831266601880392,
        "bertscore_recall_mean": 0.9830878774325053
      },
      "avg_output_length_chars": 489.6666666666667,
      "avg_output_length_words": 71.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_010"
    },
    "llama3_8b_summarization_C3_t0.3_abs_011": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 364.6666666666667,
        "std": 12.657891697365017,
        "min": 347,
        "max": 376,
        "normalized_mean": 0.6071845968712395
      },
      "rouge_l": {
        "mean": 0.4010295702230981,
        "std": 0.058586754796661276,
        "min": 0.3246753246753247,
        "max": 0.46706586826347307
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9221408168474833,
        "bertscore_f1_std": 0.007754684734339577,
        "bertscore_f1_min": 0.9127854704856873,
        "bertscore_f1_max": 0.931774377822876,
        "bertscore_precision_mean": 0.9254918098449707,
        "bertscore_recall_mean": 0.918823758761088
      },
      "avg_output_length_chars": 559.3333333333334,
      "avg_output_length_words": 77.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_011"
    },
    "llama3_8b_summarization_C3_t0.3_abs_012": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 302.0,
        "std": 67.29536883520787,
        "min": 209,
        "max": 366,
        "normalized_mean": 0.4584629891560585
      },
      "rouge_l": {
        "mean": 0.5614620043502622,
        "std": 0.09419225931860079,
        "min": 0.45962732919254656,
        "max": 0.6867469879518072
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9421051541964213,
        "bertscore_f1_std": 0.014612704491627528,
        "bertscore_f1_min": 0.9269424676895142,
        "bertscore_f1_max": 0.9618465304374695,
        "bertscore_precision_mean": 0.948104461034139,
        "bertscore_recall_mean": 0.9362451632817587
      },
      "avg_output_length_chars": 633.0,
      "avg_output_length_words": 83.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_012"
    },
    "llama3_8b_summarization_C3_t0.3_abs_013": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 69.33333333333333,
        "std": 19.362047641943473,
        "min": 44,
        "max": 91,
        "normalized_mean": 0.14139943156440427
      },
      "rouge_l": {
        "mean": 0.8690947367760536,
        "std": 0.029989228793075497,
        "min": 0.8368794326241135,
        "max": 0.9090909090909091
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9861306349436442,
        "bertscore_f1_std": 0.003953705069392414,
        "bertscore_f1_min": 0.9820259809494019,
        "bertscore_f1_max": 0.9914710521697998,
        "bertscore_precision_mean": 0.9840818444887797,
        "bertscore_recall_mean": 0.9881893992424011
      },
      "avg_output_length_chars": 470.0,
      "avg_output_length_words": 68.33333333333333,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_013"
    },
    "llama3_8b_summarization_C3_t0.3_abs_014": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 181.33333333333334,
        "std": 78.99507720245751,
        "min": 70,
        "max": 245,
        "normalized_mean": 0.2922122060714157
      },
      "rouge_l": {
        "mean": 0.7590915944484733,
        "std": 0.08231610682246093,
        "min": 0.6829268292682926,
        "max": 0.8734177215189873
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9596134821573893,
        "bertscore_f1_std": 0.012954002516283578,
        "bertscore_f1_min": 0.9460444450378418,
        "bertscore_f1_max": 0.9770572781562805,
        "bertscore_precision_mean": 0.963260551293691,
        "bertscore_recall_mean": 0.9560553431510925
      },
      "avg_output_length_chars": 568.3333333333334,
      "avg_output_length_words": 82.33333333333333,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_014"
    },
    "llama3_8b_summarization_C3_t0.3_abs_015": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 123.66666666666667,
        "std": 41.12041936664665,
        "min": 66,
        "max": 159,
        "normalized_mean": 0.22721210728590804
      },
      "rouge_l": {
        "mean": 0.8219056928734348,
        "std": 0.02863988000369966,
        "min": 0.7922077922077924,
        "max": 0.8606060606060606
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9765816132227579,
        "bertscore_f1_std": 0.0016585520126088664,
        "bertscore_f1_min": 0.9745907187461853,
        "bertscore_f1_max": 0.9786510467529297,
        "bertscore_precision_mean": 0.980431079864502,
        "bertscore_recall_mean": 0.972796618938446
      },
      "avg_output_length_chars": 520.3333333333334,
      "avg_output_length_words": 79.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_015"
    },
    "llama3_8b_summarization_C3_t0.3_abs_016": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 253.33333333333334,
        "std": 66.44463024871828,
        "min": 200,
        "max": 347,
        "normalized_mean": 0.41063278918729146
      },
      "rouge_l": {
        "mean": 0.5967140144068664,
        "std": 0.11399862453173774,
        "min": 0.43636363636363634,
        "max": 0.691358024691358
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9483975966771444,
        "bertscore_f1_std": 0.008394143117767743,
        "bertscore_f1_min": 0.93687504529953,
        "bertscore_f1_max": 0.9566318392753601,
        "bertscore_precision_mean": 0.9484542806943258,
        "bertscore_recall_mean": 0.9483665227890015
      },
      "avg_output_length_chars": 586.3333333333334,
      "avg_output_length_words": 80.66666666666667,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_016"
    },
    "llama3_8b_summarization_C3_t0.3_abs_017": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 103.0,
        "std": 47.672493816315786,
        "min": 36,
        "max": 143,
        "normalized_mean": 0.19080589408607204
      },
      "rouge_l": {
        "mean": 0.7742928524775855,
        "std": 0.09161211178512157,
        "min": 0.691358024691358,
        "max": 0.9019607843137256
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9757361809412638,
        "bertscore_f1_std": 0.011231940697162792,
        "bertscore_f1_min": 0.9668446779251099,
        "bertscore_f1_max": 0.9915810823440552,
        "bertscore_precision_mean": 0.9765804608662924,
        "bertscore_recall_mean": 0.9748945832252502
      },
      "avg_output_length_chars": 529.3333333333334,
      "avg_output_length_words": 79.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_017"
    },
    "llama3_8b_summarization_C3_t0.3_abs_018": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 118.66666666666667,
        "std": 44.43972197132751,
        "min": 57,
        "max": 160,
        "normalized_mean": 0.19666983647517292
      },
      "rouge_l": {
        "mean": 0.8411377792028966,
        "std": 0.05326588533547099,
        "min": 0.7974683544303797,
        "max": 0.9161290322580645
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9803950389226278,
        "bertscore_f1_std": 0.005042723910932744,
        "bertscore_f1_min": 0.975268542766571,
        "bertscore_f1_max": 0.9872516393661499,
        "bertscore_precision_mean": 0.9821088711420695,
        "bertscore_recall_mean": 0.9787057439486185
      },
      "avg_output_length_chars": 582.3333333333334,
      "avg_output_length_words": 79.33333333333333,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_018"
    },
    "llama3_8b_summarization_C3_t0.3_abs_019": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 71.33333333333333,
        "std": 39.43207943906698,
        "min": 16,
        "max": 105,
        "normalized_mean": 0.13481275014293884
      },
      "rouge_l": {
        "mean": 0.8967372671670358,
        "std": 0.039018216474765166,
        "min": 0.8652482269503546,
        "max": 0.9517241379310345
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9819261829058329,
        "bertscore_f1_std": 0.006967358783157054,
        "bertscore_f1_min": 0.974575936794281,
        "bertscore_f1_max": 0.9912843108177185,
        "bertscore_precision_mean": 0.981711745262146,
        "bertscore_recall_mean": 0.9821526606877645
      },
      "avg_output_length_chars": 521.3333333333334,
      "avg_output_length_words": 71.33333333333333,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_019"
    },
    "llama3_8b_summarization_C3_t0.3_abs_020": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 97.0,
        "std": 33.0252428706689,
        "min": 67,
        "max": 143,
        "normalized_mean": 0.18535650735755582
      },
      "rouge_l": {
        "mean": 0.8471707818930042,
        "std": 0.05155342505741464,
        "min": 0.7777777777777778,
        "max": 0.9012345679012346
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9857192436854044,
        "bertscore_f1_std": 0.004875851306295407,
        "bertscore_f1_min": 0.9797858595848083,
        "bertscore_f1_max": 0.9917284846305847,
        "bertscore_precision_mean": 0.9853178064028422,
        "bertscore_recall_mean": 0.9861303766568502
      },
      "avg_output_length_chars": 517.6666666666666,
      "avg_output_length_words": 80.66666666666667,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_020"
    },
    "llama3_8b_summarization_C3_t0.3_abs_021": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 201.0,
        "std": 134.35276947896037,
        "min": 11,
        "max": 297,
        "normalized_mean": 0.31504702194357365
      },
      "rouge_l": {
        "mean": 0.7759295294289958,
        "std": 0.13872312406574477,
        "min": 0.6741573033707865,
        "max": 0.9720670391061451
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9796093503634135,
        "bertscore_f1_std": 0.012389637441925406,
        "bertscore_f1_min": 0.9708299040794373,
        "bertscore_f1_max": 0.9971309304237366,
        "bertscore_precision_mean": 0.9796422123908997,
        "bertscore_recall_mean": 0.9795766274134318
      },
      "avg_output_length_chars": 636.6666666666666,
      "avg_output_length_words": 89.33333333333333,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_021"
    },
    "llama3_8b_summarization_C3_t0.3_abs_022": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 109.66666666666667,
        "std": 23.156472577277874,
        "min": 77,
        "max": 128,
        "normalized_mean": 0.18443444717954524
      },
      "rouge_l": {
        "mean": 0.7957442537388336,
        "std": 0.04996201284141476,
        "min": 0.7530864197530863,
        "max": 0.8658536585365854
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9738914966583252,
        "bertscore_f1_std": 0.007855419013145603,
        "bertscore_f1_min": 0.9671857357025146,
        "bertscore_f1_max": 0.9849148392677307,
        "bertscore_precision_mean": 0.9749704798062643,
        "bertscore_recall_mean": 0.9728171825408936
      },
      "avg_output_length_chars": 590.0,
      "avg_output_length_words": 81.66666666666667,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_022"
    },
    "llama3_8b_summarization_C3_t0.3_abs_023": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 246.33333333333334,
        "std": 4.988876515698588,
        "min": 241,
        "max": 253,
        "normalized_mean": 0.4391009483430925
      },
      "rouge_l": {
        "mean": 0.5863245272129318,
        "std": 0.06785574190828972,
        "min": 0.49101796407185627,
        "max": 0.6436781609195403
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9489558537801107,
        "bertscore_f1_std": 0.008680138649184913,
        "bertscore_f1_min": 0.9367759823799133,
        "bertscore_f1_max": 0.9563706517219543,
        "bertscore_precision_mean": 0.9454715847969055,
        "bertscore_recall_mean": 0.9524729450543722
      },
      "avg_output_length_chars": 544.6666666666666,
      "avg_output_length_words": 85.66666666666667,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_023"
    },
    "llama3_8b_summarization_C3_t0.3_abs_024": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 200.33333333333334,
        "std": 11.08552609887726,
        "min": 192,
        "max": 216,
        "normalized_mean": 0.3578981622432546
      },
      "rouge_l": {
        "mean": 0.6840958605664488,
        "std": 0.012324301197151163,
        "min": 0.6666666666666666,
        "max": 0.6928104575163399
      },
      "bert_score": {
        "bertscore_f1_mean": 0.965298612912496,
        "bertscore_f1_std": 0.00749913262101702,
        "bertscore_f1_min": 0.9548336863517761,
        "bertscore_f1_max": 0.9720208644866943,
        "bertscore_precision_mean": 0.9610581596692404,
        "bertscore_recall_mean": 0.9695968826611837
      },
      "avg_output_length_chars": 539.0,
      "avg_output_length_words": 75.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_024"
    },
    "llama3_8b_summarization_C3_t0.3_abs_025": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 78.33333333333333,
        "std": 30.291179500896884,
        "min": 38,
        "max": 111,
        "normalized_mean": 0.1270013770842653
      },
      "rouge_l": {
        "mean": 0.9261470239953614,
        "std": 0.022769937841827414,
        "min": 0.8994082840236687,
        "max": 0.9550561797752809
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9866905013720194,
        "bertscore_f1_std": 0.00366444238276016,
        "bertscore_f1_min": 0.9827517867088318,
        "bertscore_f1_max": 0.9915765523910522,
        "bertscore_precision_mean": 0.9919813275337219,
        "bertscore_recall_mean": 0.9814836184183756
      },
      "avg_output_length_chars": 593.3333333333334,
      "avg_output_length_words": 86.33333333333333,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_025"
    },
    "llama3_8b_summarization_C3_t0.3_abs_026": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 120.0,
        "std": 21.354156504062622,
        "min": 102,
        "max": 150,
        "normalized_mean": 0.22710136557824545
      },
      "rouge_l": {
        "mean": 0.7563990637555316,
        "std": 0.0584920165594319,
        "min": 0.7050359712230215,
        "max": 0.8382352941176471
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9743870695432028,
        "bertscore_f1_std": 0.005275633715335726,
        "bertscore_f1_min": 0.9674323201179504,
        "bertscore_f1_max": 0.9802036881446838,
        "bertscore_precision_mean": 0.9731194575627645,
        "bertscore_recall_mean": 0.9756580392519633
      },
      "avg_output_length_chars": 520.0,
      "avg_output_length_words": 68.33333333333333,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_026"
    },
    "llama3_8b_summarization_C3_t0.3_abs_027": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 218.66666666666666,
        "std": 25.616834742454465,
        "min": 183,
        "max": 242,
        "normalized_mean": 0.3537641708264004
      },
      "rouge_l": {
        "mean": 0.6970554090854842,
        "std": 0.03334770447429674,
        "min": 0.6578947368421053,
        "max": 0.7393939393939394
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9556126197179159,
        "bertscore_f1_std": 0.005534193653862285,
        "bertscore_f1_min": 0.9478566646575928,
        "bertscore_f1_max": 0.96039879322052,
        "bertscore_precision_mean": 0.9480140209197998,
        "bertscore_recall_mean": 0.9633520046869913
      },
      "avg_output_length_chars": 570.6666666666666,
      "avg_output_length_words": 77.33333333333333,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_027"
    },
    "llama3_8b_summarization_C3_t0.3_abs_028": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 186.66666666666666,
        "std": 95.31118623866887,
        "min": 52,
        "max": 259,
        "normalized_mean": 0.36819758016898024
      },
      "rouge_l": {
        "mean": 0.6617336529482093,
        "std": 0.15398923096399528,
        "min": 0.547945205479452,
        "max": 0.8794326241134751
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9533159931500753,
        "bertscore_f1_std": 0.02082439242561844,
        "bertscore_f1_min": 0.9381963610649109,
        "bertscore_f1_max": 0.9827625751495361,
        "bertscore_precision_mean": 0.9537235299746195,
        "bertscore_recall_mean": 0.9529121915499369
      },
      "avg_output_length_chars": 493.3333333333333,
      "avg_output_length_words": 72.33333333333333,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_028"
    },
    "llama3_8b_summarization_C3_t0.3_abs_029": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 242.33333333333334,
        "std": 67.48991694237658,
        "min": 147,
        "max": 294,
        "normalized_mean": 0.43788361864586917
      },
      "rouge_l": {
        "mean": 0.6367539739405037,
        "std": 0.11701427939139546,
        "min": 0.5316455696202531,
        "max": 0.8
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9624531865119934,
        "bertscore_f1_std": 0.009826044771495021,
        "bertscore_f1_min": 0.9539805054664612,
        "bertscore_f1_max": 0.9762282371520996,
        "bertscore_precision_mean": 0.9567561348279318,
        "bertscore_recall_mean": 0.96823650598526
      },
      "avg_output_length_chars": 550.3333333333334,
      "avg_output_length_words": 80.33333333333333,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_029"
    },
    "llama3_8b_summarization_C3_t0.3_abs_030": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 205.0,
        "std": 28.600699292150182,
        "min": 174,
        "max": 243,
        "normalized_mean": 0.47799789474856724
      },
      "rouge_l": {
        "mean": 0.5966216071924317,
        "std": 0.06417086902201107,
        "min": 0.5116279069767442,
        "max": 0.6666666666666667
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9472850561141968,
        "bertscore_f1_std": 0.006809217542785014,
        "bertscore_f1_min": 0.9381195306777954,
        "bertscore_f1_max": 0.9544257521629333,
        "bertscore_precision_mean": 0.9462212125460306,
        "bertscore_recall_mean": 0.9484011729558309
      },
      "avg_output_length_chars": 414.6666666666667,
      "avg_output_length_words": 62.666666666666664,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.3",
      "abstract": "abs_030"
    },
    "llama3_8b_summarization_C3_t0.7_abs_001": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 328.0,
        "std": 46.180804092898455,
        "min": 263,
        "max": 366,
        "normalized_mean": 0.5294278641819625
      },
      "rouge_l": {
        "mean": 0.4661618556350815,
        "std": 0.062102892026766325,
        "min": 0.4023668639053255,
        "max": 0.5503355704697986
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9289763371149699,
        "bertscore_f1_std": 0.00730140618647739,
        "bertscore_f1_min": 0.9234004616737366,
        "bertscore_f1_max": 0.9392907619476318,
        "bertscore_precision_mean": 0.9302030404408773,
        "bertscore_recall_mean": 0.9278914133707682
      },
      "avg_output_length_chars": 579.3333333333334,
      "avg_output_length_words": 80.66666666666667,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_001"
    },
    "llama3_8b_summarization_C3_t0.7_abs_002": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 272.6666666666667,
        "std": 58.73291713813946,
        "min": 190,
        "max": 321,
        "normalized_mean": 0.5021977758028487
      },
      "rouge_l": {
        "mean": 0.4625980986847065,
        "std": 0.11406592016897879,
        "min": 0.35384615384615387,
        "max": 0.6201550387596899
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9495785633722941,
        "bertscore_f1_std": 0.008696489018984488,
        "bertscore_f1_min": 0.939135730266571,
        "bertscore_f1_max": 0.9604261517524719,
        "bertscore_precision_mean": 0.9561397433280945,
        "bertscore_recall_mean": 0.943135678768158
      },
      "avg_output_length_chars": 505.3333333333333,
      "avg_output_length_words": 67.33333333333333,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_002"
    },
    "llama3_8b_summarization_C3_t0.7_abs_003": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 131.66666666666666,
        "std": 13.767917618708921,
        "min": 120,
        "max": 151,
        "normalized_mean": 0.215167192994244
      },
      "rouge_l": {
        "mean": 0.736587430442807,
        "std": 0.02601020546806118,
        "min": 0.7000000000000001,
        "max": 0.7581699346405228
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9612720807393392,
        "bertscore_f1_std": 0.0057771003141025255,
        "bertscore_f1_min": 0.9537215828895569,
        "bertscore_f1_max": 0.967750072479248,
        "bertscore_precision_mean": 0.9619815746943156,
        "bertscore_recall_mean": 0.9605653087298075
      },
      "avg_output_length_chars": 600.0,
      "avg_output_length_words": 78.33333333333333,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_003"
    },
    "llama3_8b_summarization_C3_t0.7_abs_004": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 209.66666666666666,
        "std": 40.450243784459715,
        "min": 159,
        "max": 258,
        "normalized_mean": 0.376605503596112
      },
      "rouge_l": {
        "mean": 0.6790751944458996,
        "std": 0.0489571850870995,
        "min": 0.6433566433566433,
        "max": 0.7482993197278912
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9628970821698507,
        "bertscore_f1_std": 0.004049612573712094,
        "bertscore_f1_min": 0.9590657949447632,
        "bertscore_f1_max": 0.9684991836547852,
        "bertscore_precision_mean": 0.965617299079895,
        "bertscore_recall_mean": 0.96022895971934
      },
      "avg_output_length_chars": 525.0,
      "avg_output_length_words": 74.66666666666667,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_004"
    },
    "llama3_8b_summarization_C3_t0.7_abs_005": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 292.0,
        "std": 27.434771124736336,
        "min": 257,
        "max": 324,
        "normalized_mean": 0.5380566983733733
      },
      "rouge_l": {
        "mean": 0.4305555555555556,
        "std": 0.07436286298244915,
        "min": 0.3333333333333333,
        "max": 0.513888888888889
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9273003141085306,
        "bertscore_f1_std": 0.01027659792349675,
        "bertscore_f1_min": 0.9154242873191833,
        "bertscore_f1_max": 0.9404931664466858,
        "bertscore_precision_mean": 0.9277295072873434,
        "bertscore_recall_mean": 0.926871637503306
      },
      "avg_output_length_chars": 523.0,
      "avg_output_length_words": 71.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_005"
    },
    "llama3_8b_summarization_C3_t0.7_abs_006": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 189.66666666666666,
        "std": 44.31954672852851,
        "min": 127,
        "max": 222,
        "normalized_mean": 0.31054452286956175
      },
      "rouge_l": {
        "mean": 0.708021448731876,
        "std": 0.035277155399267277,
        "min": 0.6585365853658536,
        "max": 0.738255033557047
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9533498684565226,
        "bertscore_f1_std": 0.0033754534229825397,
        "bertscore_f1_min": 0.9492437839508057,
        "bertscore_f1_max": 0.9575113654136658,
        "bertscore_precision_mean": 0.9479149182637533,
        "bertscore_recall_mean": 0.9590768615404764
      },
      "avg_output_length_chars": 548.6666666666666,
      "avg_output_length_words": 79.66666666666667,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_006"
    },
    "llama3_8b_summarization_C3_t0.7_abs_007": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 306.3333333333333,
        "std": 27.378012264508094,
        "min": 272,
        "max": 339,
        "normalized_mean": 0.5163082182025353
      },
      "rouge_l": {
        "mean": 0.5037025270278008,
        "std": 0.04579411961330033,
        "min": 0.44155844155844154,
        "max": 0.550561797752809
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9273051023483276,
        "bertscore_f1_std": 0.007114288579525133,
        "bertscore_f1_min": 0.9185879826545715,
        "bertscore_f1_max": 0.9360143542289734,
        "bertscore_precision_mean": 0.9180209239323934,
        "bertscore_recall_mean": 0.9368282953898112
      },
      "avg_output_length_chars": 544.6666666666666,
      "avg_output_length_words": 81.66666666666667,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_007"
    },
    "llama3_8b_summarization_C3_t0.7_abs_008": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 223.66666666666666,
        "std": 20.677416559027765,
        "min": 201,
        "max": 251,
        "normalized_mean": 0.3758897271031179
      },
      "rouge_l": {
        "mean": 0.5740241861047229,
        "std": 0.030334896121084583,
        "min": 0.5314685314685313,
        "max": 0.6
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9489558339118958,
        "bertscore_f1_std": 0.0037939486261405774,
        "bertscore_f1_min": 0.9435906410217285,
        "bertscore_f1_max": 0.9516842365264893,
        "bertscore_precision_mean": 0.9457749525705973,
        "bertscore_recall_mean": 0.9521869818369547
      },
      "avg_output_length_chars": 570.0,
      "avg_output_length_words": 73.66666666666667,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_008"
    },
    "llama3_8b_summarization_C3_t0.7_abs_009": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 316.6666666666667,
        "std": 45.973422273695874,
        "min": 254,
        "max": 363,
        "normalized_mean": 0.48778200368908475
      },
      "rouge_l": {
        "mean": 0.5128395061728396,
        "std": 0.11335568849398255,
        "min": 0.3888888888888889,
        "max": 0.6628571428571429
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9363992214202881,
        "bertscore_f1_std": 0.009298210577515483,
        "bertscore_f1_min": 0.9264490008354187,
        "bertscore_f1_max": 0.9488194584846497,
        "bertscore_precision_mean": 0.9312165975570679,
        "bertscore_recall_mean": 0.941651980082194
      },
      "avg_output_length_chars": 622.3333333333334,
      "avg_output_length_words": 90.66666666666667,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_009"
    },
    "llama3_8b_summarization_C3_t0.7_abs_010": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 353.3333333333333,
        "std": 57.09251750935104,
        "min": 274,
        "max": 406,
        "normalized_mean": 0.5583166878994249
      },
      "rouge_l": {
        "mean": 0.46028749166030297,
        "std": 0.10424124016845983,
        "min": 0.3258426966292135,
        "max": 0.5798816568047338
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9244788885116577,
        "bertscore_f1_std": 0.013983053534770666,
        "bertscore_f1_min": 0.9068449139595032,
        "bertscore_f1_max": 0.941046416759491,
        "bertscore_precision_mean": 0.9222246607144674,
        "bertscore_recall_mean": 0.9268077611923218
      },
      "avg_output_length_chars": 595.6666666666666,
      "avg_output_length_words": 88.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_010"
    },
    "llama3_8b_summarization_C3_t0.7_abs_011": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 324.6666666666667,
        "std": 66.94442139234273,
        "min": 238,
        "max": 401,
        "normalized_mean": 0.5851741095162147
      },
      "rouge_l": {
        "mean": 0.3439933425089854,
        "std": 0.1024769879749366,
        "min": 0.23841059602649006,
        "max": 0.48275862068965514
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9071176250775655,
        "bertscore_f1_std": 0.01236990436907726,
        "bertscore_f1_min": 0.8940091729164124,
        "bertscore_f1_max": 0.923704206943512,
        "bertscore_precision_mean": 0.9007783532142639,
        "bertscore_recall_mean": 0.9135719140370687
      },
      "avg_output_length_chars": 535.6666666666666,
      "avg_output_length_words": 74.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_011"
    },
    "llama3_8b_summarization_C3_t0.7_abs_012": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 404.6666666666667,
        "std": 13.224556283251582,
        "min": 386,
        "max": 415,
        "normalized_mean": 0.599860326403716
      },
      "rouge_l": {
        "mean": 0.3874592735087172,
        "std": 0.015791683121372708,
        "min": 0.36809815950920244,
        "max": 0.4067796610169491
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9213620821634928,
        "bertscore_f1_std": 0.008254616832995774,
        "bertscore_f1_min": 0.9110769629478455,
        "bertscore_f1_max": 0.9312869310379028,
        "bertscore_precision_mean": 0.9162215789159139,
        "bertscore_recall_mean": 0.9265702565511068
      },
      "avg_output_length_chars": 646.0,
      "avg_output_length_words": 83.33333333333333,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_012"
    },
    "llama3_8b_summarization_C3_t0.7_abs_013": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 223.33333333333334,
        "std": 36.23380864453651,
        "min": 174,
        "max": 260,
        "normalized_mean": 0.39429944328020156
      },
      "rouge_l": {
        "mean": 0.6311266985121432,
        "std": 0.06977208094191127,
        "min": 0.5786163522012578,
        "max": 0.7297297297297296
      },
      "bert_score": {
        "bertscore_f1_mean": 0.963212768236796,
        "bertscore_f1_std": 0.005325156455564012,
        "bertscore_f1_min": 0.9579841494560242,
        "bertscore_f1_max": 0.9705209136009216,
        "bertscore_precision_mean": 0.9660068949063619,
        "bertscore_recall_mean": 0.9604719281196594
      },
      "avg_output_length_chars": 531.6666666666666,
      "avg_output_length_words": 75.66666666666667,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_013"
    },
    "llama3_8b_summarization_C3_t0.7_abs_014": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 211.33333333333334,
        "std": 31.094837013383568,
        "min": 185,
        "max": 255,
        "normalized_mean": 0.33239488467068457
      },
      "rouge_l": {
        "mean": 0.6357487662237354,
        "std": 0.07416977858449796,
        "min": 0.5308641975308642,
        "max": 0.6892655367231639
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9458114306131998,
        "bertscore_f1_std": 0.008367656503522504,
        "bertscore_f1_min": 0.9371136426925659,
        "bertscore_f1_max": 0.957109272480011,
        "bertscore_precision_mean": 0.949233869711558,
        "bertscore_recall_mean": 0.9424400726954142
      },
      "avg_output_length_chars": 598.3333333333334,
      "avg_output_length_words": 83.66666666666667,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_014"
    },
    "llama3_8b_summarization_C3_t0.7_abs_015": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 336.6666666666667,
        "std": 67.74625860928082,
        "min": 241,
        "max": 389,
        "normalized_mean": 0.5659998998911316
      },
      "rouge_l": {
        "mean": 0.4510933091366949,
        "std": 0.1396202231244738,
        "min": 0.34355828220858897,
        "max": 0.6482758620689655
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9337836702664694,
        "bertscore_f1_std": 0.012163251249836872,
        "bertscore_f1_min": 0.9191480875015259,
        "bertscore_f1_max": 0.9489287734031677,
        "bertscore_precision_mean": 0.9419741233189901,
        "bertscore_recall_mean": 0.9259777267773946
      },
      "avg_output_length_chars": 531.6666666666666,
      "avg_output_length_words": 79.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_015"
    },
    "llama3_8b_summarization_C3_t0.7_abs_016": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 121.33333333333333,
        "std": 44.0630860875127,
        "min": 61,
        "max": 165,
        "normalized_mean": 0.221240912617932
      },
      "rouge_l": {
        "mean": 0.7841223924821299,
        "std": 0.05246420436798954,
        "min": 0.7236842105263158,
        "max": 0.8516129032258064
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9735913077990214,
        "bertscore_f1_std": 0.008130768879563337,
        "bertscore_f1_min": 0.9647256135940552,
        "bertscore_f1_max": 0.9843655824661255,
        "bertscore_precision_mean": 0.9738243222236633,
        "bertscore_recall_mean": 0.9733605782190958
      },
      "avg_output_length_chars": 542.6666666666666,
      "avg_output_length_words": 77.33333333333333,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_016"
    },
    "llama3_8b_summarization_C3_t0.7_abs_017": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 218.0,
        "std": 39.65686153324121,
        "min": 163,
        "max": 255,
        "normalized_mean": 0.4173237421508624
      },
      "rouge_l": {
        "mean": 0.610685078382965,
        "std": 0.06200686156647635,
        "min": 0.553191489361702,
        "max": 0.6967741935483871
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9508288502693176,
        "bertscore_f1_std": 0.0028961591392834994,
        "bertscore_f1_min": 0.9481011033058167,
        "bertscore_f1_max": 0.9548386931419373,
        "bertscore_precision_mean": 0.9414813121159872,
        "bertscore_recall_mean": 0.9603915413220724
      },
      "avg_output_length_chars": 469.6666666666667,
      "avg_output_length_words": 71.66666666666667,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_017"
    },
    "llama3_8b_summarization_C3_t0.7_abs_018": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 327.6666666666667,
        "std": 28.429249413627197,
        "min": 298,
        "max": 366,
        "normalized_mean": 0.46062569793690517
      },
      "rouge_l": {
        "mean": 0.5849962791658094,
        "std": 0.0642517057210653,
        "min": 0.5056179775280899,
        "max": 0.6629834254143646
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9429791768391927,
        "bertscore_f1_std": 0.004123808500515951,
        "bertscore_f1_min": 0.9371503591537476,
        "bertscore_f1_max": 0.946058988571167,
        "bertscore_precision_mean": 0.944695790608724,
        "bertscore_recall_mean": 0.9414715568224589
      },
      "avg_output_length_chars": 674.0,
      "avg_output_length_words": 91.66666666666667,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_018"
    },
    "llama3_8b_summarization_C3_t0.7_abs_019": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 149.0,
        "std": 19.44222209522358,
        "min": 122,
        "max": 167,
        "normalized_mean": 0.2785976291974111
      },
      "rouge_l": {
        "mean": 0.6983470430278941,
        "std": 0.055126082880359396,
        "min": 0.6382978723404256,
        "max": 0.7714285714285714
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9551934599876404,
        "bertscore_f1_std": 0.006939036090464825,
        "bertscore_f1_min": 0.9491687417030334,
        "bertscore_f1_max": 0.9649142026901245,
        "bertscore_precision_mean": 0.953151524066925,
        "bertscore_recall_mean": 0.9572704434394836
      },
      "avg_output_length_chars": 522.0,
      "avg_output_length_words": 70.66666666666667,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_019"
    },
    "llama3_8b_summarization_C3_t0.7_abs_020": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 265.0,
        "std": 26.620793877468543,
        "min": 231,
        "max": 296,
        "normalized_mean": 0.49482160845222195
      },
      "rouge_l": {
        "mean": 0.5673650014069015,
        "std": 0.04923968515087858,
        "min": 0.5301204819277109,
        "max": 0.6369426751592356
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9371409018834432,
        "bertscore_f1_std": 0.006459751383202837,
        "bertscore_f1_min": 0.9309833645820618,
        "bertscore_f1_max": 0.9460639953613281,
        "bertscore_precision_mean": 0.9405346512794495,
        "bertscore_recall_mean": 0.933785597483317
      },
      "avg_output_length_chars": 511.0,
      "avg_output_length_words": 80.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_020"
    },
    "llama3_8b_summarization_C3_t0.7_abs_021": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 247.66666666666666,
        "std": 30.922843048824316,
        "min": 206,
        "max": 280,
        "normalized_mean": 0.41850674133750987
      },
      "rouge_l": {
        "mean": 0.5672314530632918,
        "std": 0.051901291316602344,
        "min": 0.5165562913907286,
        "max": 0.6385542168674699
      },
      "bert_score": {
        "bertscore_f1_mean": 0.938413937886556,
        "bertscore_f1_std": 0.01284555739129699,
        "bertscore_f1_min": 0.9282400608062744,
        "bertscore_f1_max": 0.956534743309021,
        "bertscore_precision_mean": 0.9336519837379456,
        "bertscore_recall_mean": 0.9434576233228048
      },
      "avg_output_length_chars": 554.3333333333334,
      "avg_output_length_words": 79.66666666666667,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_021"
    },
    "llama3_8b_summarization_C3_t0.7_abs_022": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 217.66666666666666,
        "std": 47.07676945396978,
        "min": 152,
        "max": 260,
        "normalized_mean": 0.3325889678870779
      },
      "rouge_l": {
        "mean": 0.6861496416232663,
        "std": 0.07416392445621092,
        "min": 0.5975609756097562,
        "max": 0.7790697674418605
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9616313179334005,
        "bertscore_f1_std": 0.0076581779376691545,
        "bertscore_f1_min": 0.9508088827133179,
        "bertscore_f1_max": 0.9673998951911926,
        "bertscore_precision_mean": 0.9655213157335917,
        "bertscore_recall_mean": 0.9578063686688741
      },
      "avg_output_length_chars": 623.0,
      "avg_output_length_words": 85.33333333333333,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_022"
    },
    "llama3_8b_summarization_C3_t0.7_abs_023": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 171.0,
        "std": 36.5057073163453,
        "min": 129,
        "max": 218,
        "normalized_mean": 0.31646084748804576
      },
      "rouge_l": {
        "mean": 0.7405822935234699,
        "std": 0.053807121284170464,
        "min": 0.6823529411764706,
        "max": 0.8121212121212121
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9657750527064005,
        "bertscore_f1_std": 0.005909140924109203,
        "bertscore_f1_min": 0.960648238658905,
        "bertscore_f1_max": 0.974053680896759,
        "bertscore_precision_mean": 0.9654620687166849,
        "bertscore_recall_mean": 0.9660893082618713
      },
      "avg_output_length_chars": 532.3333333333334,
      "avg_output_length_words": 83.33333333333333,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_023"
    },
    "llama3_8b_summarization_C3_t0.7_abs_024": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 221.0,
        "std": 37.69173207305107,
        "min": 180,
        "max": 271,
        "normalized_mean": 0.4105913927963491
      },
      "rouge_l": {
        "mean": 0.6054654654654655,
        "std": 0.01439212490862664,
        "min": 0.5866666666666667,
        "max": 0.6216216216216216
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9473812381426493,
        "bertscore_f1_std": 0.004674484261794337,
        "bertscore_f1_min": 0.9418172836303711,
        "bertscore_f1_max": 0.9532548189163208,
        "bertscore_precision_mean": 0.9460546175638834,
        "bertscore_recall_mean": 0.9487250049908956
      },
      "avg_output_length_chars": 524.3333333333334,
      "avg_output_length_words": 74.33333333333333,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_024"
    },
    "llama3_8b_summarization_C3_t0.7_abs_025": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 214.66666666666666,
        "std": 40.18568014714805,
        "min": 171,
        "max": 268,
        "normalized_mean": 0.35443698557713876
      },
      "rouge_l": {
        "mean": 0.6536483808782063,
        "std": 0.02197023380038872,
        "min": 0.6233766233766234,
        "max": 0.6748466257668712
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9623734354972839,
        "bertscore_f1_std": 0.004466063787729531,
        "bertscore_f1_min": 0.9582597017288208,
        "bertscore_f1_max": 0.9685807824134827,
        "bertscore_precision_mean": 0.9587335785230001,
        "bertscore_recall_mean": 0.9661281903584799
      },
      "avg_output_length_chars": 557.0,
      "avg_output_length_words": 81.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_025"
    },
    "llama3_8b_summarization_C3_t0.7_abs_026": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 355.3333333333333,
        "std": 48.76018412142796,
        "min": 291,
        "max": 409,
        "normalized_mean": 0.5838997466521897
      },
      "rouge_l": {
        "mean": 0.45392889122832486,
        "std": 0.06403272808200136,
        "min": 0.40490797546012264,
        "max": 0.5443786982248521
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9344044923782349,
        "bertscore_f1_std": 0.006991235089740127,
        "bertscore_f1_min": 0.9294591546058655,
        "bertscore_f1_max": 0.9442915916442871,
        "bertscore_precision_mean": 0.9310716191927592,
        "bertscore_recall_mean": 0.937768558661143
      },
      "avg_output_length_chars": 594.0,
      "avg_output_length_words": 82.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_026"
    },
    "llama3_8b_summarization_C3_t0.7_abs_027": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 276.0,
        "std": 57.798500557252055,
        "min": 195,
        "max": 326,
        "normalized_mean": 0.480024143950146
      },
      "rouge_l": {
        "mean": 0.4300382859962866,
        "std": 0.15391647188503005,
        "min": 0.28965517241379307,
        "max": 0.6442953020134228
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9304694732030233,
        "bertscore_f1_std": 0.014002535922134199,
        "bertscore_f1_min": 0.9159049391746521,
        "bertscore_f1_max": 0.9493712186813354,
        "bertscore_precision_mean": 0.9318095246950785,
        "bertscore_recall_mean": 0.9291509787241617
      },
      "avg_output_length_chars": 566.0,
      "avg_output_length_words": 73.33333333333333,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_027"
    },
    "llama3_8b_summarization_C3_t0.7_abs_028": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 342.3333333333333,
        "std": 20.27039439401436,
        "min": 322,
        "max": 370,
        "normalized_mean": 0.5885423501895807
      },
      "rouge_l": {
        "mean": 0.4409804963956776,
        "std": 0.03441415194641881,
        "min": 0.4046242774566474,
        "max": 0.4871794871794872
      },
      "bert_score": {
        "bertscore_f1_mean": 0.931065022945404,
        "bertscore_f1_std": 0.003885786120290083,
        "bertscore_f1_min": 0.9272734522819519,
        "bertscore_f1_max": 0.9364056587219238,
        "bertscore_precision_mean": 0.9358987410863241,
        "bertscore_recall_mean": 0.9263018767038981
      },
      "avg_output_length_chars": 547.6666666666666,
      "avg_output_length_words": 82.66666666666667,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_028"
    },
    "llama3_8b_summarization_C3_t0.7_abs_029": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 320.3333333333333,
        "std": 35.405586502069916,
        "min": 290,
        "max": 370,
        "normalized_mean": 0.538537320186379
      },
      "rouge_l": {
        "mean": 0.5081810235737675,
        "std": 0.045077963006684346,
        "min": 0.4444444444444444,
        "max": 0.5411764705882354
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9392536878585815,
        "bertscore_f1_std": 0.007008153255248007,
        "bertscore_f1_min": 0.9299006462097168,
        "bertscore_f1_max": 0.9467695355415344,
        "bertscore_precision_mean": 0.9394712249437968,
        "bertscore_recall_mean": 0.9390455484390259
      },
      "avg_output_length_chars": 585.3333333333334,
      "avg_output_length_words": 84.66666666666667,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_029"
    },
    "llama3_8b_summarization_C3_t0.7_abs_030": {
      "n_outputs": 3,
      "exact_match_rate": 0.0,
      "edit_distance": {
        "mean": 297.6666666666667,
        "std": 49.45929864264375,
        "min": 255,
        "max": 367,
        "normalized_mean": 0.5286721106255207
      },
      "rouge_l": {
        "mean": 0.45249318582651915,
        "std": 0.09039739443098949,
        "min": 0.32467532467532473,
        "max": 0.5185185185185185
      },
      "bert_score": {
        "bertscore_f1_mean": 0.9329392115275065,
        "bertscore_f1_std": 0.014207398148251001,
        "bertscore_f1_min": 0.9132884740829468,
        "bertscore_f1_max": 0.9463924765586853,
        "bertscore_precision_mean": 0.928321103254954,
        "bertscore_recall_mean": 0.9377899567286173
      },
      "avg_output_length_chars": 509.0,
      "avg_output_length_words": 76.0,
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.7",
      "abstract": "abs_030"
    }
  },
  "variability_aggregated": {
    "gpt4_extraction_C2": {
      "model": "gpt4",
      "task": "extraction",
      "condition": "C2",
      "n_abstracts": 30,
      "exact_match_rate": {
        "mean": 0.44333333333333336,
        "std": 0.3410767264609729
      },
      "edit_distance_normalized": {
        "mean": 0.0723951095387781,
        "std": 0.08261749338893698
      },
      "edit_distance_raw": {
        "mean": 65.62333333333335,
        "std": 84.69965062556017
      },
      "rouge_l": {
        "mean": 0.9384089361679284,
        "std": 0.06943431671606315
      },
      "avg_output_length_chars": {
        "mean": 768.1399999999999,
        "std": 190.73310581850342
      },
      "avg_output_length_words": {
        "mean": 103.77333333333334,
        "std": 28.44342155727854
      },
      "bertscore_f1": {
        "mean": 0.990415643453598,
        "std": 0.010571829788025611
      }
    },
    "gpt4_extraction_C3_t0.0": {
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.0",
      "n_abstracts": 14,
      "exact_match_rate": {
        "mean": 0.38095238095238093,
        "std": 0.43080411510590927
      },
      "edit_distance_normalized": {
        "mean": 0.07210317544181981,
        "std": 0.1065588517574116
      },
      "edit_distance_raw": {
        "mean": 68.78571428571429,
        "std": 112.45843350944946
      },
      "rouge_l": {
        "mean": 0.9355735107416748,
        "std": 0.08714084798152046
      },
      "avg_output_length_chars": {
        "mean": 808.4880952380953,
        "std": 192.14651710454976
      },
      "avg_output_length_words": {
        "mean": 110.13095238095238,
        "std": 29.472874576045953
      },
      "bertscore_f1": {
        "mean": 0.9899903777099791,
        "std": 0.013407954225857301
      }
    },
    "gpt4_extraction_C3_t0.3": {
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.3",
      "n_abstracts": 14,
      "exact_match_rate": {
        "mean": 0.14285714285714285,
        "std": 0.28387687575088005
      },
      "edit_distance_normalized": {
        "mean": 0.14765215803807985,
        "std": 0.12872990366306772
      },
      "edit_distance_raw": {
        "mean": 137.78571428571428,
        "std": 122.73051621043402
      },
      "rouge_l": {
        "mean": 0.8669472093319747,
        "std": 0.10916194774773472
      },
      "avg_output_length_chars": {
        "mean": 827.0714285714284,
        "std": 174.9840617196945
      },
      "avg_output_length_words": {
        "mean": 112.28571428571426,
        "std": 26.65636247805879
      },
      "bertscore_f1": {
        "mean": 0.9799100359280905,
        "std": 0.017690833413332427
      }
    },
    "gpt4_extraction_C3_t0.7": {
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.7",
      "n_abstracts": 17,
      "exact_match_rate": {
        "mean": 0.0,
        "std": 0.0
      },
      "edit_distance_normalized": {
        "mean": 0.22474580409022493,
        "std": 0.1065044441483196
      },
      "edit_distance_raw": {
        "mean": 205.078431372549,
        "std": 118.02086264558416
      },
      "rouge_l": {
        "mean": 0.7889613261095606,
        "std": 0.0984357420077968
      },
      "avg_output_length_chars": {
        "mean": 797.8627450980392,
        "std": 180.4544055534808
      },
      "avg_output_length_words": {
        "mean": 107.34313725490196,
        "std": 27.268245076324288
      },
      "bertscore_f1": {
        "mean": 0.9708336217730654,
        "std": 0.013137872566646204
      }
    },
    "gpt4_summarization_C1": {
      "model": "gpt4",
      "task": "summarization",
      "condition": "C1",
      "n_abstracts": 3,
      "exact_match_rate": {
        "mean": 0.1111111111111111,
        "std": 0.19245008972987526
      },
      "edit_distance_normalized": {
        "mean": 0.08792149504082854,
        "std": 0.05230182530098472
      },
      "edit_distance_raw": {
        "mean": 53.555555555555564,
        "std": 29.612935111635352
      },
      "rouge_l": {
        "mean": 0.8963046608254316,
        "std": 0.07671595969008195
      },
      "avg_output_length_chars": {
        "mean": 614.0555555555557,
        "std": 26.62201122157154
      },
      "avg_output_length_words": {
        "mean": 83.72222222222223,
        "std": 6.920848802578205
      },
      "bertscore_f1": {
        "mean": 0.9897118674384223,
        "std": 0.0051506001486399805
      }
    },
    "gpt4_summarization_C2": {
      "model": "gpt4",
      "task": "summarization",
      "condition": "C2",
      "n_abstracts": 30,
      "exact_match_rate": {
        "mean": 0.23,
        "std": 0.1967844961653089
      },
      "edit_distance_normalized": {
        "mean": 0.1365097121353443,
        "std": 0.10947765877325843
      },
      "edit_distance_raw": {
        "mean": 88.44,
        "std": 75.95667367560318
      },
      "rouge_l": {
        "mean": 0.869519540203214,
        "std": 0.09991508137881866
      },
      "avg_output_length_chars": {
        "mean": 606.2666666666667,
        "std": 80.24806940779423
      },
      "avg_output_length_words": {
        "mean": 86.41999999999999,
        "std": 11.23385891877304
      },
      "bertscore_f1": {
        "mean": 0.9839429303010304,
        "std": 0.01349433502960619
      }
    },
    "gpt4_summarization_C3_t0.0": {
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.0",
      "n_abstracts": 30,
      "exact_match_rate": {
        "mean": 0.14444444444444443,
        "std": 0.3117535785552716
      },
      "edit_distance_normalized": {
        "mean": 0.16234488195964258,
        "std": 0.15309299437396795
      },
      "edit_distance_raw": {
        "mean": 102.23333333333335,
        "std": 96.6781661808495
      },
      "rouge_l": {
        "mean": 0.8478557003473348,
        "std": 0.1362181543595866
      },
      "avg_output_length_chars": {
        "mean": 601.7888888888889,
        "std": 78.86097585075157
      },
      "avg_output_length_words": {
        "mean": 85.58888888888889,
        "std": 10.704918242795213
      },
      "bertscore_f1": {
        "mean": 0.9804408397939469,
        "std": 0.0183057841978045
      }
    },
    "gpt4_summarization_C3_t0.3": {
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.3",
      "n_abstracts": 30,
      "exact_match_rate": {
        "mean": 0.0,
        "std": 0.0
      },
      "edit_distance_normalized": {
        "mean": 0.28321169501889504,
        "std": 0.14126296041711517
      },
      "edit_distance_raw": {
        "mean": 177.65555555555557,
        "std": 94.39906662850039
      },
      "rouge_l": {
        "mean": 0.7237729746100136,
        "std": 0.12783826288707442
      },
      "avg_output_length_chars": {
        "mean": 607.1222222222221,
        "std": 83.6466974077794
      },
      "avg_output_length_words": {
        "mean": 86.45555555555556,
        "std": 12.122770232493643
      },
      "bertscore_f1": {
        "mean": 0.9661668757597606,
        "std": 0.016144562809226173
      }
    },
    "gpt4_summarization_C3_t0.7": {
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.7",
      "n_abstracts": 30,
      "exact_match_rate": {
        "mean": 0.0,
        "std": 0.0
      },
      "edit_distance_normalized": {
        "mean": 0.4365639422307474,
        "std": 0.09873681396564125
      },
      "edit_distance_raw": {
        "mean": 279.8111111111111,
        "std": 78.97051602352431
      },
      "rouge_l": {
        "mean": 0.5553605884015086,
        "std": 0.0946226183596611
      },
      "avg_output_length_chars": {
        "mean": 613.2,
        "std": 85.6737820740155
      },
      "avg_output_length_words": {
        "mean": 87.03333333333333,
        "std": 13.033511184370214
      },
      "bertscore_f1": {
        "mean": 0.9476989779207443,
        "std": 0.012681956785687948
      }
    },
    "llama3_8b_extraction_C1": {
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C1",
      "n_abstracts": 30,
      "exact_match_rate": {
        "mean": 0.9866666666666667,
        "std": 0.07302967433402216
      },
      "edit_distance_normalized": {
        "mean": 0.0031172839506172843,
        "std": 0.01707406737901907
      },
      "edit_distance_raw": {
        "mean": 1.3466666666666667,
        "std": 7.375997107736238
      },
      "rouge_l": {
        "mean": 0.9965635738831615,
        "std": 0.01882208101392323
      },
      "avg_output_length_chars": {
        "mean": 462.93333333333334,
        "std": 88.54258018460456
      },
      "avg_output_length_words": {
        "mean": 57.65333333333333,
        "std": 11.694612179246839
      },
      "bertscore_f1": {
        "mean": 0.9997337945302327,
        "std": 0.0014579998634831094
      }
    },
    "llama3_8b_extraction_C2": {
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C2",
      "n_abstracts": 30,
      "exact_match_rate": {
        "mean": 0.9866666666666667,
        "std": 0.07302967433402216
      },
      "edit_distance_normalized": {
        "mean": 0.0031172839506172843,
        "std": 0.01707406737901907
      },
      "edit_distance_raw": {
        "mean": 1.3466666666666667,
        "std": 7.375997107736238
      },
      "rouge_l": {
        "mean": 0.9965635738831615,
        "std": 0.01882208101392323
      },
      "avg_output_length_chars": {
        "mean": 462.93333333333334,
        "std": 88.54258018460456
      },
      "avg_output_length_words": {
        "mean": 57.65333333333333,
        "std": 11.694612179246839
      },
      "bertscore_f1": {
        "mean": 0.9997337945302327,
        "std": 0.0014579998634831094
      }
    },
    "llama3_8b_extraction_C3_t0.0": {
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.0",
      "n_abstracts": 30,
      "exact_match_rate": {
        "mean": 0.9777777777777777,
        "std": 0.12171612389003691
      },
      "edit_distance_normalized": {
        "mean": 0.005195473251028806,
        "std": 0.028456778965031775
      },
      "edit_distance_raw": {
        "mean": 2.2444444444444445,
        "std": 12.293328512893728
      },
      "rouge_l": {
        "mean": 0.9942726231386025,
        "std": 0.031370135023205376
      },
      "avg_output_length_chars": {
        "mean": 462.75555555555553,
        "std": 88.62875921735949
      },
      "avg_output_length_words": {
        "mean": 57.62222222222223,
        "std": 11.715246421772816
      },
      "bertscore_f1": {
        "mean": 0.9995563321643406,
        "std": 0.0024300012720065403
      }
    },
    "llama3_8b_extraction_C3_t0.3": {
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.3",
      "n_abstracts": 30,
      "exact_match_rate": {
        "mean": 0.21111111111111108,
        "std": 0.34445062409485616
      },
      "edit_distance_normalized": {
        "mean": 0.12237648215565697,
        "std": 0.11356922630097
      },
      "edit_distance_raw": {
        "mean": 64.86666666666666,
        "std": 64.61638049560258
      },
      "rouge_l": {
        "mean": 0.8837834272468813,
        "std": 0.1150525343379089
      },
      "avg_output_length_chars": {
        "mean": 460.7777777777777,
        "std": 70.54355646603109
      },
      "avg_output_length_words": {
        "mean": 57.666666666666664,
        "std": 9.53718272898922
      },
      "bertscore_f1": {
        "mean": 0.9850869139035543,
        "std": 0.013662844199620255
      }
    },
    "llama3_8b_extraction_C3_t0.7": {
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.7",
      "n_abstracts": 30,
      "exact_match_rate": {
        "mean": 0.0,
        "std": 0.0
      },
      "edit_distance_normalized": {
        "mean": 0.2530494749735601,
        "std": 0.08900310254162437
      },
      "edit_distance_raw": {
        "mean": 129.53333333333333,
        "std": 51.797246927534964
      },
      "rouge_l": {
        "mean": 0.7719033067617753,
        "std": 0.08893949648001312
      },
      "avg_output_length_chars": {
        "mean": 463.11111111111114,
        "std": 66.5839908809523
      },
      "avg_output_length_words": {
        "mean": 58.06666666666667,
        "std": 8.42974118217561
      },
      "bertscore_f1": {
        "mean": 0.9692769938045078,
        "std": 0.01156895578958067
      }
    },
    "llama3_8b_summarization_C1": {
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C1",
      "n_abstracts": 30,
      "exact_match_rate": {
        "mean": 0.9466666666666668,
        "std": 0.13829836145670418
      },
      "edit_distance_normalized": {
        "mean": 0.004954337569803004,
        "std": 0.014868824028811801
      },
      "edit_distance_raw": {
        "mean": 2.64,
        "std": 7.742475769060098
      },
      "rouge_l": {
        "mean": 0.9945211816170462,
        "std": 0.01564020595928295
      },
      "avg_output_length_chars": {
        "mean": 547.7,
        "std": 50.33807772417931
      },
      "avg_output_length_words": {
        "mean": 77.70666666666666,
        "std": 8.00650310397885
      },
      "bertscore_f1": {
        "mean": 0.9990480816364289,
        "std": 0.002565641660148281
      }
    },
    "llama3_8b_summarization_C2": {
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C2",
      "n_abstracts": 30,
      "exact_match_rate": {
        "mean": 0.9466666666666668,
        "std": 0.13829836145670418
      },
      "edit_distance_normalized": {
        "mean": 0.004954337569803004,
        "std": 0.014868824028811801
      },
      "edit_distance_raw": {
        "mean": 2.64,
        "std": 7.742475769060098
      },
      "rouge_l": {
        "mean": 0.9945211816170462,
        "std": 0.01564020595928295
      },
      "avg_output_length_chars": {
        "mean": 547.7,
        "std": 50.33807772417931
      },
      "avg_output_length_words": {
        "mean": 77.70666666666666,
        "std": 8.00650310397885
      },
      "bertscore_f1": {
        "mean": 0.9990480816364289,
        "std": 0.002565641660148281
      }
    },
    "llama3_8b_summarization_C3_t0.0": {
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.0",
      "n_abstracts": 30,
      "exact_match_rate": {
        "mean": 0.911111111111111,
        "std": 0.230497269094507
      },
      "edit_distance_normalized": {
        "mean": 0.008257229283005006,
        "std": 0.024781373381353
      },
      "edit_distance_raw": {
        "mean": 4.4,
        "std": 12.904126281766832
      },
      "rouge_l": {
        "mean": 0.9908686360284106,
        "std": 0.026067009932138233
      },
      "avg_output_length_chars": {
        "mean": 547.7,
        "std": 50.91442373297868
      },
      "avg_output_length_words": {
        "mean": 77.71111111111112,
        "std": 8.051906955461702
      },
      "bertscore_f1": {
        "mean": 0.998413473367691,
        "std": 0.00427607884781625
      }
    },
    "llama3_8b_summarization_C3_t0.3": {
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.3",
      "n_abstracts": 30,
      "exact_match_rate": {
        "mean": 0.0,
        "std": 0.0
      },
      "edit_distance_normalized": {
        "mean": 0.27898571775059117,
        "std": 0.13645098068504033
      },
      "edit_distance_raw": {
        "mean": 161.12222222222223,
        "std": 83.46350675839524
      },
      "rouge_l": {
        "mean": 0.7441137624683871,
        "std": 0.13051465994803935
      },
      "avg_output_length_chars": {
        "mean": 551.3111111111112,
        "std": 52.89510572303292
      },
      "avg_output_length_words": {
        "mean": 78.41111111111111,
        "std": 7.319570854688626
      },
      "bertscore_f1": {
        "mean": 0.9668628235658009,
        "std": 0.017154740279947876
      }
    },
    "llama3_8b_summarization_C3_t0.7": {
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.7",
      "n_abstracts": 30,
      "exact_match_rate": {
        "mean": 0.0,
        "std": 0.0
      },
      "edit_distance_normalized": {
        "mean": 0.4437631685173161,
        "std": 0.11295669955258883
      },
      "edit_distance_raw": {
        "mean": 262.29999999999995,
        "std": 72.45214301054126
      },
      "rouge_l": {
        "mean": 0.5589163198797281,
        "std": 0.11824679568859435
      },
      "avg_output_length_chars": {
        "mean": 558.9888888888888,
        "std": 45.54114667933225
      },
      "avg_output_length_words": {
        "mean": 79.16666666666667,
        "std": 5.928533509945278
      },
      "bertscore_f1": {
        "mean": 0.9431747145122952,
        "std": 0.01575463417540097
      }
    }
  },
  "overhead": {
    "logging_overhead": {
      "mean_ms": 25.425520386266093,
      "std_ms": 8.99703971627539,
      "min_ms": 11.05,
      "max_ms": 51.88,
      "total_ms": 47393.17,
      "n_runs": 1864
    },
    "storage_overhead": {
      "mean_kb": 4.12493025751073,
      "std_kb": 0.362630168233159,
      "min_kb": 3.33,
      "max_kb": 5.29,
      "total_kb": 7688.870000000001,
      "n_runs": 1864
    },
    "overhead_ratio": {
      "mean_ratio": 0.005452073298150104,
      "std_ratio": 0.0025095840025618123,
      "mean_percent": 0.5452073298150104,
      "max_percent": 1.621183463928668,
      "n_runs": 1864
    },
    "directory_sizes": {
      "runs": {
        "total_kb": 7728.5146484375,
        "total_mb": 7.547377586364746,
        "file_count": 1864
      },
      "provenance": {
        "total_kb": 1736.224609375,
        "total_mb": 1.6955318450927734,
        "file_count": 331
      },
      "run_cards": {
        "total_kb": 2610.0,
        "total_mb": 2.548828125,
        "file_count": 1864
      },
      "prompt_cards": {
        "total_kb": 3.2373046875,
        "total_mb": 0.0031614303588867188,
        "file_count": 2
      },
      "total_output": {
        "total_kb": 19983.830078125,
        "total_mb": 19.515459060668945,
        "file_count": 4063
      }
    }
  },
  "execution_times": {
    "gpt4_extraction_C2": {
      "model": "gpt4",
      "task": "extraction",
      "condition": "C2",
      "n_runs": 150,
      "mean_ms": 5453.508666666667,
      "std_ms": 1749.1276863750822,
      "min_ms": 3097.43,
      "max_ms": 12321.53,
      "median_ms": 5033.0650000000005
    },
    "gpt4_extraction_C3_t0.0": {
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.0",
      "n_runs": 45,
      "mean_ms": 5974.369333333333,
      "std_ms": 1956.684200277463,
      "min_ms": 3278.56,
      "max_ms": 12022.26,
      "median_ms": 5621.23
    },
    "gpt4_extraction_C3_t0.3": {
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.3",
      "n_runs": 46,
      "mean_ms": 5712.394130434783,
      "std_ms": 1517.5580514681783,
      "min_ms": 3662.33,
      "max_ms": 11101.09,
      "median_ms": 5353.53
    },
    "gpt4_extraction_C3_t0.7": {
      "model": "gpt4",
      "task": "extraction",
      "condition": "C3_t0.7",
      "n_runs": 55,
      "mean_ms": 5704.975454545454,
      "std_ms": 1379.1289724688659,
      "min_ms": 3200.07,
      "max_ms": 9339.73,
      "median_ms": 5571.63
    },
    "gpt4_summarization_C1": {
      "model": "gpt4",
      "task": "summarization",
      "condition": "C1",
      "n_runs": 8,
      "mean_ms": 4170.3475,
      "std_ms": 809.5976597359952,
      "min_ms": 3094.6,
      "max_ms": 5754.72,
      "median_ms": 4189.74
    },
    "gpt4_summarization_C2": {
      "model": "gpt4",
      "task": "summarization",
      "condition": "C2",
      "n_runs": 150,
      "mean_ms": 3922.957266666666,
      "std_ms": 1030.1778321788006,
      "min_ms": 2359.23,
      "max_ms": 7050.08,
      "median_ms": 3708.685
    },
    "gpt4_summarization_C3_t0.0": {
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.0",
      "n_runs": 90,
      "mean_ms": 3790.2787777777776,
      "std_ms": 910.2827516739612,
      "min_ms": 2136.93,
      "max_ms": 6245.28,
      "median_ms": 3731.96
    },
    "gpt4_summarization_C3_t0.3": {
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.3",
      "n_runs": 90,
      "mean_ms": 3770.7768888888895,
      "std_ms": 978.7777267565432,
      "min_ms": 2442.41,
      "max_ms": 6635.96,
      "median_ms": 3530.6400000000003
    },
    "gpt4_summarization_C3_t0.7": {
      "model": "gpt4",
      "task": "summarization",
      "condition": "C3_t0.7",
      "n_runs": 90,
      "mean_ms": 3716.5255555555555,
      "std_ms": 816.8718023664259,
      "min_ms": 2067.22,
      "max_ms": 6739.66,
      "median_ms": 3684.995
    },
    "llama3_8b_extraction_C1": {
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C1",
      "n_runs": 150,
      "mean_ms": 5234.221866666668,
      "std_ms": 973.7912137277707,
      "min_ms": 3569.56,
      "max_ms": 8021.02,
      "median_ms": 5140.085
    },
    "llama3_8b_extraction_C2": {
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C2",
      "n_runs": 150,
      "mean_ms": 5284.704266666666,
      "std_ms": 958.8529493792837,
      "min_ms": 3680.69,
      "max_ms": 7812.03,
      "median_ms": 5118.745
    },
    "llama3_8b_extraction_C3_t0.0": {
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.0",
      "n_runs": 90,
      "mean_ms": 5733.264666666668,
      "std_ms": 1375.7033375648132,
      "min_ms": 3568.86,
      "max_ms": 13560.54,
      "median_ms": 5513.755
    },
    "llama3_8b_extraction_C3_t0.3": {
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.3",
      "n_runs": 90,
      "mean_ms": 5436.052666666666,
      "std_ms": 1336.6208899535516,
      "min_ms": 3623.76,
      "max_ms": 12672.49,
      "median_ms": 5223.115
    },
    "llama3_8b_extraction_C3_t0.7": {
      "model": "llama3_8b",
      "task": "extraction",
      "condition": "C3_t0.7",
      "n_runs": 90,
      "mean_ms": 5672.643111111111,
      "std_ms": 1670.0561035302158,
      "min_ms": 3538.91,
      "max_ms": 12021.64,
      "median_ms": 5183.014999999999
    },
    "llama3_8b_summarization_C1": {
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C1",
      "n_runs": 150,
      "mean_ms": 5350.4302,
      "std_ms": 1485.1541268835667,
      "min_ms": 3745.51,
      "max_ms": 12460.31,
      "median_ms": 5098.98
    },
    "llama3_8b_summarization_C2": {
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C2",
      "n_runs": 150,
      "mean_ms": 5068.854666666667,
      "std_ms": 754.7740943444683,
      "min_ms": 3763.93,
      "max_ms": 7470.55,
      "median_ms": 4996.515
    },
    "llama3_8b_summarization_C3_t0.0": {
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.0",
      "n_runs": 90,
      "mean_ms": 5325.647666666667,
      "std_ms": 839.7944658458288,
      "min_ms": 3901.59,
      "max_ms": 7500.65,
      "median_ms": 5258.96
    },
    "llama3_8b_summarization_C3_t0.3": {
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.3",
      "n_runs": 90,
      "mean_ms": 5052.829111111111,
      "std_ms": 629.6582109771033,
      "min_ms": 3766.37,
      "max_ms": 6506.69,
      "median_ms": 4985.8150000000005
    },
    "llama3_8b_summarization_C3_t0.7": {
      "model": "llama3_8b",
      "task": "summarization",
      "condition": "C3_t0.7",
      "n_runs": 90,
      "mean_ms": 5135.776111111111,
      "std_ms": 699.477258411786,
      "min_ms": 4100.7,
      "max_ms": 7310.39,
      "median_ms": 5054.094999999999
    }
  }
}