% Auto-generated LaTeX tables for JAIR paper
% Generated from 190 LLaMA 3 8B experimental runs

% === table1_experimental_design ===

\begin{table}[t]
\centering
\caption{Experimental design: conditions, parameters, and expected outcomes.}
\label{tab:experimental-design}
\small
\begin{tabular}{@{}llcccl@{}}
\toprule
\textbf{Cond.} & \textbf{Description} & \textbf{Temp.} & \textbf{Seed} & \textbf{Reps} & \textbf{Expected Outcome} \\
\midrule
C1 & Fixed seed, greedy & 0.0 & 42 (fixed) & 5 & Deterministic output \\
C2 & Variable seeds, greedy & 0.0 & 5 different & 5 & Near-deterministic \\
C3$_{t{=}0.0}$ & Temp.\ baseline & 0.0 & per-rep & 3 & Deterministic \\
C3$_{t{=}0.3}$ & Low temperature & 0.3 & per-rep & 3 & Low variability \\
C3$_{t{=}0.7}$ & High temperature & 0.7 & per-rep & 3 & High variability \\
\bottomrule
\end{tabular}
\vspace{4pt}
\raggedright\footnotesize
Experiments run on LLaMA~3 8B via Ollama (local, Apple M4, 24\,GB RAM). \\
Each condition applied to 5 abstracts $\times$ 2 tasks = 10 groups per condition. \\
Total: 190 logged runs.
\end{table}


% === table2_main_results ===
\begin{table}[t]
\centering
\caption{Output variability across experimental conditions (mean over 5~abstracts). Exact Match Rate (EMR), Normalized Edit Distance (NED), and ROUGE-L F1 computed over all pairwise comparisons within each condition group.}
\label{tab:variability-results}
\small
\begin{tabular}{@{}llccc@{}}
\toprule
\textbf{Task} & \textbf{Condition} & \textbf{EMR}$\uparrow$ & \textbf{NED}$\downarrow$ & \textbf{ROUGE-L}$\uparrow$ \\
\midrule
  Summarization & C1 (fixed seed, $t{=}0$) & 0.840 & 0.0148 & 0.9823 \\
   & C2 (var.\ seeds, $t{=}0$) & 0.840 & 0.0148 & 0.9823 \\
   & C3 ($t{=}0.0$) & 0.733 & 0.0247 & 0.9706 \\
   & C3 ($t{=}0.3$) & 0.000 & 0.2289 & 0.7820 \\
   & C3 ($t{=}0.7$) & 0.000 & 0.4323 & 0.5550 \\
\midrule
  Extraction & C1 (fixed seed, $t{=}0$) & \textbf{1.000} & \textbf{0.0000} & \textbf{1.0000} \\
   & C2 (var.\ seeds, $t{=}0$) & \textbf{1.000} & \textbf{0.0000} & \textbf{1.0000} \\
   & C3 ($t{=}0.0$) & \textbf{1.000} & \textbf{0.0000} & \textbf{1.0000} \\
   & C3 ($t{=}0.3$) & 0.133 & 0.1883 & 0.8458 \\
   & C3 ($t{=}0.7$) & 0.000 & 0.3031 & 0.7447 \\
\bottomrule
\end{tabular}
\end{table}

% === table3_overhead ===

\begin{table}[t]
\centering
\caption{Protocol overhead: logging time and storage costs for 190~runs.}
\label{tab:overhead}
\small
\begin{tabular}{@{}lrl@{}}
\toprule
\textbf{Metric} & \textbf{Value} & \textbf{Unit} \\
\midrule
\multicolumn{3}{l}{\textit{Logging time overhead}} \\
\quad Mean per run & 31.26 $\pm$ 5.16 & ms \\
\quad Min / Max & 12.85 / 43.17 & ms \\
\quad Total (190 runs) & 5940.1 & ms \\
\quad Mean overhead ratio & 0.609\% & of inference time \\
\quad Max overhead ratio & 1.133\% & of inference time \\
\midrule
\multicolumn{3}{l}{\textit{Storage overhead}} \\
\quad Mean per run record & 4.04 $\pm$ 0.20 & KB \\
\quad Run logs (190~files) & 771.2 & KB \\
\quad PROV documents (191~files) & 1286.0 & KB \\
\quad Run Cards (190~files) & 261.1 & KB \\
\quad Total output & 3.04 & MB \\
\bottomrule
\end{tabular}
\end{table}


% === table4_comparison ===

\begin{table*}[t]
\centering
\caption{Comparison of our protocol with existing reproducibility tools and frameworks for GenAI experiments. Checkmarks (\ding{51}) indicate full support; tildes ($\sim$) indicate partial support; dashes (--) indicate no support.}
\label{tab:comparison}
\small
\begin{tabular}{@{}lcccccc@{}}
\toprule
\textbf{Feature} & \textbf{Ours} & \textbf{MLflow} & \textbf{W\&B} & \textbf{DVC} & \textbf{OpenAI Evals} & \textbf{LangSmith} \\
\midrule
Prompt versioning (Prompt Card) & \ding{51} & -- & $\sim$ & -- & $\sim$ & $\sim$ \\
Run-level provenance (W3C PROV) & \ding{51} & -- & -- & -- & -- & -- \\
Cryptographic output hashing & \ding{51} & -- & -- & \ding{51} & -- & -- \\
Seed \& param logging & \ding{51} & \ding{51} & \ding{51} & -- & \ding{51} & \ding{51} \\
Environment fingerprinting & \ding{51} & $\sim$ & $\sim$ & $\sim$ & -- & -- \\
Model weights hashing & \ding{51} & -- & $\sim$ & \ding{51} & -- & -- \\
Overhead $<$1\% of inference & \ding{51} & $\sim$ & $\sim$ & N/A & N/A & $\sim$ \\
Designed for GenAI text output & \ding{51} & -- & -- & -- & \ding{51} & \ding{51} \\
Open standard (PROV-JSON) & \ding{51} & -- & -- & -- & -- & -- \\
Local-first (no cloud dependency) & \ding{51} & \ding{51} & -- & \ding{51} & -- & -- \\
\bottomrule
\end{tabular}
\end{table*}


% === table5_per_abstract ===
\begin{table}[t]
\centering
\caption{Per-abstract Exact Match Rate for the summarization task across conditions. Each cell reports EMR over all pairwise output comparisons within that group.}
\label{tab:per-abstract}
\small
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Condition} & \textbf{abs\_001} & \textbf{abs\_002} & \textbf{abs\_003} & \textbf{abs\_004} & \textbf{abs\_005} \\
\midrule
  C1 (fixed seed) & 0.60 & \textbf{1.00} & 0.60 & \textbf{1.00} & \textbf{1.00} \\
  C2 (var.\ seeds) & 0.60 & \textbf{1.00} & 0.60 & \textbf{1.00} & \textbf{1.00} \\
  C3 ($t{=}0.0$) & 0.33 & \textbf{1.00} & 0.33 & \textbf{1.00} & \textbf{1.00} \\
  C3 ($t{=}0.3$) & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\
  C3 ($t{=}0.7$) & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\
\bottomrule
\end{tabular}
\end{table}

% === table6_exec_times ===
\begin{table}[t]
\centering
\caption{Inference execution time (ms) per task and condition. Protocol overhead excluded.}
\label{tab:exec-times}
\small
\begin{tabular}{@{}llrrrr@{}}
\toprule
\textbf{Task} & \textbf{Condition} & \textbf{N} & \textbf{Mean} & \textbf{Std} & \textbf{Median} \\
\midrule
  Summarization & C1 & 25 & 5279 & 801 & 5215 \\
   & C2 & 25 & 5396 & 823 & 5353 \\
   & C3_t0.0 & 15 & 5522 & 899 & 5327 \\
   & C3_t0.3 & 15 & 5276 & 638 & 5087 \\
   & C3_t0.7 & 15 & 4991 & 564 & 4870 \\
\midrule
  Extraction & C1 & 25 & 5244 & 1173 & 4787 \\
   & C2 & 25 & 5281 & 1131 & 4859 \\
   & C3_t0.0 & 15 & 5414 & 1153 & 4895 \\
   & C3_t0.3 & 15 & 5273 & 952 & 5081 \\
   & C3_t0.7 & 15 & 5638 & 1326 & 5176 \\
\bottomrule
\end{tabular}
\end{table}

