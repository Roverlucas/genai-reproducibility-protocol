% Auto-generated LaTeX tables for JAIR paper (v2: LLaMA + GPT-4)
% Generated from 1864 experimental runs

% === table2_main_results_v2 ===
\begin{table*}[t]
\centering
\caption{Output variability across experimental conditions for LLaMA~3 8B (local) and GPT-4 (API). Mean over 30~abstracts. EMR = Exact Match Rate, NED = Normalized Edit Distance, ROUGE-L = word-level LCS F1, BS-F1 = BERTScore F1.}
\label{tab:variability-results}
\small
\begin{tabular}{@{}lllcccc@{}}
\toprule
\textbf{Model} & \textbf{Task} & \textbf{Condition} & \textbf{EMR}$\uparrow$ & \textbf{NED}$\downarrow$ & \textbf{ROUGE-L}$\uparrow$ & \textbf{BS-F1}$\uparrow$ \\
\midrule
  \multirow{10}{*}{\rotatebox[origin=c]{90}{LLaMA~3 8B}} & Summarization & C1 (fixed seed, $t{=}0$) & 0.947 & 0.0050 & 0.9945 & 0.9990 \\
   &  & C2 (same params, $t{=}0$) & 0.947 & 0.0050 & 0.9945 & 0.9990 \\
   &  & C3 ($t{=}0.0$) & 0.911 & 0.0083 & 0.9909 & 0.9984 \\
   &  & C3 ($t{=}0.3$) & 0.000 & 0.2790 & 0.7441 & 0.9669 \\
   &  & C3 ($t{=}0.7$) & 0.000 & 0.4438 & 0.5589 & 0.9432 \\
\cmidrule{2-7}
   & Extraction & C1 (fixed seed, $t{=}0$) & 0.987 & 0.0031 & 0.9966 & 0.9997 \\
   &  & C2 (same params, $t{=}0$) & 0.987 & 0.0031 & 0.9966 & 0.9997 \\
   &  & C3 ($t{=}0.0$) & 0.978 & 0.0052 & 0.9943 & 0.9996 \\
   &  & C3 ($t{=}0.3$) & 0.211 & 0.1224 & 0.8838 & 0.9851 \\
   &  & C3 ($t{=}0.7$) & 0.000 & 0.2530 & 0.7719 & 0.9693 \\
\midrule
  \multirow{10}{*}{\rotatebox[origin=c]{90}{GPT-4 (API)}} & Summarization & C1 (fixed seed, $t{=}0$) & 0.111 & 0.0879 & 0.8963 & 0.9897 \\
   &  & C2 (same params, $t{=}0$) & 0.230 & 0.1365 & 0.8695 & 0.9839 \\
   &  & C3 ($t{=}0.0$) & 0.144 & 0.1623 & 0.8479 & 0.9804 \\
   &  & C3 ($t{=}0.3$) & 0.000 & 0.2832 & 0.7238 & 0.9662 \\
   &  & C3 ($t{=}0.7$) & 0.000 & 0.4366 & 0.5554 & 0.9477 \\
\cmidrule{2-7}
   & Extraction & C2 (same params, $t{=}0$) & 0.443 & 0.0724 & 0.9384 & 0.9904 \\
   &  & C3 ($t{=}0.0$) & 0.381 & 0.0721 & 0.9356 & 0.9900 \\
   &  & C3 ($t{=}0.3$) & 0.143 & 0.1477 & 0.8669 & 0.9799 \\
   &  & C3 ($t{=}0.7$) & 0.000 & 0.2247 & 0.7890 & 0.9708 \\
\bottomrule
\end{tabular}
\end{table*}

% === table3_overhead_v2 ===

\begin{table}[t]
\centering
\caption{Protocol overhead: logging time and storage costs for 1864~runs (1140 LLaMA~3 + 724 GPT-4).}
\label{tab:overhead}
\small
\begin{tabular}{@{}lrl@{}}
\toprule
\textbf{Metric} & \textbf{Value} & \textbf{Unit} \\
\midrule
\multicolumn{3}{l}{\textit{Logging time overhead}} \\
\quad Mean per run & 25.43 $\pm$ 9.00 & ms \\
\quad Min / Max & 11.05 / 51.88 & ms \\
\quad Total (1864 runs) & 47393 & ms \\
\quad Mean overhead ratio & 0.545\% & of inference time \\
\quad Max overhead ratio & 1.621\% & of inference time \\
\midrule
\multicolumn{3}{l}{\textit{Storage overhead}} \\
\quad Run logs (1864 files) & 7729 & KB \\
\quad PROV documents (331 files) & 1736 & KB \\
\quad Run Cards (1864 files) & 2610 & KB \\
\quad Total output & 19.52 & MB \\
\bottomrule
\end{tabular}
\end{table}


% === table_model_comparison ===
\begin{table}[t]
\centering
\caption{Reproducibility comparison: LLaMA~3 8B (local) vs.\ GPT-4 (API) under greedy decoding ($t{=}0$). GPT-4 shows significantly lower reproducibility due to server-side non-determinism.}
\label{tab:model-comparison}
\small
\begin{tabular}{@{}llcc@{}}
\toprule
\textbf{Task} & \textbf{Metric} & \textbf{LLaMA~3 8B} & \textbf{GPT-4} \\
\midrule
  Summarization & EMR & 0.947 & 0.230 \\
   & NED & 0.0050 & 0.1365 \\
   & ROUGE-L & 0.9945 & 0.8695 \\
\midrule
  Extraction & EMR & 0.987 & 0.443 \\
   & NED & 0.0031 & 0.0724 \\
   & ROUGE-L & 0.9966 & 0.9384 \\
\bottomrule
\end{tabular}
\end{table}

